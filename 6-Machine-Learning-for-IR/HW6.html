<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta http-equiv="Content-Type" content="text/html">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CS 6200 | Information Retrieval</title>
	<meta name="keywords" content="">
	<meta name="description" content="">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
		integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous">
	</script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"
		integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous">
	</script>
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css"
		integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js"
		integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous">
	</script>

	<link href="../assets/style.css" rel="stylesheet" type="text/css">
</head>

<body>
	
	<div id="main-content" class="container">
		<div class="container">
			<div class="row">
			  <div class="col-md-12">
				<div>
				  <article id="content">
					<h3 style="background-color: white; color: black;
					  font-family: AppleGothic;"><big><big><big><small>CS6200
	
	
	
	
	
	
							  Information Retrieval<br>
							</small> <small><font color="#3333ff">Homework6:
	
	
	
								Machine Learning for IR</font></small></big></big></big></h3>
					<hr>
					<h1 id="objective">Objective</h1>
					<p>In this assignment, you will represent documents as
					  numerical features, and apply machine learning to
					  obtain retrieval ranked lists. The data is the AP89
					  collection we used for HW1.</p>
					<h1 id="data">Data</h1>
					<p>Restrict the data to documents present in the QREL.
					  That is, for each of the 25 queries only consider
					  documents that have a qrel assessment. You should have
					  about 14193 documents; some of the docIDs will appear
					  multiple times for multiple queries, while most
					  documents will not appear at all. For each query, consider additional non-relevant documents (not from qrel) so that you end up with about 1000 non-relevant docs per query; you can obtain these for example with IR runs from HW1</p>
					<p>Split the data randomly into 20 “training” quereis
					  and 5 “testing” queries.</p>
					<h3 id="document_query_features">Document-Query
					  IR Features</h3>
					<p>The general plan is to build a query-doc static
					  feature matrix. <br>
					  qid-docid f1 f2 f3 … fd label <br>
					  qid-docid f1 f2 f3 … fd label … <br>
					  qid-docid f1 f2 f3 … fd label</p>
					<p>You can rearrange this matrix in any format required
					  by the training procedure of the learning algorithm.</p>
					<p>Extract for each query-doc the IR features. These are
					  your HW1 and HW2 models like BM25, Language Models,
					  Cosine, Proximity, etc. The cells are feature values,
					  or the scores given by the IR functions. The label is
					  the qrel relevance value. <br>
					</p>
					<p><b>Note: your IR models from HW1 and HW2 might be
						truncated at 1000 docs,</b> in which case the ranked
					  lists wont contain features for every single qrel
					  document. The better option is to go back to these IR
					  models and compute the scores for all&nbsp; required
					  qrel docs. <br>
					  <b>Alternative option (BAD, DONT DO THIS):</b> every document not in top
					  1000 for an IR function, gets a make-up score=feature
					  value. <b>This make up score should not be 0 by
						default; instead it should be the lowest IR score
						form HW1 for that query, i.e. the score at rank
						1000.</b> (A 0 score might be fine for some, but for
					  certain IR functions like language models 0 is
					  actually a big value.)<br>
					</p>
					<p>You can add other features (columns) that are
					  doc-query-dependent (like number of query terms in
					  document), or doc-dependent and query-independent
					  (like pagerank). <br>
					  You cannot add features that have, for a given
					  document, different meanings on different queries, for
					  example the unigram "China"; such a feature might be
					  important for a query and irrelevant for a different
					  query.<br>
					</p>
					<h3 id="train_a_learning_algorithm">Train a learning
					  algorithm</h3>
					<p>Using the “train” queries static matrix, train a
					  learner to compute a model relating labels to the
					  features. You can use a learning library like <a href="http://www.scipy.org">SciPy/NumPy</a>, <a href="https://github.com/scottjulian/C4.5">C4.5</a>,
					  <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>,
					  <a href="http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/">LibLinear</a>,
					  <a href="http://svmlight.joachims.org">SVM Light</a>,
					  etc. The easiest models are linear regression and
					  decision trees.</p>
					<h3 id="test_the_model">Test the model</h3>
					<p>For each of the 5 testing queries:</p>
					<ol>
					  <li>Run the model to obtain scores</li>
					  <li>Treat the scores as coming from an IR function,
						and rank the documents</li>
					  <li>Format the results as in HW1</li>
					  <li>Run trec<em>eval and report evaluation as “testing</em>performance”.</li>
					</ol>
					<h3 id="test_the_model_on_training_data">Test the model
					  on training data</h3>
					<p>Same as for testing, but on the 20 training queries.
					  Run the learned model against the training matrix,
					  compute prediction/scores, rank, and trec<em>eval.
						Report as “training</em> performance”.</p>
					<p> &nbsp; <br>
					</p>
					<h1 id="extra_credit">Extra Credit</h1>
					<p>These extra problems are provided for students who
					  wish to dig deeper into this project. Extra credit is
					  meant to be significantly harder and more open-ended
					  than the standard problems. We strongly recommend
					  completing all of the above before attempting any of
					  these problems.</p>
					<p>Points will be awarded based on the difficulty of the
					  solution you attempt and how far you get. You will
					  receive no credit unless your solution is “at least
					  half right,” as determined by the graders.</p>
					<h2 id="ec1_document_static_features">EC1: Document
					  static features</h2>
					<p>Extract for each document in the collection “static”
					  features (query independent), such as document length,
					  timestamp, pagerank, etc. Add these to the feature
					  matrix, and rerun the learning algorithm(s). </p>
					<h2 id="ec2_test_on_your_crawled_data">EC2: Test on your
					  crawled data</h2>
					<p>Extract the same features (as in the training matrix)
					  for your evaluated documents (about 200/query). Run
					  the learned model to obtain predictions, rank
					  documents accordingly, and evaluate using the qrel
					  produced in HW4 for your crawl.</p>
					<h2 id="ec3_advance_learning_algorithms">EC3: Advance
					  Learning Algorithms</h2>
					<p>Run fancy learning algorithms like SVM or Neural
					  Networks</p>
					<h2 id="ec4_ranking_algorithms">EC4: Ranking Algorithms</h2>
					<p>Run learning algorithms with a ranking objective,
					  such as SVM-Rank, RankBoost, or LambdaMart.</p>
					<br>
					<!-- ### Deliverables
	
	1. Your group should submit a compressed file containing the TREC and WARC formatted files you crawled and the merged link graph from your individual crawls.
	2. You should each submit the code for your own crawler. -->
					<h3 id="rubric">Rubric<br>
					</h3>
					<dl class="dl-horizontal">
					  <dt>20 points</dt>
					  <dd>A proper static matrix setup for feature values
						and labels</dd>
					  <dt>15 points</dt>
					  <dd>feature values fro IR functions, at least 5
						columns</dd>
					  <dt>15 points</dt>
					  <dd>Training successfully a learning algorithm </dd>
					  <dt>15 points</dt>
					  <dd>Evaluate on training queries</dd>
					  <dt>15 points</dt>
					  <dd>Evaluate on testing queries</dd>
					</dl>
					<br>
					<dl class="dl-horizontal">
					  <dt><br>
					  </dt>
					</dl>
				  </article>
				</div>
			  </div>
			</div>
		  </div>
	</div>
	<footer>
		<hr>
		<center>
			Ⓒ Northeastern University, 2020, all rights reserved<br><br>
		</center>
	</footer>
</body>

</html>