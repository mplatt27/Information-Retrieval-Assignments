{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.client import IndicesClient\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import operator\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "ic = IndicesClient(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Part 1: Read in queries\n",
    "\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: query_analyzer()\n",
    "# Input: The full query as a string (one or more words)\n",
    "# Output: A list of strings where each string is one word (token) of the query\n",
    "def query_analyzer(query):\n",
    "    body = {\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\": [\"english_stemmer\", \"lowercase\", \"english_stop\"],\n",
    "        \"text\": query\n",
    "    }\n",
    "    response = ic.analyze(body=body, index=\"ap_dataset\")\n",
    "    cleaned_queries = [list[\"token\"] for list in response[\"tokens\"]]\n",
    "    return cleaned_queries\n",
    "\n",
    "\n",
    "# Function: read_queries()\n",
    "# Input: The folder path to the queries file as a string\n",
    "# Output: A dictionary mapping each query ID to a list of terms in that query (as str)\n",
    "def read_queries(folder_path):\n",
    "    # iterate over each line in the query\n",
    "    lines = []\n",
    "    ids = []\n",
    "    for line in open(folder_path, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        curr_query = str(line)\n",
    "        id_end = curr_query.find(\".\")\n",
    "        id = curr_query[:id_end].strip()\n",
    "        ids.append(id)\n",
    "        curr_query = curr_query[id_end + 3:].strip()\n",
    "        lines.append(curr_query)\n",
    "\n",
    "    # put them through the analyzer\n",
    "    cleaned_queries = {}\n",
    "    for i in range(len(lines)):\n",
    "        # cleaned_query will be a list of the query words as strings\n",
    "        cleaned_query = query_analyzer(lines[i])\n",
    "        if cleaned_query:\n",
    "            cleaned_queries[ids[i]] = cleaned_query\n",
    "    return cleaned_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ['alleg', 'corrupt', 'public', 'offici', 'govern']\n",
      "59 ['weather', 'caus', 'fatal']\n",
      "56 ['predict', 'prime', 'lend', 'rate', 'prime', 'rate', 'move']\n",
      "71 ['incurs', 'border', 'militari', 'forc', 'guerrilla']\n",
      "64 ['polit', 'hostag']\n",
      "62 ['militari', 'coup', \"d'etat\"]\n",
      "93 ['support', 'nation', 'rifl', 'associat', 'nra']\n",
      "99 ['iran', 'contra', 'affair']\n",
      "58 ['rail', 'strike']\n",
      "77 ['poach', 'wildlif']\n",
      "54 ['contract', 'preliminari', 'agreement', 'tent', 'reserv', 'launch', 'commerci', 'satellit']\n",
      "87 ['crimin', 'offic', 'fail', 'u.s', 'financi', 'institut']\n",
      "94 ['crime', 'aid', 'comput']\n",
      "100 ['non', 'communist', 'industri', 'state', 'regul', 'transfer', 'high', 'tech', 'good', 'technolog', 'undesir', 'nation']\n",
      "89 ['exist', 'pend', 'invest', 'opec', 'member', 'state', 'ani', 'downstream', 'oper']\n",
      "61 ['israel', 'iran', 'contra', 'affair']\n",
      "95 ['comput', 'crime', 'solv']\n",
      "68 ['studi', 'safeti', 'manufactur', 'employe', 'instal', 'worker', 'fine', 'diamet', 'fiber', 'insul']\n",
      "57 ['mci', 'bell', 'system', 'breakup']\n",
      "97 ['fiber', 'optic', 'technolog', 'actual']\n",
      "98 ['individu', 'organ', 'produc', 'fiber', 'optic', 'equip']\n",
      "60 ['perform', 'salari', 'incent', 'pai', 'determin', 'senior']\n",
      "80 ['1988', 'presidenti', 'candid']\n",
      "63 ['machin', 'translat', 'system']\n",
      "91 ['army', 'advanc', 'weapon', 'system']\n"
     ]
    }
   ],
   "source": [
    "# Main code to get the queries and print them out\n",
    "queries_path = \"C:/6200-IR/homework1-mplatt27/IR_data/AP_DATA/queries_modified_x.txt\"\n",
    "queries = read_queries(queries_path)\n",
    "for key, value in queries.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Part 2: Retrieval models\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "\n",
    "# Function: get_all_docs()\n",
    "# Input: None\n",
    "# Output: A list of all doc_ids in the index\n",
    "# Does: Uses es search() API to retrieve the document ids of each doc in the index. Uses scroll to do this.\n",
    "def get_all_docs():\n",
    "    # initialize list that will store all doc ids that we need to return\n",
    "    doc_ids = []\n",
    "\n",
    "    # start the search process, we will then scroll until we have all docs\n",
    "    query_body = {\n",
    "        \"size\": 10000,\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"ap_dataset\", body=query_body, scroll='3m')\n",
    "    old_scroll_id = response['_scroll_id']\n",
    "\n",
    "    for doc in response['hits']['hits']:\n",
    "        doc_ids.append(doc['_id'])\n",
    "\n",
    "    while len(response['hits']['hits']):\n",
    "        response = es.scroll(scroll_id=old_scroll_id, scroll='3m')\n",
    "        old_scroll_id = response['_scroll_id']\n",
    "        for doc in response['hits']['hits']:\n",
    "            doc_ids.append(doc['_id'])\n",
    "\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "# Function: get_term_vectors\n",
    "# Input: A list of document ids\n",
    "# Output: Dictionary mapping doc-ids --> term vector for that document\n",
    "# Does: Collects the term vector for each document in the input list using the es termvectors API. Returns as dict.\n",
    "# This output takes a very long time, so we will dump to pickle and load it in each time to run the models.\n",
    "def get_term_vectors(doc_ids):\n",
    "    vector_per_doc = {}\n",
    "    for doc in doc_ids:\n",
    "        tv = es.termvectors(index=\"ap_dataset\", id=doc, fields=\"text\", term_statistics=True)\n",
    "        vector_per_doc[doc] = tv\n",
    "\n",
    "    return vector_per_doc\n",
    "\n",
    "\n",
    "# Function: sort_scores_dict()\n",
    "# Input: scores, a dictionary that maps query # to a dictionary (docno --> score)\n",
    "# Output: The same dictionary, but the value (dict that each key maps to) is now sorted by the scores\n",
    "def sort_scores_dict(scores):\n",
    "    for q_id, d in scores.items():\n",
    "        sorted_dict = dict(sorted(d.items(), key=operator.itemgetter(1), reverse=True))\n",
    "        scores[q_id] = sorted_dict\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Function: write_scores_to_file_es()\n",
    "# Input: A dictionary of query responses (documents returned for each query) and a name for the file\n",
    "# Output: None\n",
    "# Does: Writes a file for the output to ES built in model. Scores will already by sorted.\n",
    "# For each query response, writes a line for each document that was returned that includes the query number,\n",
    "# doc number, rank, and score. Each line should be of the form: <query-number> Q0 <docno> <rank> <score> Exp\n",
    "def write_scores_to_file_es(response_dict, name):\n",
    "    # assumes scores are already sorted\n",
    "    file_name = name + \".txt\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    output = open(file_name, \"w\")\n",
    "\n",
    "    # iterate over the response_dict for each query (maps query number from input to response dict)\n",
    "    # response[\"hits\"][\"hits\"] is a list of dicts for each doc with keys:\n",
    "    # _id, _score, _source (dict of keys \"file_name\", \"text\")\n",
    "    for q_id, response in response_dict.items():\n",
    "        query_number = q_id\n",
    "        rank = 1\n",
    "        for doc in response[\"hits\"][\"hits\"]:\n",
    "            docno = doc[\"_id\"]\n",
    "            score = doc[\"_score\"]\n",
    "            new_line = query_number + \" Q0 \" + docno + \" \" + str(rank) + \" \" + str(score) + \" Exp\\n\"\n",
    "            output.write(new_line)\n",
    "            rank += 1\n",
    "    output.close()\n",
    "\n",
    "\n",
    "# Function: write_scores_to_file()\n",
    "# Input: A dictionary of scored documents for each query and a name for the file\n",
    "# Output: None\n",
    "# Does: Writes a file for the output. Assumes scores are already sorted. For each query response, writes a line\n",
    "# for each document that was returned that includes the query number, doc number, rank, and score.\n",
    "# Each line should be of the form: <query-number> Q0 <docno> <rank> <score> Exp\n",
    "# This is for all models, except the ES built in, due to the differing format of results.\n",
    "def write_scores_to_file(scores, name):\n",
    "    # assumes scores are already sorted\n",
    "    # scores is dict of query id --> dict (doc_id --> score)\n",
    "    file_name = name + \".txt\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    output = open(file_name, \"w\")\n",
    "\n",
    "    # iterate over query id responses\n",
    "    for q_id, dict in scores.items():\n",
    "        query_number = q_id\n",
    "        rank = 1\n",
    "        for doc_id, score in dict.items():\n",
    "#             if rank > 2000:\n",
    "#                 break\n",
    "            new_line = query_number + \" \" + \"Q0\" + \" \" + doc_id + \" \" + str(rank) + \" \" + str(score) + \" Exp\\n\"\n",
    "            output.write(new_line)\n",
    "            rank += 1\n",
    "    output.close()\n",
    "    \n",
    "\n",
    "# Function: vocab_size\n",
    "# Input: None\n",
    "# Output: size of vocab for index\n",
    "def vocab_size():\n",
    "    req = {\n",
    "        'aggs': {\n",
    "            \"vocabSize\": {\n",
    "                \"cardinality\": {\n",
    "                    \"field\": \"text\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    resp = es.search(index=\"ap_dataset\", body=req)\n",
    "    v = resp[\"aggregations\"][\"vocabSize\"][\"value\"]\n",
    "    return v\n",
    "\n",
    "\n",
    "# Function: create_terms_dict()\n",
    "# Input: term vectors dictionary that maps doc id --> tvs (maps doc id --> tv, dict with \n",
    "#        keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "# Output: terms dictionary that  will map terms (str) --> dict of info: doc_freq, ttf\n",
    "def create_terms_dict(tvs):\n",
    "    \n",
    "    # terms_dict will map terms (str) --> dict of info: doc_freq, ttf; will be size V\n",
    "    terms_dict = {}\n",
    "    \n",
    "    for doc_id, tv in tvs.items():\n",
    "        terms = tv[\"term_vectors\"][\"text\"][\"terms\"]\n",
    "        for term, info in terms.items():\n",
    "            term = term.strip()\n",
    "            if term not in terms_dict.keys():\n",
    "                df = info[\"doc_freq\"]\n",
    "                ttf = info[\"ttf\"]\n",
    "                terms_dict[term] = {\"doc_freq\": df, \"ttf\": ttf}\n",
    "    \n",
    "    return terms_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval Models\n",
    "\"\"\"\n",
    "\n",
    "# Model 1: ES Built-in\n",
    "# Input: A dictionary of queries where their ID is mapped to a list of the queries as a string, each token separated\n",
    "# by a single whitespace\n",
    "# Returns: A dictionary of the responses provided by ES for each query\n",
    "# Does: Iterates through each query and saves the HIT responses in a response dictionary. Max 1000 hits per query\n",
    "def es_built_in(query_dict):\n",
    "    responses = {}\n",
    "    for id, query in query_dict.items():\n",
    "        query = \" \".join(query)\n",
    "        query_body = {\n",
    "            \"size\": 2000,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        response = es.search(index=\"ap_dataset\", body=query_body)\n",
    "        responses[id] = response\n",
    "    return responses\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 2; Okapi TF\n",
    "\n",
    "# Function: okapi_tf_calc, a helper function to calculate score\n",
    "def okapi_tf_calc(tf, doc_len, avg_corp_len):\n",
    "    score = tf / (tf + 0.5 + (1.5 * (doc_len / avg_corp_len)))\n",
    "    return score\n",
    "\n",
    "\n",
    "# Input: A dictionary of term vectors for each document (doc-id --> term vector dict) and a dictionary\n",
    "# of queries (query id --> list of each word in the query as str\n",
    "# Returns: A scores dictionary (query id --> dictionary (doc-id --> score)\n",
    "# Does: Iterates through each document, and through each query term and calculates okapi-tf for the document-word\n",
    "# combination. Sums score and returns as dict.\n",
    "def okapi_tf(tvs, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    avg_doc_len = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"] / \\\n",
    "                                  tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]\n",
    "                    # calculate okapi tf\n",
    "                    temp_score = okapi_tf_calc(tf_value, doc_len, avg_doc_len)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    if doc_id not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_id] = temp_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_id] += temp_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 3: TF-IDF\n",
    "\n",
    "\n",
    "# Function for calculating tf-idf\n",
    "def tf_idf_calc(okapi_tf_score, total_docs, df):\n",
    "    return okapi_tf_score * math.log((total_docs/df), 2)\n",
    "\n",
    "\n",
    "# Function tf-idf\n",
    "def tf_idf(tvs, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    avg_doc_len = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"] / \\\n",
    "                                  tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]\n",
    "                    # calculate okapi tf\n",
    "                    okapi_tf_score = okapi_tf_calc(tf_value, doc_len, avg_doc_len)\n",
    "                    total_docs = len(tvs)\n",
    "                    df = tf['doc_freq']\n",
    "                    tf_idf_score = tf_idf_calc(okapi_tf_score, total_docs, df)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    if doc_id not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_id] = tf_idf_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_id] += tf_idf_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 4: Okapi BM25\n",
    "\n",
    "\n",
    "# Function for calculating BM25\n",
    "def okapi_BM25_calc(total_docs, df, tf_value, tf_query, doc_len, avg_doc_len, k_1, k_2, b):\n",
    "    calc1_num = total_docs + 0.5\n",
    "    calc1_den = df + 0.5\n",
    "    calc1 = math.log((calc1_num/calc1_den), 2)\n",
    "\n",
    "    calc2_num = tf_value + (k_1 * tf_value)\n",
    "    cal2_den = tf_value + k_1 * ((1-b) + (b * (doc_len/avg_doc_len)))\n",
    "    calc2 = calc2_num/cal2_den\n",
    "\n",
    "    calc3_num = tf_query + (k_2 * tf_query)\n",
    "    calc3_den = tf_query + k_2\n",
    "    calc3 = calc3_num/calc3_den\n",
    "\n",
    "    return calc1 * calc2 * calc3\n",
    "\n",
    "\n",
    "# Function: Okapi BM25\n",
    "def okapi_BM25(tvs, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # maps the query # --> Counter (for each query)\n",
    "    queries_counter = {}\n",
    "    # populate with query ids mapped to empty dict and queries counter with tf_queries\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "        queries_counter[q_id] = Counter()\n",
    "        for word in query:\n",
    "            queries_counter[q_id][word] += 1\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "\n",
    "                    # for first block of equation\n",
    "                    total_docs = len(tvs)\n",
    "                    df = tf['doc_freq']\n",
    "\n",
    "                    # for second block of equation\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    avg_doc_len = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"] / \\\n",
    "                                  tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]\n",
    "\n",
    "                    # for third block of equation, number of times the word occurs in the query\n",
    "                    tf_query = queries_counter[q_id][word]\n",
    "\n",
    "                    # constants\n",
    "                    k_1 = 1.2\n",
    "                    k_2 = 1.2\n",
    "                    b = 0.75\n",
    "\n",
    "                    # calculate okapi BM25\n",
    "                    okapi_BM25_score = okapi_BM25_calc(total_docs, df, tf_value, tf_query, doc_len, avg_doc_len, k_1, k_2, b)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    if doc_id not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_id] = okapi_BM25_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_id] += okapi_BM25_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 5: Unigram LM with Laplace smoothing\n",
    "\n",
    "\n",
    "# Function for laplace calculation\n",
    "def p_laplace_calc(tf_value, doc_len, v):\n",
    "    return (tf_value + 1) / (doc_len + v)\n",
    "\n",
    "\n",
    "# Function: Unigram LM with Laplace smoothing\n",
    "def unigram_lm_laplace(tvs, query_dict):\n",
    "    \n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    \n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "\n",
    "    # get vocabulary size from es search API, will be used in calculation\n",
    "    v = vocab_size()\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    p_laplace_score = p_laplace_calc(tf_value, doc_len, v)\n",
    "                    score = math.log(p_laplace_score)\n",
    "                else:\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    p_laplace_score = p_laplace_calc(0, doc_len, v)\n",
    "                    score = math.log(p_laplace_score)\n",
    "\n",
    "                # add score to dictionary\n",
    "                if doc_id not in scores[q_id].keys():\n",
    "                    scores[q_id][doc_id] = score\n",
    "                else:\n",
    "                    scores[q_id][doc_id] += score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 6: Unigram LM with Jelinek-Mercer smoothing\n",
    "\n",
    "\n",
    "# Caluclation for p_jm when the term is in the current document\n",
    "def p_jm_calc(tf_value, doc_len, ttf, sum_ttf, v):\n",
    "    lambda_val = 0.9\n",
    "    calc1 = lambda_val * (tf_value/doc_len)\n",
    "    calc2 = (1-lambda_val) * (ttf / v)\n",
    "#     calc2 = (1-lambda_val) * ((ttf - tf_value) / (sum_ttf - doc_len))\n",
    "    return calc1 + calc2\n",
    "\n",
    "\n",
    "# Caluclation for p_jm when the term is not in the current document\n",
    "def p_jm_calc_term_not_present(ttf, v):\n",
    "    lambda_val = 0.9\n",
    "    return (1-lambda_val) * (ttf / v)\n",
    "\n",
    "\n",
    "# Function: Unigram LM with Jelinek-Mercer smoothing\n",
    "def unigram_lm_jm(tvs, query_dict, terms_dict):\n",
    "    \n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    \n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "        \n",
    "    # get vocabulary size from es search API, will be used in calculation\n",
    "    v = vocab_size()\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    ttf = tf['ttf']\n",
    "                    sum_ttf = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"]\n",
    "                    p_jm_score = p_jm_calc(tf_value, doc_len, ttf, sum_ttf, v)\n",
    "                    score = math.log(p_jm_score)\n",
    "                else:\n",
    "                    # case where word is not in the doc, we still need to account for that\n",
    "                    term_info = terms_dict.get(word, 0)\n",
    "                    ttf = 100\n",
    "                    if term_info != 0:\n",
    "                        ttf = int(term_info['ttf'])\n",
    "                    p_jm_score = p_jm_calc_term_not_present(ttf, v)\n",
    "                    score = math.log(p_jm_score)\n",
    "\n",
    "                # add score to dictionary\n",
    "                if doc_id not in scores[q_id].keys():\n",
    "                    scores[q_id][doc_id] = score\n",
    "                else:\n",
    "                    scores[q_id][doc_id] += score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code for running the models and saving output to file\n",
    "# ONLY RUN IF FIRST TIME USING THIS: Create pickle file with all termvectors for each doc\n",
    "\n",
    "\n",
    "# 1. Use search API to get ids of all docs, this will be used to get term vectors\n",
    "# return_ids = get_all_docs()\n",
    "# print(\"We got responses for this many docs: \", len(return_ids))\n",
    "\n",
    "# 2. Create dictionary that will store term vectors\n",
    "# return_term_vectors = get_term_vectors(return_ids)\n",
    "# print(\"We got term vectors for this many docs: \", len(return_term_vectors))\n",
    "\n",
    "# 3. Save term vectors in pickle file\n",
    "# with open('termvectorswstats.pickle', 'wb') as handle:\n",
    "#     pickle.dump(return_term_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file has opened\n",
      "There are this many documents to search:  84678\n"
     ]
    }
   ],
   "source": [
    "# IF RUNNING AFTER CREATING PICKLE FILE:\n",
    "\n",
    "\n",
    "# Load pickle file that contains term vectors for each doc\n",
    "handle = open('C:/6200-IR/homework1-mplatt27/config/termvectorswstats.pickle', 'rb')\n",
    "return_term_vectors = pickle.load(handle)\n",
    "handle.close()\n",
    "print(\"Pickle file has opened\")\n",
    "print(\"There are this many documents to search: \", len(return_term_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are this many terms:  180393\n"
     ]
    }
   ],
   "source": [
    "# Create terms dictionary from the term vectors. This maps the terms to info that does not change (ttf and doc_freq)\n",
    "# This is needed for the Unigram LM with Jelinek-Mercer Smoothing model\n",
    "terms_stats_dict = create_terms_dict(return_term_vectors)\n",
    "print(\"There are this many terms: \", len(terms_stats_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES-Built in finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 1:\n",
    "hits = es_built_in(queries)\n",
    "write_scores_to_file_es(hits, \"es_built_in_results\")\n",
    "print(\"ES-Built in finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi-TF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 2:\n",
    "doc_scores = okapi_tf(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_tf_results\")\n",
    "print(\"Okapi-TF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 3:\n",
    "doc_scores = tf_idf(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"tfidf_results\")\n",
    "print(\"TF-IDF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi BM25 finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 4:\n",
    "doc_scores = okapi_BM25(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_bm25_results\")\n",
    "print(\"Okapi BM25 finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Laplace Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 5:\n",
    "doc_scores = unigram_lm_laplace(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_laplace_results\")\n",
    "print(\"Unigram LM with Laplace Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Jelinek-Mercer Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 6:\n",
    "doc_scores = unigram_lm_jm(return_term_vectors, queries, terms_stats_dict)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_jm_results\")\n",
    "print(\"Unigram LM with Jelinek-Mercer Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
