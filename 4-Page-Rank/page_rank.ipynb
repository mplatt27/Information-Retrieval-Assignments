{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Homework 4: Web graph computation\"\"\"\n",
    "\n",
    "import pickle\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.client import IndicesClient\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89289\n",
      "89289\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Part 1: Page Rank from crawl\"\"\"\n",
    "\n",
    "# load in inlink files\n",
    "save_path = \"C:/6200-IR/homework-4-mplatt27/\"\n",
    "handle = open(save_path + \"inlinks2.pickle\", 'rb')\n",
    "in_links = pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "# create outlinks from inlinks\n",
    "out_links = {}\n",
    "for key, value in in_links.items():\n",
    "    for each in value:\n",
    "        if each not in out_links.keys():\n",
    "            out_links[each] = [key]\n",
    "        else:\n",
    "            out_links[each].append(key)\n",
    "                \n",
    "for key, value in in_links.items():\n",
    "    if key not in out_links.keys():\n",
    "        out_links[key] = []\n",
    "\n",
    "for key, value in in_links.items():\n",
    "    in_links[key] = list(set(value))\n",
    "        \n",
    "for key, value in out_links.items():\n",
    "    out_links[key] = list(set(value))\n",
    "\n",
    "\n",
    "# check the size of them (should be 89289)\n",
    "print(len(in_links))\n",
    "print(len(out_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to run page rank\n",
    "\n",
    "\"\"\"\n",
    " Function: calc_perplexity\n",
    " Input: dictionary of scores for each link in set\n",
    " Output: perplexity calculation for set\n",
    "\"\"\"\n",
    "def calc_perplexity(pr_scores):\n",
    "    e = 0\n",
    "    for l, rank in pr_scores.items():\n",
    "        e += rank * math.log(rank,2)\n",
    "    return 2**(-e)\n",
    "    \n",
    "\"\"\"\n",
    " Function: page_rank\n",
    " Input: inlink and outlink dictionaries\n",
    " Output: the page ranking for every link in the set\n",
    "\"\"\"\n",
    "def page_rank(inlinks, outlinks):\n",
    "    \n",
    "    # all pages in the set\n",
    "    p = list(inlinks.keys())\n",
    "    n = len(p)\n",
    "    \n",
    "    # all pages with no outlinks\n",
    "    s = set([]) \n",
    "    for each in p:\n",
    "        if len(outlinks[each]) == 0:\n",
    "            s.add(each)\n",
    "    \n",
    "    # links mapped to page rank\n",
    "    pr = {}\n",
    "    for each in p:\n",
    "        pr[each] = 1/n\n",
    "        \n",
    "    # damping/teleportation factor\n",
    "    d = 0.85\n",
    "\n",
    "    # compute starting perplexity\n",
    "    old_pp = calc_perplexity(pr)\n",
    "\n",
    "    # iterate until convergence\n",
    "    j = 0\n",
    "    while j < 4:\n",
    "\n",
    "        # get sink page rank\n",
    "        sink_pr = 0\n",
    "        for each in s:\n",
    "            sink_pr += pr[each]\n",
    "            \n",
    "        # calc new page rank score\n",
    "        updated_pr = {}\n",
    "        for each in p:\n",
    "            updated_pr[each] = (1 - d)/n\n",
    "            updated_pr[each] += d * (sink_pr/n)\n",
    "            for q in inlinks[each]:\n",
    "                try:\n",
    "                    updated_pr[each] += d * (pr[q]/len(outlinks[q]))\n",
    "                except:\n",
    "                    pass # should there be a defalut value here?\n",
    "                    \n",
    "        # update new calculated score\n",
    "        for each in p:\n",
    "            pr[each] = updated_pr[each]\n",
    "            \n",
    "        new_pp = calc_perplexity(pr)\n",
    "        \n",
    "        if abs(old_pp - new_pp) < 1:\n",
    "            j += 1\n",
    "            \n",
    "        old_pp = new_pp\n",
    "\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Run page rank on coprus_wwii\n",
    "\"\"\"\n",
    "page_rankings = page_rank(in_links, out_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the page rankings\n",
    "sorted_page_rankings = sorted(page_rankings.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://en.wikipedia.org/wiki/Eisenstadt 0.0031 outlinks:  1 inlinks:  206\n",
      "http://en.wikipedia.org/wiki/Sunshine_duration 0.0028 outlinks:  1 inlinks:  676\n",
      "http://en.wikipedia.org/wiki/Wikipedia:Stand-alone_lists 0.0025 outlinks:  1 inlinks:  21\n",
      "http://www.worldcat.org/issn/0261-3077 0.0025 outlinks:  1 inlinks:  566\n",
      "http://en.wikipedia.org/wiki/Institutional_seats_of_the_European_Union 0.0024 outlinks:  1 inlinks:  174\n",
      "http://en.wikipedia.org/wiki/UTC+2 0.0024 outlinks:  1 inlinks:  629\n",
      "http://en.wikipedia.org/wiki/Religion 0.0024 outlinks:  1 inlinks:  333\n",
      "http://en.wikipedia.org/wiki/Gothic_architecture 0.0022 outlinks:  1 inlinks:  281\n",
      "http://en.wikipedia.org/wiki/Horsepower 0.0020 outlinks:  1 inlinks:  729\n",
      "http://en.wikipedia.org/wiki/Wikipedia:Article_wizard 0.0020 outlinks:  1 inlinks:  115\n"
     ]
    }
   ],
   "source": [
    "# print x highest ranked pages\n",
    "for i in range(10):\n",
    "    print(sorted_page_rankings[i][0], \"{:.4f}\".format(sorted_page_rankings[i][1]), \"outlinks: \", \n",
    "          len(out_links[sorted_page_rankings[i][0]]), \"inlinks: \", len(in_links[sorted_page_rankings[i][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write top 500 to file\n",
    "file_name = \"crawled_pagerank_scores.txt\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "crawled_pr = open(file_name, \"w\")\n",
    "for i in range(500):\n",
    "    crawled_pr.write(sorted_page_rankings[i][0] + \"\\t\" + str(sorted_page_rankings[i][1]) + \"\\toutlinks: \" + str(len(out_links[sorted_page_rankings[i][0]])) + \"\\tinlinks: \" + str(len(in_links[sorted_page_rankings[i][0]])) + \"\\n\")\n",
    "crawled_pr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Part 2: Page rank with wt2g inlinks graph\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 183811 entries in inlinks\n",
      "We have 183811 entries in outlinks\n"
     ]
    }
   ],
   "source": [
    "# read in file and make in links graph\n",
    "\n",
    "\"\"\"\n",
    "Function: get_data_from_text_file()\n",
    "Input: file: a single file path\n",
    "Returns: data: a list of lists; each sub-list is a line from the file\n",
    "Does: reads each line of the file and appends it in a list to data, then joins those lists together and returns the string\n",
    "\"\"\"\n",
    "def get_data_from_text_file(file):\n",
    "    # declare an empty list for the data\n",
    "    data = []\n",
    "    for line in open(file, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        data += [str(line)]\n",
    "        \n",
    "    return data\n",
    "        \n",
    "\"\"\"\n",
    "Function: get_link_graph()\n",
    "Input: List of data where each list item is that url's raw inlink and outlink data as a string\n",
    "output: Inlink and outlink dictionaries for each url in the corpus\n",
    "\"\"\"\n",
    "def make_link_graph(data_list):\n",
    "    inlinks = {}\n",
    "    outlinks = {}\n",
    "    for link in data_list:\n",
    "        link_list = link.split()\n",
    "        inlinks[link_list[0]] = list(link_list[1:])\n",
    "        \n",
    "    for key, value in inlinks.items():\n",
    "        for each in value:\n",
    "            if each not in outlinks.keys():\n",
    "                outlinks[each] = [key]\n",
    "            else:\n",
    "                outlinks[each].append(key)\n",
    "                \n",
    "    for key, value in inlinks.items():\n",
    "        if key not in outlinks.keys():\n",
    "            outlinks[key] = []\n",
    "            \n",
    "    for key, value in inlinks.items():\n",
    "        inlinks[key] = list(set(value))\n",
    "        \n",
    "    for key, value in outlinks.items():\n",
    "        outlinks[key] = list(set(value))\n",
    "\n",
    "    return inlinks, outlinks\n",
    "\n",
    "# read in from file\n",
    "filepath = \"C:/6200-IR/homework-4-mplatt27/wt2g_inlinks.txt\"\n",
    "raw_data = get_data_from_text_file(filepath)\n",
    "\n",
    "# put into link graph form\n",
    "in_links_wt2g, out_links_wt2g = make_link_graph(raw_data)\n",
    "print(\"We have {} entries in inlinks\".format(len(in_links_wt2g)))\n",
    "print(\"We have {} entries in outlinks\".format(len(out_links_wt2g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# check that it is correct with expected output on Piazza\n",
    "print(len(in_links_wt2g['WT24-B26-10']))\n",
    "print(len(out_links_wt2g['WT24-B26-10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run page rank on wt2g set\n",
    "page_rankings_w = page_rank(in_links_wt2g, out_links_wt2g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the page rankings\n",
    "sorted_page_rankings_w = sorted(page_rankings_w.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT21-B37-76 0.0026945 outlinks:  5 inlinks:  2568\n",
      "WT21-B37-75 0.0015332 outlinks:  1 inlinks:  1704\n",
      "WT25-B39-116 0.0014685 outlinks:  1 inlinks:  169\n",
      "WT23-B21-53 0.0013735 outlinks:  1 inlinks:  198\n",
      "WT24-B26-10 0.0012762 outlinks:  1 inlinks:  291\n",
      "WT24-B40-171 0.0012453 outlinks:  209 inlinks:  270\n",
      "WT23-B39-340 0.0012429 outlinks:  395 inlinks:  274\n",
      "WT23-B37-134 0.0012054 outlinks:  2 inlinks:  207\n",
      "WT08-B18-400 0.0011448 outlinks:  0 inlinks:  990\n",
      "WT13-B06-284 0.0011366 outlinks:  2 inlinks:  454\n"
     ]
    }
   ],
   "source": [
    "# print x highest ranked pages\n",
    "for i in range(10):\n",
    "    print(sorted_page_rankings_w[i][0], \"{:.7f}\".format(sorted_page_rankings_w[i][1]), \"outlinks: \", \n",
    "          len(out_links_wt2g[sorted_page_rankings_w[i][0]]), \"inlinks: \", len(in_links_wt2g[sorted_page_rankings_w[i][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save top 500 to file\n",
    "file_name = \"wt2g_pagerank_scores.txt\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "wt2g_pr = open(file_name, \"w\")\n",
    "for i in range(500):\n",
    "    wt2g_pr.write(sorted_page_rankings_w[i][0] + \"\\t\" + str(sorted_page_rankings_w[i][1]) + \"\\toutlinks: \" + str(len(out_links_wt2g[sorted_page_rankings_w[i][0]])) + \"\\tinlinks: \" + str(len(in_links_wt2g[sorted_page_rankings_w[i][0]])) + \"\\n\")\n",
    "wt2g_pr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Part 3: HITS from crawl\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# A. Create a root set: Obtain the root set of about 1000 documents by ranking all pages using an IR function \n",
    "# (e.g. BM25, ES Search). You will need to use your topic as your query\n",
    "\n",
    "host='https://elastic:cwHN1LsyXbAGmb5LxCbADTkj@cs6200.es.us-west1.gcp.cloud.es.io:9243'\n",
    "\n",
    "es = Elasticsearch([host],timeout=3000)\n",
    "print(es.ping())\n",
    "ic = IndicesClient(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unit', 'state', 'battl', 'world', 'war', 'ii']\n"
     ]
    }
   ],
   "source": [
    "# get the query ready for elasticsearch\n",
    "\n",
    "\"\"\"\n",
    "Function: query_analyzer()\n",
    "Input: The full query as a string (one or more words)\n",
    "Output: A list of strings where each string is one word (token) of the query\n",
    "\"\"\"\n",
    "def query_analyzer(query):\n",
    "    body = {\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\": [\"english_stemmer\", \"lowercase\", \"english_stop\"],\n",
    "        \"text\": query\n",
    "    }\n",
    "    response = ic.analyze(body=body, index=\"corpus_wwii\")\n",
    "    cleaned_queries = [list[\"token\"] for list in response[\"tokens\"]]\n",
    "    return cleaned_queries\n",
    "\n",
    "q = 'united states battles world war ii'\n",
    "query_clean = query_analyzer(q)\n",
    "print(query_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES-Built in finished running!\n"
     ]
    }
   ],
   "source": [
    "# search elastic search for the documents, sort them, and save them to a file\n",
    "\n",
    "\"\"\"\n",
    "Function: write_scores_to_file_es()\n",
    "Input: A dictionary of query responses (documents returned for each query) and a name for the file\n",
    "Output: None\n",
    "Does: Writes a file for the output to ES built in model. Scores will already by sorted.\n",
    "For each query response, writes a line for each document that was returned that includes the query number,\n",
    "doc number, rank, and score. Each line should be of the form: <query-number> Q0 <docno> <rank> <score> Exp\n",
    "\"\"\"\n",
    "def write_scores_to_file_es(response_dict, name):\n",
    "    # assumes scores are already sorted\n",
    "    file_name = name + \".txt\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    output = open(file_name, \"w\")\n",
    "\n",
    "    # iterate over the response_dict for each query (maps query number from input to response dict)\n",
    "    # response[\"hits\"][\"hits\"] is a list of dicts for each doc with keys:\n",
    "    # _id, _score, _source (dict of keys \"file_name\", \"text\")\n",
    "    for q_id, response in response_dict.items():\n",
    "        query_number = q_id\n",
    "        rank = 1\n",
    "        for doc in response[\"hits\"][\"hits\"]:\n",
    "            docno = doc[\"_id\"]\n",
    "            score = doc[\"_score\"]\n",
    "            new_line = query_number + \" Q0 \" + docno + \" \" + str(rank) + \" \" + str(score) + \" Exp\\n\"\n",
    "            output.write(new_line)\n",
    "            rank += 1\n",
    "    output.close()\n",
    "\n",
    "\"\"\"\n",
    "Model: ES Built-in\n",
    "Input: A dictionary of queries where their ID is mapped to a list of the queries as a string, each token separated\n",
    "by a single whitespace\n",
    "Returns: A dictionary of the responses provided by ES for each query\n",
    "Does: Iterates through each query and saves the HIT responses in a response dictionary. Max 1000 hits per query\n",
    "\"\"\"\n",
    "def es_built_in(query_dict):\n",
    "    responses = {}\n",
    "    for _id, query in query_dict.items():\n",
    "        query = \" \".join(query)\n",
    "        query_body = {\n",
    "            \"size\": 1000,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        response = es.search(index=\"corpus_wwii\", body=query_body)\n",
    "        responses[_id] = response\n",
    "    return responses\n",
    "\n",
    "# run model and write to file\n",
    "q_dict = {\"1\" : query_clean}\n",
    "r = es_built_in(q_dict)\n",
    "write_scores_to_file_es(r, \"es_built_in_results\")\n",
    "print(\"ES-Built in finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are starting with 1000 docs\n",
      "We now have a full root set of 12120 docs\n"
     ]
    }
   ],
   "source": [
    "# B. Repeat few two or three time this expansion to get a base set of about 10,000 pages:\n",
    "    # For each page in the set, add all pages that the page points to\n",
    "    # For each page in the set, obtain a set of pages that pointing to the page\n",
    "    # if the size of the set is less than or equal to d, add all pages in the set to the root set\n",
    "    # if the size of the set is greater than d, add an RANDOM (must be random) set of d pages from the set to the root set\n",
    "    # Note: The constant d can be 200. The idea of it is trying to include more possibly strong hubs into the root set \n",
    "    #     while constraining the size of the root size.\n",
    "    \n",
    "\"\"\"\n",
    "Function: read_root_set\n",
    "Input: Path to the results from es search\n",
    "Output: A list of the names of the top 1000 urls from the search\n",
    "\"\"\"\n",
    "def read_root_set(path):\n",
    "    data = []\n",
    "    for line in open(path, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        line_list = line.split()\n",
    "        \n",
    "        data += [line_list[2]]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: expand_root_set\n",
    "Input: the root set, the inlink and outlink dictionaries\n",
    "Output: the expanded base set (~10,000 docs)\n",
    "\"\"\"\n",
    "def expand_root_set(r, inlinks, outlinks):\n",
    "    \n",
    "    # the final set\n",
    "    expanded_set = set(r)\n",
    "    d = 200\n",
    "    \n",
    "    # the latest layer that was added (starts with root layer)\n",
    "    new_set = set(r)\n",
    "    while len(expanded_set) < 10000:\n",
    "        \n",
    "        # initialize what we will be adding this time\n",
    "        to_add = set([])\n",
    "        \n",
    "        # for each page in latest layer that was added, add in and out links\n",
    "        # if we reach 10000 during this process, break\n",
    "        for each in new_set:\n",
    "            outs = outlinks[each]\n",
    "            to_add.update(outs)\n",
    "            \n",
    "            ins = set(inlinks[each])\n",
    "            if len(ins) <= d:\n",
    "                to_add.update(ins)\n",
    "            else:\n",
    "                to_add.update(random.sample(ins, d))\n",
    "                \n",
    "            if len(expanded_set) + len(to_add) >= 10000:\n",
    "                break\n",
    "                \n",
    "        # once for loop stops, add new layer to full set, set the last layer to what we just found\n",
    "        expanded_set.update(to_add)\n",
    "        new_set = to_add\n",
    "    \n",
    "    return list(expanded_set)\n",
    "        \n",
    "\n",
    "# create root set\n",
    "root = read_root_set(\"C:/6200-IR/homework-4-mplatt27/es_built_in_results.txt\")\n",
    "print(\"We are starting with {} docs\".format(len(root)))\n",
    "base = expand_root_set(root, in_links, out_links)\n",
    "print(\"We now have a full root set of {} docs\".format(len(base)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write base set to file to check\n",
    "def write_base_set(b):\n",
    "    # assumes scores are already sorted\n",
    "    file_name = \"base_set.txt\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    output = open(file_name, \"w\")\n",
    "\n",
    "    for each in b:\n",
    "        output.write(each + \"\\n\")\n",
    "    output.close()\n",
    "    \n",
    "write_base_set(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Compute HITS. For each web page, initialize its authority and hub scores to 1. Update hub and authority \n",
    "# scores for each page in the base set until convergence \n",
    "\n",
    "def compute_hits(r, b, inlinks, outlinks):\n",
    "    \n",
    "    # we may have outlinks that we didn't crawl, so we don't have the in and outlinks for them\n",
    "    check_set = set(b)\n",
    "    \n",
    "    # initialize scores to 1\n",
    "    hub_scores = {}\n",
    "    authority_scores = {}\n",
    "    for each in b:\n",
    "        hub_scores[each] = 1\n",
    "        authority_scores[each] = 1\n",
    "        \n",
    "    # initialize pp\n",
    "    old_pp_a = calc_perplexity(authority_scores)\n",
    "    old_pp_h = calc_perplexity(hub_scores)\n",
    "    print(\"old ppa: \", old_pp_a)\n",
    "    print(\"old pph: \", old_pp_h)\n",
    "    \n",
    "    j = 0\n",
    "    while j < 1:\n",
    "        \n",
    "        # update authority scores and normalize\n",
    "        norm = 0\n",
    "        new_authority_scores = {}\n",
    "        for each in b:\n",
    "            new_authority_scores[each] = 0\n",
    "            if inlinks.get(each,0):\n",
    "                ins = inlinks[each]\n",
    "                for q in ins:\n",
    "                    if q in check_set:\n",
    "                        new_authority_scores[each] += hub_scores[q]\n",
    "            if new_authority_scores[each] == 0:\n",
    "                new_authority_scores[each] = 1\n",
    "            norm += (new_authority_scores[each])**2\n",
    "        norm = math.sqrt(norm)\n",
    "        for key, value in new_authority_scores.items():\n",
    "            new_authority_scores[key] = value / norm\n",
    "            \n",
    "        # update all hub scores and normalize\n",
    "        norm = 0\n",
    "        new_hub_scores = {}\n",
    "        for each in b:\n",
    "            new_hub_scores[each] = 0\n",
    "            if outlinks.get(each,0):\n",
    "                outs = outlinks[each]\n",
    "                for q in outs:\n",
    "                    if q in check_set:\n",
    "                        new_hub_scores[each] += authority_scores[q]\n",
    "            if new_hub_scores[each] == 0:\n",
    "                new_hub_scores[each] = 1\n",
    "            norm += (new_hub_scores[each])**2\n",
    "        norm = math.sqrt(norm)\n",
    "        for key, value in new_hub_scores.items():\n",
    "            new_hub_scores[key] = value / norm\n",
    "            \n",
    "        # update dictionaries    \n",
    "        for each in b:\n",
    "            hub_scores[each] = new_hub_scores[each]\n",
    "            authority_scores[each] = new_authority_scores[each]\n",
    "            \n",
    "        # calc perplexity to check for convergence\n",
    "        new_pp_a = calc_perplexity(authority_scores)\n",
    "        new_pp_h = calc_perplexity(hub_scores)\n",
    "        \n",
    "        print(\"a diff: \", abs(old_pp_a - new_pp_a))\n",
    "        print(\"h diff: \", abs(old_pp_h - new_pp_h))\n",
    "\n",
    "        if abs(old_pp_a - new_pp_a) < 1 and abs(old_pp_h - new_pp_h) < 1:\n",
    "            j += 1\n",
    "            print(\"increment j\")\n",
    "            \n",
    "        # update pp\n",
    "        old_pp_a = new_pp_a\n",
    "        old_pp_h = new_pp_h\n",
    "\n",
    "                \n",
    "    return hub_scores, authority_scores\n",
    "\n",
    "# get hub and authority scores\n",
    "h, a = compute_hits(root, base, in_links, out_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort and print top x results\n",
    "sorted_h = sorted(h.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_a = sorted(a.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to files\n",
    "file_name = \"hub_scores.txt\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "hub_out = open(file_name, \"w\")\n",
    "\n",
    "file_name = \"auth_scores.txt\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "auth_out = open(file_name, \"w\")\n",
    "for i in range(500):\n",
    "    hub_out.write(sorted_h[i][0] + \"\\t\" + str(sorted_h[i][1]) + \"\\n\")\n",
    "    auth_out.write(sorted_a[i][0] + \"\\t\" + str(sorted_a[i][1]) + \"\\n\")\n",
    "    \n",
    "hub_out.close()\n",
    "auth_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hubs --> has good links to other pages\n",
    "# authority --> is linked to by many hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
