{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.client import IndicesClient\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import operator\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "ic = IndicesClient(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Part 1: Read in queries\n",
    "\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: query_analyzer()\n",
    "# Input: The full query as a string (one or more words)\n",
    "# Output: A list of strings where each string is one word (token) of the query\n",
    "def query_analyzer(query):\n",
    "    body = {\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\": [\"english_stemmer\", \"lowercase\", \"english_stop\"],\n",
    "        \"text\": query\n",
    "    }\n",
    "    response = ic.analyze(body=body, index=\"ap_dataset\")\n",
    "    cleaned_queries = [list[\"token\"] for list in response[\"tokens\"]]\n",
    "    return cleaned_queries\n",
    "\n",
    "\n",
    "# Function: read_queries()\n",
    "# Input: The folder path to the queries file as a string\n",
    "# Output: A dictionary mapping each query ID to a list of terms in that query (as str)\n",
    "def read_queries(folder_path):\n",
    "    # iterate over each line in the query\n",
    "    lines = []\n",
    "    ids = []\n",
    "    for line in open(folder_path, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        curr_query = str(line)\n",
    "        id_end = curr_query.find(\".\")\n",
    "        id = curr_query[:id_end].strip()\n",
    "        ids.append(id)\n",
    "        curr_query = curr_query[id_end + 3:].strip()\n",
    "        lines.append(curr_query)\n",
    "\n",
    "    # put them through the analyzer\n",
    "    cleaned_queries = {}\n",
    "    for i in range(len(lines)):\n",
    "        # cleaned_query will be a list of the query words as strings\n",
    "        cleaned_query = query_analyzer(lines[i])\n",
    "        if cleaned_query:\n",
    "            cleaned_queries[ids[i]] = cleaned_query\n",
    "    return cleaned_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ['alleg', 'corrupt', 'public', 'offici', 'govern']\n",
      "59 ['weather', 'caus', 'fatal']\n",
      "56 ['predict', 'prime', 'lend', 'rate', 'prime', 'rate', 'move']\n",
      "71 ['incurs', 'border', 'militari', 'forc', 'guerrilla']\n",
      "64 ['polit', 'hostag']\n",
      "62 ['militari', 'coup', \"d'etat\"]\n",
      "93 ['support', 'nation', 'rifl', 'associat', 'nra']\n",
      "99 ['iran', 'contra', 'affair']\n",
      "58 ['rail', 'strike']\n",
      "77 ['poach', 'wildlif']\n",
      "54 ['contract', 'preliminari', 'agreement', 'tent', 'reserv', 'launch', 'commerci', 'satellit']\n",
      "87 ['crimin', 'offic', 'fail', 'u.s', 'financi', 'institut']\n",
      "94 ['crime', 'aid', 'comput']\n",
      "100 ['non', 'communist', 'industri', 'state', 'regul', 'transfer', 'high', 'tech', 'good', 'technolog', 'undesir', 'nation']\n",
      "89 ['exist', 'pend', 'invest', 'opec', 'member', 'state', 'ani', 'downstream', 'oper']\n",
      "61 ['israel', 'iran', 'contra', 'affair']\n",
      "95 ['comput', 'crime', 'solv']\n",
      "68 ['studi', 'safeti', 'manufactur', 'employe', 'instal', 'worker', 'fine', 'diamet', 'fiber', 'insul']\n",
      "57 ['mci', 'bell', 'system', 'breakup']\n",
      "97 ['fiber', 'optic', 'technolog', 'actual']\n",
      "98 ['individu', 'organ', 'produc', 'fiber', 'optic', 'equip']\n",
      "60 ['perform', 'salari', 'incent', 'pai', 'determin', 'senior']\n",
      "80 ['1988', 'presidenti', 'candid']\n",
      "63 ['machin', 'translat', 'system']\n",
      "91 ['army', 'advanc', 'weapon', 'system']\n"
     ]
    }
   ],
   "source": [
    "# Main code to get the queries and print them out\n",
    "queries_path = \"C:/6200-IR/homework1-mplatt27/IR_data/AP_DATA/queries_modified_x.txt\"\n",
    "queries = read_queries(queries_path)\n",
    "for key, value in queries.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Part 2: Retrieval models\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "\n",
    "# Function: get_all_docs()\n",
    "# Input: None\n",
    "# Output: A list of all doc_ids in the index\n",
    "# Does: Uses es search() API to retrieve the document ids of each doc in the index. Uses scroll to do this.\n",
    "def get_all_docs():\n",
    "    # initialize list that will store all doc ids that we need to return\n",
    "    doc_ids = []\n",
    "\n",
    "    # start the search process, we will then scroll until we have all docs\n",
    "    query_body = {\n",
    "        \"size\": 10000,\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"ap_dataset\", body=query_body, scroll='3m')\n",
    "    old_scroll_id = response['_scroll_id']\n",
    "\n",
    "    for doc in response['hits']['hits']:\n",
    "        doc_ids.append(doc['_id'])\n",
    "\n",
    "    while len(response['hits']['hits']):\n",
    "        response = es.scroll(scroll_id=old_scroll_id, scroll='3m')\n",
    "        old_scroll_id = response['_scroll_id']\n",
    "        for doc in response['hits']['hits']:\n",
    "            doc_ids.append(doc['_id'])\n",
    "\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "# Function: get_term_vectors\n",
    "# Input: A list of document ids\n",
    "# Output: Dictionary mapping doc-ids --> term vector for that document\n",
    "# Does: Collects the term vector for each document in the input list using the es termvectors API. Returns as dict.\n",
    "# This output takes a very long time, so we will dump to pickle and load it in each time to run the models.\n",
    "def get_term_vectors(doc_ids):\n",
    "    vector_per_doc = {}\n",
    "    for doc in doc_ids:\n",
    "        tv = es.termvectors(index=\"ap_dataset\", id=doc, fields=\"text\", term_statistics=True)\n",
    "        vector_per_doc[doc] = tv\n",
    "\n",
    "    return vector_per_doc\n",
    "\n",
    "\n",
    "# Function: sort_scores_dict()\n",
    "# Input: scores, a dictionary that maps query # to a dictionary (docno --> score)\n",
    "# Output: The same dictionary, but the value (dict that each key maps to) is now sorted by the scores\n",
    "def sort_scores_dict(scores):\n",
    "    for q_id, d in scores.items():\n",
    "        sorted_dict = dict(sorted(d.items(), key=operator.itemgetter(1), reverse=True))\n",
    "        scores[q_id] = sorted_dict\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Function: write_scores_to_file_es()\n",
    "# Input: A dictionary of query responses (documents returned for each query) and a name for the file\n",
    "# Output: None\n",
    "# Does: Writes a file for the output to ES built in model. Scores will already by sorted.\n",
    "# For each query response, writes a line for each document that was returned that includes the query number,\n",
    "# doc number, rank, and score. Each line should be of the form: <query-number> Q0 <docno> <rank> <score> Exp\n",
    "def write_scores_to_file_es(response_dict, name):\n",
    "    # assumes scores are already sorted\n",
    "    file_name = name + \".txt\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    output = open(file_name, \"w\")\n",
    "\n",
    "    # iterate over the response_dict for each query (maps query number from input to response dict)\n",
    "    # response[\"hits\"][\"hits\"] is a list of dicts for each doc with keys:\n",
    "    # _id, _score, _source (dict of keys \"file_name\", \"text\")\n",
    "    for q_id, response in response_dict.items():\n",
    "        query_number = q_id\n",
    "        rank = 1\n",
    "        for doc in response[\"hits\"][\"hits\"]:\n",
    "            docno = doc[\"_id\"]\n",
    "            score = doc[\"_score\"]\n",
    "            new_line = query_number + \" Q0 \" + docno + \" \" + str(rank) + \" \" + str(score) + \" Exp\\n\"\n",
    "            output.write(new_line)\n",
    "            rank += 1\n",
    "    output.close()\n",
    "\n",
    "\n",
    "# Function: write_scores_to_file()\n",
    "# Input: A dictionary of scored documents for each query and a name for the file\n",
    "# Output: None\n",
    "# Does: Writes a file for the output. Assumes scores are already sorted. For each query response, writes a line\n",
    "# for each document that was returned that includes the query number, doc number, rank, and score.\n",
    "# Each line should be of the form: <query-number> Q0 <docno> <rank> <score> Exp\n",
    "# This is for all models, except the ES built in, due to the differing format of results.\n",
    "def write_scores_to_file(scores, name):\n",
    "    # assumes scores are already sorted\n",
    "    # scores is dict of query id --> dict (doc_id --> score)\n",
    "    file_name = name + \".txt\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    output = open(file_name, \"w\")\n",
    "\n",
    "    # iterate over query id responses\n",
    "    for q_id, dict in scores.items():\n",
    "        query_number = q_id\n",
    "        rank = 1\n",
    "        for doc_id, score in dict.items():\n",
    "            if rank > 1000:\n",
    "                break\n",
    "            new_line = query_number + \" \" + \"Q0\" + \" \" + doc_id + \" \" + str(rank) + \" \" + str(score) + \" Exp\\n\"\n",
    "            output.write(new_line)\n",
    "            rank += 1\n",
    "    output.close()\n",
    "    \n",
    "\n",
    "# Function: vocab_size\n",
    "# Input: None\n",
    "# Output: size of vocab for index\n",
    "def vocab_size():\n",
    "    req = {\n",
    "        'aggs': {\n",
    "            \"vocabSize\": {\n",
    "                \"cardinality\": {\n",
    "                    \"field\": \"text\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    resp = es.search(index=\"ap_dataset\", body=req)\n",
    "    v = resp[\"aggregations\"][\"vocabSize\"][\"value\"]\n",
    "    return v\n",
    "\n",
    "\n",
    "# Function: create_terms_dict()\n",
    "# Input: term vectors dictionary that maps doc id --> tvs (maps doc id --> tv, dict with \n",
    "#        keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "# Output: terms dictionary that  will map terms (str) --> dict of info: doc_freq, ttf\n",
    "def create_terms_dict(tvs):\n",
    "    \n",
    "    # terms_dict will map terms (str) --> dict of info: doc_freq, ttf; will be size V\n",
    "    terms_dict = {}\n",
    "    \n",
    "    for doc_id, tv in tvs.items():\n",
    "        terms = tv[\"term_vectors\"][\"text\"][\"terms\"]\n",
    "        for term, info in terms.items():\n",
    "            term = term.strip()\n",
    "            if term not in terms_dict.keys():\n",
    "                df = info[\"doc_freq\"]\n",
    "                ttf = info[\"ttf\"]\n",
    "                terms_dict[term] = {\"doc_freq\": df, \"ttf\": ttf}\n",
    "    \n",
    "    return terms_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval Models\n",
    "\"\"\"\n",
    "\n",
    "# Model 1: ES Built-in\n",
    "# Input: A dictionary of queries where their ID is mapped to a list of the queries as a string, each token separated\n",
    "# by a single whitespace\n",
    "# Returns: A dictionary of the responses provided by ES for each query\n",
    "# Does: Iterates through each query and saves the HIT responses in a response dictionary. Max 1000 hits per query\n",
    "def es_built_in(query_dict):\n",
    "    responses = {}\n",
    "    for id, query in query_dict.items():\n",
    "        query = \" \".join(query)\n",
    "        query_body = {\n",
    "            \"size\": 1000,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        response = es.search(index=\"ap_dataset\", body=query_body)\n",
    "        responses[id] = response\n",
    "    return responses\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 2; Okapi TF\n",
    "\n",
    "# Function: okapi_tf_calc, a helper function to calculate score\n",
    "def okapi_tf_calc(tf, doc_len, avg_corp_len):\n",
    "    score = tf / (tf + 0.5 + (1.5 * (doc_len / avg_corp_len)))\n",
    "    return score\n",
    "\n",
    "\n",
    "# Input: A dictionary of term vectors for each document (doc-id --> term vector dict) and a dictionary\n",
    "# of queries (query id --> list of each word in the query as str\n",
    "# Returns: A scores dictionary (query id --> dictionary (doc-id --> score)\n",
    "# Does: Iterates through each document, and through each query term and calculates okapi-tf for the document-word\n",
    "# combination. Sums score and returns as dict.\n",
    "def okapi_tf(tvs, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    avg_doc_len = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"] / \\\n",
    "                                  tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]\n",
    "                    # calculate okapi tf\n",
    "                    temp_score = okapi_tf_calc(tf_value, doc_len, avg_doc_len)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    if doc_id not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_id] = temp_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_id] += temp_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 3: TF-IDF\n",
    "\n",
    "\n",
    "# Function for calculating tf-idf\n",
    "def tf_idf_calc(okapi_tf_score, total_docs, df):\n",
    "    return okapi_tf_score * math.log((total_docs/df), 2)\n",
    "\n",
    "\n",
    "# Function tf-idf\n",
    "def tf_idf(tvs, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    avg_doc_len = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"] / \\\n",
    "                                  tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]\n",
    "                    # calculate okapi tf\n",
    "                    okapi_tf_score = okapi_tf_calc(tf_value, doc_len, avg_doc_len)\n",
    "                    total_docs = len(tvs)\n",
    "                    df = tf['doc_freq']\n",
    "                    tf_idf_score = tf_idf_calc(okapi_tf_score, total_docs, df)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    if doc_id not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_id] = tf_idf_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_id] += tf_idf_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 4: Okapi BM25\n",
    "\n",
    "\n",
    "# Function for calculating BM25\n",
    "def okapi_BM25_calc(total_docs, df, tf_value, tf_query, doc_len, avg_doc_len, k_1, k_2, b):\n",
    "    calc1_num = total_docs + 0.5\n",
    "    calc1_den = df + 0.5\n",
    "    calc1 = math.log((calc1_num/calc1_den), 2)\n",
    "\n",
    "    calc2_num = tf_value + (k_1 * tf_value)\n",
    "    cal2_den = tf_value + k_1 * ((1-b) + (b * (doc_len/avg_doc_len)))\n",
    "    calc2 = calc2_num/cal2_den\n",
    "\n",
    "    calc3_num = tf_query + (k_2 * tf_query)\n",
    "    calc3_den = tf_query + k_2\n",
    "    calc3 = calc3_num/calc3_den\n",
    "\n",
    "    return calc1 * calc2 * calc3\n",
    "\n",
    "\n",
    "# Function: Okapi BM25\n",
    "def okapi_BM25(tvs, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # maps the query # --> Counter (for each query)\n",
    "    queries_counter = {}\n",
    "    # populate with query ids mapped to empty dict and queries counter with tf_queries\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "        queries_counter[q_id] = Counter()\n",
    "        for word in query:\n",
    "            queries_counter[q_id][word] += 1\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "\n",
    "                    # for first block of equation\n",
    "                    total_docs = len(tvs)\n",
    "                    df = tf['doc_freq']\n",
    "\n",
    "                    # for second block of equation\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    avg_doc_len = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"] / \\\n",
    "                                  tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]\n",
    "\n",
    "                    # for third block of equation, number of times the word occurs in the query\n",
    "                    tf_query = queries_counter[q_id][word]\n",
    "\n",
    "                    # constants\n",
    "                    k_1 = 1.2\n",
    "                    k_2 = 1.2\n",
    "                    b = 0.75\n",
    "\n",
    "                    # calculate okapi BM25\n",
    "                    okapi_BM25_score = okapi_BM25_calc(total_docs, df, tf_value, tf_query, doc_len, avg_doc_len, k_1, k_2, b)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    if doc_id not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_id] = okapi_BM25_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_id] += okapi_BM25_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 5: Unigram LM with Laplace smoothing\n",
    "\n",
    "\n",
    "# Function for laplace calculation\n",
    "def p_laplace_calc(tf_value, doc_len, v):\n",
    "    return (tf_value + 1) / (doc_len + v)\n",
    "\n",
    "\n",
    "# Function: Unigram LM with Laplace smoothing\n",
    "def unigram_lm_laplace(tvs, query_dict):\n",
    "    \n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    \n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "\n",
    "    # get vocabulary size from es search API, will be used in calculation\n",
    "    v = vocab_size()\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    p_laplace_score = p_laplace_calc(tf_value, doc_len, v)\n",
    "                    score = math.log(p_laplace_score)\n",
    "                else:\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    p_laplace_score = p_laplace_calc(0, doc_len, v)\n",
    "                    score = math.log(p_laplace_score)\n",
    "\n",
    "                # add score to dictionary\n",
    "                if doc_id not in scores[q_id].keys():\n",
    "                    scores[q_id][doc_id] = score\n",
    "                else:\n",
    "                    scores[q_id][doc_id] += score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 6: Unigram LM with Jelinek-Mercer smoothing\n",
    "\n",
    "\n",
    "# Caluclation for p_jm when the term is in the current document\n",
    "def p_jm_calc(tf_value, doc_len, ttf, sum_ttf, v):\n",
    "    lambda_val = 0.9\n",
    "    calc1 = lambda_val * (tf_value/doc_len)\n",
    "    calc2 = (1-lambda_val) * (ttf / v)\n",
    "#     calc2 = (1-lambda_val) * ((ttf - tf_value) / (sum_ttf - doc_len))\n",
    "    return calc1 + calc2\n",
    "\n",
    "\n",
    "# Caluclation for p_jm when the term is not in the current document\n",
    "def p_jm_calc_term_not_present(ttf, v):\n",
    "    lambda_val = 0.9\n",
    "    return (1-lambda_val) * (ttf / v)\n",
    "\n",
    "\n",
    "# Function: Unigram LM with Jelinek-Mercer smoothing\n",
    "def unigram_lm_jm(tvs, query_dict, terms_dict):\n",
    "    \n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    \n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "        \n",
    "    # get vocabulary size from es search API, will be used in calculation\n",
    "    v = vocab_size()\n",
    "\n",
    "    # iterate over each document\n",
    "    # tv is a dict with keys: _index, _type, _id, _version, _found, took, term_vectors\n",
    "    for doc_id, tv in tvs.items():\n",
    "        # now iterate over each query that we have\n",
    "        for q_id, query in query_dict.items():\n",
    "            # iterate over each word in the query that we have to check with the current document\n",
    "            for word in query:\n",
    "                if word in tv[\"term_vectors\"][\"text\"][\"terms\"].keys():\n",
    "                    tf = tv[\"term_vectors\"][\"text\"][\"terms\"].get(word, 0)\n",
    "                    tf_value = tf['term_freq']\n",
    "                    doc_len = sum(map(lambda doc_length_term: doc_length_term['term_freq'],\n",
    "                                      tv[\"term_vectors\"][\"text\"][\"terms\"].values()))\n",
    "                    ttf = tf['ttf']\n",
    "                    sum_ttf = tv[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"]\n",
    "                    p_jm_score = p_jm_calc(tf_value, doc_len, ttf, sum_ttf, v)\n",
    "                    score = math.log(p_jm_score)\n",
    "                else:\n",
    "                    # case where word is not in the doc, we still need to account for that\n",
    "                    term_info = terms_dict.get(word, 0)\n",
    "                    ttf = 100\n",
    "                    if term_info != 0:\n",
    "                        ttf = int(term_info['ttf'])\n",
    "                    p_jm_score = p_jm_calc_term_not_present(ttf, v)\n",
    "                    score = math.log(p_jm_score)\n",
    "\n",
    "                # add score to dictionary\n",
    "                if doc_id not in scores[q_id].keys():\n",
    "                    scores[q_id][doc_id] = score\n",
    "                else:\n",
    "                    scores[q_id][doc_id] += score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code for running the models and saving output to file\n",
    "# ONLY RUN IF FIRST TIME USING THIS: Create pickle file with all termvectors for each doc\n",
    "\n",
    "\n",
    "# 1. Use search API to get ids of all docs, this will be used to get term vectors\n",
    "# return_ids = get_all_docs()\n",
    "# print(\"We got responses for this many docs: \", len(return_ids))\n",
    "\n",
    "# 2. Create dictionary that will store term vectors\n",
    "# return_term_vectors = get_term_vectors(return_ids)\n",
    "# print(\"We got term vectors for this many docs: \", len(return_term_vectors))\n",
    "\n",
    "# 3. Save term vectors in pickle file\n",
    "# with open('termvectorswstats.pickle', 'wb') as handle:\n",
    "#     pickle.dump(return_term_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file has opened\n",
      "There are this many documents to search:  84678\n"
     ]
    }
   ],
   "source": [
    "# IF RUNNING AFTER CREATING PICKLE FILE:\n",
    "\n",
    "\n",
    "# Load pickle file that contains term vectors for each doc\n",
    "handle = open('C:/6200-IR/homework1-mplatt27/config/termvectorswstats.pickle', 'rb')\n",
    "return_term_vectors = pickle.load(handle)\n",
    "handle.close()\n",
    "print(\"Pickle file has opened\")\n",
    "print(\"There are this many documents to search: \", len(return_term_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are this many terms:  180393\n"
     ]
    }
   ],
   "source": [
    "# Create terms dictionary from the term vectors. This maps the terms to info that does not change (ttf and doc_freq)\n",
    "# This is needed for the Unigram LM with Jelinek-Mercer Smoothing model\n",
    "terms_stats_dict = create_terms_dict(return_term_vectors)\n",
    "print(\"There are this many terms: \", len(terms_stats_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES-Built in finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 1:\n",
    "hits = es_built_in(queries)\n",
    "write_scores_to_file_es(hits, \"es_built_in_results\")\n",
    "print(\"ES-Built in finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi-TF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 2:\n",
    "doc_scores = okapi_tf(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_tf_results\")\n",
    "print(\"Okapi-TF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 3:\n",
    "doc_scores = tf_idf(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"tfidf_results\")\n",
    "print(\"TF-IDF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi BM25 finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 4:\n",
    "doc_scores = okapi_BM25(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_bm25_results\")\n",
    "print(\"Okapi BM25 finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Laplace Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 5:\n",
    "doc_scores = unigram_lm_laplace(return_term_vectors, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_laplace_results\")\n",
    "print(\"Unigram LM with Laplace Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Jelinek-Mercer Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 6:\n",
    "doc_scores = unigram_lm_jm(return_term_vectors, queries, terms_stats_dict)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_jm_results\")\n",
    "print(\"Unigram LM with Jelinek-Mercer Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Part 3: Pseudo-Relevance Feedback\n",
    "\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************************************************************************* #\n",
    "# PSEUDO-RELEVANCE FEEDBACK\n",
    "\n",
    "# This section will attempt to improve the current models. The first try does this by retrieving\n",
    "# The top X documents for each query. For each query, we then score each term in those top X documents\n",
    "# using tf-idf. We then take the top X terms from that group of documents (for each query separately).\n",
    "# Those top X query terms are appended to the existing query terms, and the models are re-run.\n",
    "\n",
    "\n",
    "# This function gets the top x documents for each query (currently 1)\n",
    "# Returns as dict (query id --> list of docs)\n",
    "def get_top_docs(folder_path):\n",
    "    top_docs = {}  # query # --> list of doc ids\n",
    "\n",
    "    for line in open(folder_path, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        curr_line = str(line)\n",
    "        curr_line_list = curr_line.split(\" \")\n",
    "        q_id = curr_line_list[0]\n",
    "        doc_id = curr_line_list[2]\n",
    "        rank = curr_line_list[3]\n",
    "        if int(rank) <= 1:\n",
    "            if q_id not in top_docs.keys():\n",
    "                temp = [doc_id]\n",
    "                top_docs[q_id] = temp\n",
    "\n",
    "            else:\n",
    "                top_docs[q_id].append(doc_id)\n",
    "\n",
    "    return top_docs\n",
    "\n",
    "\n",
    "# This function calculates the tfidf score for each term in the top documents\n",
    "# Returns them as a dict (query id --> dict of terms (term name --> tfidf score)\n",
    "def get_all_top_terms(results, tvs):\n",
    "\n",
    "    query_term_calcs = {} # q id --> term calcs dict\n",
    "    # iterate for each query\n",
    "    for q_id, docs in results.items():\n",
    "        # iterate over each top doc\n",
    "        term_calcs = {} # term --> tfidf score\n",
    "        for doc in docs:\n",
    "            # get term vector for that document\n",
    "            terms = tvs[doc][\"term_vectors\"][\"text\"][\"terms\"]\n",
    "            for term, info in terms.items():\n",
    "                tf = info['term_freq']\n",
    "                d = 84678\n",
    "                df = info['doc_freq']\n",
    "                score = math.log(1 + tf, 2) * math.log(d / df)  # log(1+tf) * log(D/df)\n",
    "                if term not in term_calcs.keys():\n",
    "                    term_calcs[term] = score\n",
    "                else:\n",
    "                    if term_calcs[term] < score:\n",
    "                        term_calcs[term] = score\n",
    "        query_term_calcs[q_id] = term_calcs\n",
    "    return query_term_calcs\n",
    "\n",
    "\n",
    "# This function finds the top terms from the dictionary\n",
    "# Returns a new dict (q id --> list of top x terms (currently x)\n",
    "def get_top_x_terms(term_calcs):\n",
    "\n",
    "    most_common_terms = {}  # q id --> list of terms\n",
    "    for q_id, d in term_calcs.items():\n",
    "        k = Counter(d)\n",
    "        top = k.most_common(3)\n",
    "        most_common_terms[q_id] = top\n",
    "\n",
    "    return most_common_terms\n",
    "\n",
    "\n",
    "# This function adds the top query terms to the existing queries\n",
    "# returns as new query dict (q id --> list of queries (words))\n",
    "def add_terms_to_queries(new, queries):\n",
    "    # queries: q id --> list of terms\n",
    "    updated_queries = {}\n",
    "    for q_id, query_list in queries.items():\n",
    "        new_list_tuples = new[q_id] # list of tuples\n",
    "        new_list = []\n",
    "        for word in new_list_tuples:\n",
    "            new_list.append(word[0])\n",
    "        new_list = new_list + query_list\n",
    "        updated_queries[q_id] = new_list\n",
    "\n",
    "    return updated_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the new queries\n",
      "85 ['roh', 'chun', 'referendum', 'alleg', 'corrupt', 'public', 'offici', 'govern']\n",
      "59 ['tornado', 'twister', '_at', 'weather', 'caus', 'fatal']\n",
      "56 ['economist', 'eas', 'fed', 'predict', 'prime', 'lend', 'rate', 'prime', 'rate', 'move']\n",
      "71 ['swapo', 'namibia', 'angola', 'incurs', 'border', 'militari', 'forc', 'guerrilla']\n",
      "64 ['nahar', 'hostag', 'unfulfil', 'polit', 'hostag']\n",
      "62 ['giroldi', 'oleochea', 'coup', 'militari', 'coup', \"d'etat\"]\n",
      "93 ['nra', 'bennett', \"nra'\", 'support', 'nation', 'rifl', 'associat', 'nra']\n",
      "99 ['mees', 'contra', 'iran', 'iran', 'contra', 'affair']\n",
      "58 ['rail', 'railwaymen', 'dock', 'rail', 'strike']\n",
      "77 ['gator', 'allig', 'doxei', 'poach', 'wildlif']\n",
      "54 ['marcopolo', 'satellit', 'mcdonnel', 'contract', 'preliminari', 'agreement', 'tent', 'reserv', 'launch', 'commerci', 'satellit']\n",
      "87 ['fada', 'rtc', 'headhunt', 'crimin', 'offic', 'fail', 'u.s', 'financi', 'institut']\n",
      "94 ['shoben', 'mcafe', 'disk', 'crime', 'aid', 'comput']\n",
      "100 ['technolog', 'coher', 'curri', 'non', 'communist', 'industri', 'state', 'regul', 'transfer', 'high', 'tech', 'good', 'technolog', 'undesir', 'nation']\n",
      "89 ['baldwin', 'scam', 'swindler', 'exist', 'pend', 'invest', 'opec', 'member', 'state', 'ani', 'downstream', 'oper']\n",
      "61 ['mees', 'divers', \"meese'\", 'israel', 'iran', 'contra', 'affair']\n",
      "95 ['tekalp', \"tekalp'\", 'pixel', 'comput', 'crime', 'solv']\n",
      "68 ['semiconductor', 'easterl', 'sia', 'studi', 'safeti', 'manufactur', 'employe', 'instal', 'worker', 'fine', 'diamet', 'fiber', 'insul']\n",
      "57 ['mci', 'distanc', 'fcc', 'mci', 'bell', 'system', 'breakup']\n",
      "97 ['optic', 'swirbul', 'fiber', 'fiber', 'optic', 'technolog', 'actual']\n",
      "98 ['pco', 'kessler', 'optic', 'individu', 'organ', 'produc', 'fiber', 'optic', 'equip']\n",
      "60 ['124,400', 'salari', '7,200', 'perform', 'salari', 'incent', 'pai', 'determin', 'senior']\n",
      "80 ['fec', 'eiland', 'ongression', '1988', 'presidenti', 'candid']\n",
      "63 ['mcconnel', 'introductori', 'translat', 'machin', 'translat', 'system']\n",
      "91 ['schlumberg', 'weston', 'fairchild', 'army', 'advanc', 'weapon', 'system']\n"
     ]
    }
   ],
   "source": [
    "# Main code to run Pseudo-relevance feedback\n",
    "\n",
    "# Change path for whichever results are needed\n",
    "results_path = \"C:/6200-IR/homework1-mplatt27/IR_data/AP_DATA/okapi_tf_results.txt\"\n",
    "\n",
    "# Get top x documents for each query\n",
    "top_results = get_top_docs(results_path)\n",
    "\n",
    "# Using the top x documents, score the terms in them using tfidf\n",
    "top_terms = get_all_top_terms(top_results, return_term_vectors)\n",
    "\n",
    "# For each query, get the top x terms\n",
    "most_common = get_top_x_terms(top_terms)\n",
    "# for key, value in most_common.items():\n",
    "#     print(key, value)\n",
    "\n",
    "# Add those top x terms to the existing queries\n",
    "updated_queries = add_terms_to_queries(most_common, queries)\n",
    "print(\"Here are the new queries\")\n",
    "for key, value in updated_queries.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES-Built in finished running!\n"
     ]
    }
   ],
   "source": [
    "# Run the models again with the updated queries\n",
    "\n",
    "# run model 1:\n",
    "hits = es_built_in(updated_queries)\n",
    "write_scores_to_file_es(hits, \"es_built_in_results_pseudo1\")\n",
    "print(\"ES-Built in finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi-TF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 2:\n",
    "doc_scores = okapi_tf(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_tf_results_pseudo1\")\n",
    "print(\"Okapi-TF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 3:\n",
    "doc_scores = tf_idf(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"tfidf_results_pseudo1\")\n",
    "print(\"TF-IDF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi BM25 finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 4:\n",
    "doc_scores = okapi_BM25(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_bm25_results_pseudo1\")\n",
    "print(\"Okapi BM25 finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Laplace Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 5:\n",
    "doc_scores = unigram_lm_laplace(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_laplace_results_pseudo1\")\n",
    "print(\"Unigram LM with Laplace Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Jelinek-Mercer Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 6:\n",
    "doc_scores = unigram_lm_jm(return_term_vectors, updated_queries, terms_stats_dict)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_jm_results_pseudo1\")\n",
    "print(\"Unigram LM with Jelinek-Mercer Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ******************************************************************************************* #\n",
    "# PSEUDO-RELEVANCE FEEDBACK using ElasticSearch aggs \"significant terms\"\n",
    "\n",
    "# This is a second attempt to improve the models using pseudo-relevant feedback, specifically with the\n",
    "# es search() API. This process follows these steps:\n",
    "\n",
    "# 1. First search for significant terms for each word in the query. Then take the top terms X \n",
    "#    for each query (top being those that occur for more than one word in the query, if possible). \n",
    "# 2. Eliminate any that are stop words. \n",
    "# 3. Score those most frequent terms using IDF scores. Take the top X (2) terms from the IDF scoring. \n",
    "# 4. Add those words to the queries and run the models again. \n",
    "\n",
    "# Unlike the first pseudo-relevance feedback attempt which updates the queries for each model individually, \n",
    "# based on model output, this attempt looks at the quries alone and then applies that update to each model. \n",
    "\n",
    "\n",
    "# Get set of stop words to use in this procedure\n",
    "def get_stop_words(folder_path):\n",
    "    words = []\n",
    "    for line in open(folder_path, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        curr_word = str(line)\n",
    "        curr_word = curr_word.strip()\n",
    "        words.append(curr_word)\n",
    "        \n",
    "    return set(words)\n",
    "\n",
    "\n",
    "# This function gets the significant terms for each query using the search() API\n",
    "# Finds those that appear for multiple words in each query, and returns the three top\n",
    "# Returns dict (q id --> counter of terms)\n",
    "def get_sig_terms(queries):\n",
    "\n",
    "    terms_to_add = {}\n",
    "\n",
    "    # iterate over each query\n",
    "    for q_id, query_list in queries.items():\n",
    "        terms_to_add[q_id] = Counter()\n",
    "        for word in query_list:\n",
    "            body = {\n",
    "                \"query\": {\n",
    "                    \"terms\": {\"text\": [word]}\n",
    "                },\n",
    "                \"aggregations\": {\n",
    "                    \"significantCrimeTypes\": {\n",
    "                        \"significant_terms\": {\n",
    "                            \"field\": \"text\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"size\": 0\n",
    "            }\n",
    "            response = es.search(index=\"ap_dataset\", body=body)\n",
    "            terms = response['aggregations']['significantCrimeTypes']['buckets']  # list of dictionaries\n",
    "            for term in terms:\n",
    "                if term['key'] != word:\n",
    "                    terms_to_add[q_id][term['key']] += 1\n",
    "\n",
    "    most_common_terms = {}  # q id --> list of terms\n",
    "    for q_id, c in terms_to_add.items():\n",
    "        top = c.most_common(5)\n",
    "        most_common_terms[q_id] = top\n",
    "\n",
    "    return most_common_terms\n",
    "\n",
    "\n",
    "# terms is dict (q id --> list of tuples (word, freq))\n",
    "def get_idf_score(terms, return_term_vectors, terms_stats):\n",
    "    \n",
    "    # idf_scores will be a dict (q id --> dict (term : idf score))\n",
    "    idf_scores = {}\n",
    "    for q_id, t in terms.items():\n",
    "        terms_dict = {}\n",
    "        for term in t:\n",
    "            if term not in stop_words: # remove stop words\n",
    "                curr_term = term[0]\n",
    "                term_info = terms_stats.get(curr_term, 0)\n",
    "                df = 100\n",
    "                if term_info != 0:\n",
    "                    df = int(term_info['doc_freq'])\n",
    "                d = 84679\n",
    "                idf = math.log(d/df)\n",
    "                terms_dict[curr_term] = idf\n",
    "        idf_scores[q_id] = terms_dict\n",
    "        \n",
    "    most_common_terms = {}  # q id --> list of terms\n",
    "    for q_id, c in idf_scores.items():\n",
    "        c = Counter(c)\n",
    "        top = c.most_common(2)\n",
    "        most_common_terms[q_id] = top\n",
    "\n",
    "    return most_common_terms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stave', 'except', 'all', 'had', 'anyone', 'are', 'into', 'therein', 'halves', 'i', 'year', 'ms', 'via', 'underneath', 'with', 'other', 'sprung', 'whoever', 'yippee', 'can', 'together', 'inc', 'across', 'ours', 'of', 'above', 'always', 'both', 'whenever', 'plenty', 'excepting', 'been', 'henceforth', 'nowadays', 'whensoever', 'who', 'inwards', 'for', 'whosoever', 'ye', 'excluding', 'nevertheless', 'hereafter', 'we', 'everything', 'slew', 'my', 'maybe', 'hence', 'where', 'and', 'towards', 'being', 'slung', 'shown', 'outside', 'up', 'herein', 'whereinto', 'smote', 'latter', 'whence', 'beforehand', 'see', 'must', 'cu', 'would', 'because', 'using', 'somehow', 'nope', 'upward', 'whatever', 'those', 'even', 'than', 'wherefore', 'get', 'indeed', 'each', 'whose', 'becomes', 'beyond', 'seldom', 'about', 'exception', 'furthest', 'though', 'seemed', 'km', 'too', 'canst', 'besides', 'several', 'just', 'whatsoever', 'thereabout', 'an', 'not', 'own', 'contrariwise', 'on', 'let', 'forth', 'quite', 'whew', 'mr', 'sometimes', 'hindmost', 'meanwhile', 'seem', 'under', 'round', 'vs', 'by', 'off', 'their', 'front', 'anything', 'per', 'selves', 'whomsoever', 'wow', 'while', 'slept', 'yet', 'does', 'hath', 'inward', 'furthermore', 'which', 'sang', 'will', 'otherwise', 'it', 'cf', 'if', 'almost', 'further', 'onto', 'latterly', 'farther', 'nor', 'according', 'many', 'ugh', 'again', 'wilt', 'included', 'go', 'ff', 'exclusive', 'everyone', 'whilst', 'itself', 'ltd', 'next', 'inasmuch', 'one', 'after', 'never', 'whoa', 'down', 'notwithstanding', 'dual', 'were', 'others', 'elsewhere', 'only', 'save', 'thereto', 'sideways', 'thereof', 'once', 'namely', 'use', 'thru', 'as', 'much', 'somebody', 'there', 'might', 'became', 'unable', 'cannot', 'now', 'more', 'few', 'seeing', 'whichever', 'et', 'exclude', 'whereabouts', 'then', 'so', 'the', 'either', 'seems', 'us', 'another', 'hither', 'in', 'anywhere', 'whichsoever', 'sprang', 'rather', 'yourselves', 'whither', 'during', 'these', 'yours', 'when', 'over', 'insomuch', 'here', 'among', 'said', 'day', 'choose', 'forward', 'such', 'thou', 'noone', 'worse', 'your', 'has', 'very', 'most', \"doesn't\", 'at', 'thereafter', 'her', 'seen', 'spoken', 'him', 'how', 'around', 'its', 'am', 'already', 'thus', 'do', 'provide', 'nowhere', 'throughout', 'enough', 'sent', 'unless', 'ourselves', 'hardly', 'thereabouts', 'before', 'themselves', 'thyself', 'else', 'without', 'out', 'till', 'some', 'spake', 'somewhat', 'used', 'unlike', 'against', 'could', 'including', 'apart', 'sometime', 'what', 'thereon', 'beside', 'instead', 'cos', 'farthest', 'any', 'nobody', 'hereby', 'inside', 'myself', 'mostly', 'whereunto', 'along', 'this', 'sake', 'whereafter', 'everywhere', 'wherein', 'ever', 'ie', 'worst', 'albeit', 'yourself', 'be', 'nothing', 'first', 'therefore', 'becoming', 'double', 'less', 'anyhow', 'behind', 'herself', 'excepted', 'dost', 'mrs', 'every', 'whereupon', 'although', 'everybody', 'hereupon', 'a', 'is', 'seeming', 'anybody', 'supposing', 'howsoever', 'like', 'they', 'hereto', 'somewhere', 'doing', 'no', 'certain', 'doth', 'alone', 'why', 'need', 'whereat', 'slunk', 'was', 'thee', 'av', 'whereto', 'that', 'perhaps', 'whomever', 'you', 'me', 'thrice', 'to', 'none', 'whole', 'thy', 'little', 'have', 'between', 'whereon', 'until', 'hers', 'well', 'afterwards', 'nonetheless', 'moreover', 'he', 'hast', 'whereof', 'often', 'far', 'whom', 'spoke', 'thenceforth', 'week', 'upwards', 'whereas', 'become', 'them', 'toward', 'wherefrom', 'spat', 'upon', 'wherewith', 'below', 'thereupon', 'neither', 'or', 'whereby', 'same', 'formerly', 'within', 'thereby', 'staves', 'ought', 'meantime', 'etc', 'include', 'may', 'ok', 'our', 'she', 'whether', 'really', 'someone', 'through', 'since', 'something', 'hereabouts', 'amongst', 'from', 'lest', 'kg', 'shalt', 'last', 'himself', 'still', 'anyway', 'want', 'however', 'should', 'thence', 'his', 'indoors', 'also', 'but', 'wherever', 'saw', 'wheresoever', 'kind', 'hitherto'}\n"
     ]
    }
   ],
   "source": [
    "# get stop words we will need for this model\n",
    "stop_words_path = \"C:/6200-IR/homework1-mplatt27/config/stoplist.txt\"\n",
    "stop_words = get_stop_words(stop_words_path)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ['communist', 'parti', 'alleg', 'corrupt', 'public', 'offici', 'govern']\n",
      "59 ['temperatur', 'forecast', 'weather', 'caus', 'fatal']\n",
      "56 ['shamir', 'thatcher', 'predict', 'prime', 'lend', 'rate', 'prime', 'rate', 'move']\n",
      "71 ['rebel', 'gen', 'incurs', 'border', 'militari', 'forc', 'guerrilla']\n",
      "64 ['reform', 'democrat', 'polit', 'hostag']\n",
      "62 ['gen', 'soldier', 'militari', 'coup', \"d'etat\"]\n",
      "93 ['elect', 'polit', 'support', 'nation', 'rifl', 'associat', 'nra']\n",
      "99 [\"north'\", 'tehran', 'iran', 'contra', 'affair']\n",
      "58 ['midgetman', 'mx', 'rail', 'strike']\n",
      "77 ['leakei', 'tusk', 'poach', 'wildlif']\n",
      "54 ['liftoff', 'nasa', 'contract', 'preliminari', 'agreement', 'tent', 'reserv', 'launch', 'commerci', 'satellit']\n",
      "87 ['prosecutor', 'loan', 'crimin', 'offic', 'fail', 'u.s', 'financi', 'institut']\n",
      "94 ['crimin', 'murder', 'crime', 'aid', 'comput']\n",
      "100 ['polit', 'support', 'non', 'communist', 'industri', 'state', 'regul', 'transfer', 'high', 'tech', 'good', 'technolog', 'undesir', 'nation']\n",
      "89 ['billion', 'compani', 'exist', 'pend', 'invest', 'opec', 'member', 'state', 'ani', 'downstream', 'oper']\n",
      "61 [\"north'\", 'oliv', 'israel', 'iran', 'contra', 'affair']\n",
      "95 ['softwar', 'ibm', 'comput', 'crime', 'solv']\n",
      "68 ['asbestosi', 'research', 'studi', 'safeti', 'manufactur', 'employe', 'instal', 'worker', 'fine', 'diamet', 'fiber', 'insul']\n",
      "57 ['ameritech', 'cwa', 'mci', 'bell', 'system', 'breakup']\n",
      "97 ['metamucil', 'solubl', 'fiber', 'optic', 'technolog', 'actual']\n",
      "98 ['deduct', 'incom', 'individu', 'organ', 'produc', 'fiber', 'optic', 'equip']\n",
      "60 ['opera', 'song', 'perform', 'salari', 'incent', 'pai', 'determin', 'senior']\n",
      "80 ['1989', 'campaign', '1988', 'presidenti', 'candid']\n",
      "63 ['gainer', \"nyse'\", 'machin', 'translat', 'system']\n",
      "91 ['nyse', \"nyse'\", 'army', 'advanc', 'weapon', 'system']\n"
     ]
    }
   ],
   "source": [
    "# Main code for pseudo-relevance feedback with using ElasticSearch aggs \"significant terms\"\n",
    "\n",
    "# get new terms that will be added to queries\n",
    "new_terms = get_sig_terms(queries)\n",
    "highest_scored_terms = get_idf_score(new_terms, return_term_vectors, terms_stats_dict)\n",
    "\n",
    "# add the new terms to queries\n",
    "updated_queries = add_terms_to_queries(highest_scored_terms, queries)\n",
    "for key, value in updated_queries.items():\n",
    "   print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES-Built in finished running!\n"
     ]
    }
   ],
   "source": [
    "# Run the models again with the updated queries\n",
    "\n",
    "# run model 1:\n",
    "hits = es_built_in(updated_queries)\n",
    "write_scores_to_file_es(hits, \"es_built_in_results_pseudo2\")\n",
    "print(\"ES-Built in finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi-TF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 2:\n",
    "doc_scores = okapi_tf(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_tf_results_pseudo2\")\n",
    "print(\"Okapi-TF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 3:\n",
    "doc_scores = tf_idf(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"tfidf_results_pseudo2\")\n",
    "print(\"TF-IDF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi BM25 finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 4:\n",
    "doc_scores = okapi_BM25(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_bm25_results_pseudo2\")\n",
    "print(\"Okapi BM25 finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Laplace Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 5:\n",
    "doc_scores = unigram_lm_laplace(return_term_vectors, updated_queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_laplace_results_pseudo2\")\n",
    "print(\"Unigram LM with Laplace Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Jelinek-Mercer Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# run model 6:\n",
    "doc_scores = unigram_lm_jm(return_term_vectors, updated_queries, terms_stats_dict)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_lm_jm_results_pseudo2\")\n",
    "print(\"Unigram LM with Jelinek-Mercer Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
