{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Indexer - Compressed\n",
    "\n",
    "CS 6200 Information Retreival \n",
    "Homework 2\n",
    "Melanie Platt\n",
    "\n",
    "Implementation of a document indexer with compression using zlib. \n",
    "Parse, tokenize, and index documents to use ranking models on for query searching.\n",
    "\n",
    "This will use the catalog and inverted index files generated to create the stemmed index, before merging. Compression\n",
    "will be done in parallel with the merging step. \n",
    "                                                                    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib \n",
    "from collections import OrderedDict \n",
    "from itertools import islice\n",
    "import os\n",
    "import re\n",
    "\n",
    "STEMMING = True\n",
    "\n",
    "# these should actually be the same\n",
    "if STEMMING:\n",
    "    DOC_HASHES_PATH = \"C:/6200-IR/doc_hashes.txt\"\n",
    "else:\n",
    "    DOC_HASHES_PATH = \"C:/6200-IR/doc_hashes_nostemming.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Part 4: Merging the partial indicies with compression \"\"\"\n",
    "\n",
    "# 1. Create a list of the file names for the partial catalog and indicies\n",
    "# 2. Create one larger catalog and index by cycling through partial files, each time appending\n",
    "#    another file to the larger one. Use merging algorithm to accomplish this. Add compressed rather than regular string\n",
    "#    of data as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for completing part 3\n",
    "\n",
    "\"\"\"\n",
    "Function: get_files_in_dir()\n",
    "Input: folder_path: a path to a folder of files\n",
    "Output: file_path_list: a list of paths to each file in the folder\n",
    "Description: Gets the names of all files in the folder then appends each file's path name to a list to return\n",
    "\"\"\"\n",
    "def get_files_in_dir(folder_path):\n",
    "    # gets all names of files in directory\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    # append them to list with their full paths\n",
    "    file_path_list = []\n",
    "    for file in file_list:\n",
    "        file_path_list.append(os.path.join(folder_path, file))\n",
    "\n",
    "    return file_path_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: read_catalog()\n",
    "Input: file path to the catalog file\n",
    "Output: the catalog as a dictionary (term --> (offset, length))\n",
    "\"\"\"\n",
    "def read_catalog(file_path):\n",
    "    cat = OrderedDict()\n",
    "    with open(file_path, encoding=\"ISO-8859-1\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line_list = re.split(r'[:,]', line)\n",
    "            # term --> (offset, length)\n",
    "            cat[line_list[0].strip()] = (line_list[1].strip(), line_list[2].strip())\n",
    "            \n",
    "    f.close()\n",
    "        \n",
    "    return cat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: merge_indicies()\n",
    "Input: merging_file_path: path where merged files will be saved,  m_no: the number merge we are on for file naming,\n",
    "       cat1_list: the catalog we are adding to in the form of a list of tuples (term, offset, len), \n",
    "       ind1_path: the path of the index we are adding to, cat2_listL the catalog we are adding, ind2_path: the path \n",
    "       of the index we are adding\n",
    "Output: None\n",
    "Description: Merges two catalog and index files together and writes two new files for each in the provided directory\n",
    "\"\"\"\n",
    "def merge_indicies(t, merge_path, m_no, cat1_list, ind1_path, cat2_list, ind2_path):\n",
    "    \n",
    "    # initialize files that we will write the merged catalog and index to\n",
    "    merged_cat = open(merge_path + \"merged\" + str(m_no) + \"_catalog.txt\", \"x\")\n",
    "    merged_ind = open(merge_path + \"merged\" + str(m_no) + \"_index.txt\", \"wb\") # index file will be compressed\n",
    "\n",
    "    # pointers to the term we are on in each partial catalog\n",
    "    p1 = 0 \n",
    "    p2 = 0\n",
    "    \n",
    "    # offset for saving the index file\n",
    "    new_offset = 0\n",
    "    \n",
    "    # loop until we have finished the terms in the shorter catalog\n",
    "    while (p1 < len(cat1_list) and p2 < len(cat2_list)):\n",
    "        if cat1_list[p1][0] == cat2_list[p2][0]:\n",
    "\n",
    "            # get the start and offsets for each\n",
    "            term = cat1_list[p1][0].strip()\n",
    "            cat1_start = cat1_list[p1][1][0]\n",
    "            cat1_len = cat1_list[p1][1][1]\n",
    "            cat2_start = cat2_list[p2][1][0]\n",
    "            cat2_len = cat2_list[p2][1][1]\n",
    "\n",
    "            # get the doc information for each\n",
    "            if t == 1:\n",
    "                f1 = open(ind1_path, \"r\")\n",
    "            else:\n",
    "                f1 = open(ind1_path, \"rb\")\n",
    "                \n",
    "            f2 = open(ind2_path, \"r\")\n",
    "            f1.seek(int(cat1_start))\n",
    "            f2.seek(int(cat2_start))\n",
    "            p1_doc_info = f1.read(int(cat1_len))\n",
    "            p2_doc_info = f2.read(int(cat2_len))\n",
    "            f1.close()\n",
    "            f2.close()\n",
    "\n",
    "            # decompress info if after first file\n",
    "            if t != 1:\n",
    "                p1_doc_info = zlib.decompress(p1_doc_info)\n",
    "                p1_doc_info = str(p1_doc_info, 'utf-8')\n",
    "\n",
    "            # combine it\n",
    "            new_info = p1_doc_info + p2_doc_info\n",
    "\n",
    "            # compress nad write to files\n",
    "            new_info_c = zlib.compress(new_info.encode('utf-8'),6)\n",
    "            merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(new_info_c)) + \"\\n\")\n",
    "            merged_ind.write(new_info_c)\n",
    "\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "            new_offset += len(new_info_c)\n",
    "\n",
    "                \n",
    "        elif cat1_list[p1][0] > cat2_list[p2][0]:\n",
    "                      \n",
    "            # just append from cat2 (will always be txt file)\n",
    "            term = cat2_list[p2][0].strip()\n",
    "            cat2_start = cat2_list[p2][1][0]\n",
    "            cat2_len = cat2_list[p2][1][1]\n",
    "            f2 = open(ind2_path, \"r\")\n",
    "            f2.seek(int(cat2_start))\n",
    "            p2_doc_info = f2.read(int(cat2_len))\n",
    "            f2.close()\n",
    "\n",
    "            # compress and write to files\n",
    "            p2_doc_info_c = zlib.compress(p2_doc_info.encode('utf-8'),6)\n",
    "            merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p2_doc_info_c)) + \"\\n\")\n",
    "            merged_ind.write(p2_doc_info_c)\n",
    "            p2 += 1\n",
    "            new_offset += len(p2_doc_info_c)\n",
    "        else:\n",
    "                      \n",
    "            # just append from cat1\n",
    "            term = cat1_list[p1][0].strip()\n",
    "            cat1_start = cat1_list[p1][1][0]\n",
    "            cat1_len = cat1_list[p1][1][1]\n",
    "            \n",
    "            if t == 1:\n",
    "                f1 = open(ind1_path, \"r\")\n",
    "            else:\n",
    "                f1 = open(ind1_path, \"rb\")\n",
    "            f1.seek(int(cat1_start))\n",
    "            p1_doc_info = f1.read(int(cat1_len))\n",
    "            f1.close()\n",
    "            \n",
    "            # copmress if from first file\n",
    "            if t == 1:\n",
    "                p1_doc_info = zlib.compress(p1_doc_info.encode('utf-8'),6)\n",
    "\n",
    "            # write to files\n",
    "            merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p1_doc_info)) + \"\\n\")\n",
    "            merged_ind.write(p1_doc_info)\n",
    "            p1 += 1\n",
    "            new_offset += len(p1_doc_info)\n",
    "\n",
    "\n",
    "    # finish appending whatever is left over in each file\n",
    "    while p1 < len(cat1_list):\n",
    "        term = cat1_list[p1][0].strip()\n",
    "        cat1_start = cat1_list[p1][1][0]\n",
    "        cat1_len = cat1_list[p1][1][1]\n",
    "        \n",
    "        if t == 1:\n",
    "            f1 = open(ind1_path, \"r\")\n",
    "        else:\n",
    "            f1 = open(ind1_path, \"rb\")\n",
    "        f1.seek(int(cat1_start))\n",
    "        p1_doc_info = f1.read(int(cat1_len))\n",
    "        f1.close()\n",
    "        \n",
    "        # copmress if from first file\n",
    "        if t == 1:\n",
    "            p1_doc_info = zlib.compress(p1_doc_info.encode('utf-8'),6)\n",
    "\n",
    "        merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p1_doc_info)) + \"\\n\")\n",
    "        merged_ind.write(p1_doc_info)\n",
    "        p1 += 1\n",
    "        new_offset += len(p1_doc_info)\n",
    "\n",
    "    while p2 < len(cat2_list):\n",
    "        term = cat2_list[p2][0].strip()\n",
    "        cat2_start = cat2_list[p2][1][0]\n",
    "        cat2_len = cat2_list[p2][1][1]\n",
    "        f2 = open(ind2_path, \"r\")\n",
    "        f2.seek(int(cat2_start))\n",
    "        p2_doc_info = f2.read(int(cat2_len))\n",
    "        f2.close()\n",
    "        \n",
    "        # compress and write to files\n",
    "        p2_doc_info_c = zlib.compress(p2_doc_info.encode('utf-8'),6)\n",
    "        merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p2_doc_info_c)) + \"\\n\")\n",
    "        merged_ind.write(p2_doc_info_c)\n",
    "        p2 += 1\n",
    "        new_offset += len(p2_doc_info_c)\n",
    "\n",
    "    merged_cat.close()\n",
    "    merged_ind.close()\n",
    "    print(\"Merging complete! The documents for merge {} have been saved.\".format(m_no))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 85 catalogs\n",
      "We have 85 inverted indicies\n"
     ]
    }
   ],
   "source": [
    "# file paths for partial catalog and indicies\n",
    "partial_catalogs_path = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/stemmed/catalogs/'\n",
    "partial_indicies_path = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/stemmed/indicies/'\n",
    "\n",
    "# file path for saving files as we merge them\n",
    "merging_file_path = 'C:/6200-IR/homework-2-mplatt27/merging/'\n",
    "\n",
    "# collect list of file names\n",
    "catalogs = get_files_in_dir(partial_catalogs_path)\n",
    "indicies = get_files_in_dir(partial_indicies_path)\n",
    "\n",
    "print(\"We have {} catalogs\".format(len(catalogs)))\n",
    "print(\"We have {} inverted indicies\".format(len(indicies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete! The documents for merge 1 have been saved.\n",
      "Merging complete! The documents for merge 2 have been saved.\n",
      "Merging complete! The documents for merge 3 have been saved.\n",
      "Merging complete! The documents for merge 4 have been saved.\n",
      "Merging complete! The documents for merge 5 have been saved.\n",
      "Merging complete! The documents for merge 6 have been saved.\n",
      "Merging complete! The documents for merge 7 have been saved.\n",
      "Merging complete! The documents for merge 8 have been saved.\n",
      "Merging complete! The documents for merge 9 have been saved.\n",
      "Merging complete! The documents for merge 10 have been saved.\n",
      "Merging complete! The documents for merge 11 have been saved.\n",
      "Merging complete! The documents for merge 12 have been saved.\n",
      "Merging complete! The documents for merge 13 have been saved.\n",
      "Merging complete! The documents for merge 14 have been saved.\n",
      "Merging complete! The documents for merge 15 have been saved.\n",
      "Merging complete! The documents for merge 16 have been saved.\n",
      "Merging complete! The documents for merge 17 have been saved.\n",
      "Merging complete! The documents for merge 18 have been saved.\n",
      "Merging complete! The documents for merge 19 have been saved.\n",
      "Merging complete! The documents for merge 20 have been saved.\n",
      "Merging complete! The documents for merge 21 have been saved.\n",
      "Merging complete! The documents for merge 22 have been saved.\n",
      "Merging complete! The documents for merge 23 have been saved.\n",
      "Merging complete! The documents for merge 24 have been saved.\n",
      "Merging complete! The documents for merge 25 have been saved.\n",
      "Merging complete! The documents for merge 26 have been saved.\n",
      "Merging complete! The documents for merge 27 have been saved.\n",
      "Merging complete! The documents for merge 28 have been saved.\n",
      "Merging complete! The documents for merge 29 have been saved.\n",
      "Merging complete! The documents for merge 30 have been saved.\n",
      "Merging complete! The documents for merge 31 have been saved.\n",
      "Merging complete! The documents for merge 32 have been saved.\n",
      "Merging complete! The documents for merge 33 have been saved.\n",
      "Merging complete! The documents for merge 34 have been saved.\n",
      "Merging complete! The documents for merge 35 have been saved.\n",
      "Merging complete! The documents for merge 36 have been saved.\n",
      "Merging complete! The documents for merge 37 have been saved.\n",
      "Merging complete! The documents for merge 38 have been saved.\n",
      "Merging complete! The documents for merge 39 have been saved.\n",
      "Merging complete! The documents for merge 40 have been saved.\n",
      "Merging complete! The documents for merge 41 have been saved.\n",
      "Merging complete! The documents for merge 42 have been saved.\n",
      "Merging complete! The documents for merge 43 have been saved.\n",
      "Merging complete! The documents for merge 44 have been saved.\n",
      "Merging complete! The documents for merge 45 have been saved.\n",
      "Merging complete! The documents for merge 46 have been saved.\n",
      "Merging complete! The documents for merge 47 have been saved.\n",
      "Merging complete! The documents for merge 48 have been saved.\n",
      "Merging complete! The documents for merge 49 have been saved.\n",
      "Merging complete! The documents for merge 50 have been saved.\n",
      "Merging complete! The documents for merge 51 have been saved.\n",
      "Merging complete! The documents for merge 52 have been saved.\n",
      "Merging complete! The documents for merge 53 have been saved.\n",
      "Merging complete! The documents for merge 54 have been saved.\n",
      "Merging complete! The documents for merge 55 have been saved.\n",
      "Merging complete! The documents for merge 56 have been saved.\n",
      "Merging complete! The documents for merge 57 have been saved.\n",
      "Merging complete! The documents for merge 58 have been saved.\n",
      "Merging complete! The documents for merge 59 have been saved.\n",
      "Merging complete! The documents for merge 60 have been saved.\n",
      "Merging complete! The documents for merge 61 have been saved.\n",
      "Merging complete! The documents for merge 62 have been saved.\n",
      "Merging complete! The documents for merge 63 have been saved.\n",
      "Merging complete! The documents for merge 64 have been saved.\n",
      "Merging complete! The documents for merge 65 have been saved.\n",
      "Merging complete! The documents for merge 66 have been saved.\n",
      "Merging complete! The documents for merge 67 have been saved.\n",
      "Merging complete! The documents for merge 68 have been saved.\n",
      "Merging complete! The documents for merge 69 have been saved.\n",
      "Merging complete! The documents for merge 70 have been saved.\n",
      "Merging complete! The documents for merge 71 have been saved.\n",
      "Merging complete! The documents for merge 72 have been saved.\n",
      "Merging complete! The documents for merge 73 have been saved.\n",
      "Merging complete! The documents for merge 74 have been saved.\n",
      "Merging complete! The documents for merge 75 have been saved.\n",
      "Merging complete! The documents for merge 76 have been saved.\n",
      "Merging complete! The documents for merge 77 have been saved.\n",
      "Merging complete! The documents for merge 78 have been saved.\n",
      "Merging complete! The documents for merge 79 have been saved.\n",
      "Merging complete! The documents for merge 80 have been saved.\n",
      "Merging complete! The documents for merge 81 have been saved.\n",
      "Merging complete! The documents for merge 82 have been saved.\n",
      "Merging complete! The documents for merge 83 have been saved.\n",
      "Merging complete! The documents for merge 84 have been saved.\n",
      "All docs have been merged. File number 85 has the full merged index.\n"
     ]
    }
   ],
   "source": [
    "# Merge all files together\n",
    "\n",
    "# get first catalog and read in as dict and list; get corresponding index\n",
    "# we will iterativly add to this one\n",
    "full_cat_file = catalogs[0] # the file name of the first partial catalog\n",
    "full_cat_dict = read_catalog(full_cat_file) # a dictionary of the first catalog\n",
    "full_cat_list = list(full_cat_dict.items()) # (term, (start, offset))\n",
    "full_index_file = indicies[0] # the file name of the corresponding first index\n",
    "\n",
    "# iterate 84 times to merge each catalog and index to the larger one\n",
    "merge_no = 1\n",
    "t = 1\n",
    "for i in range(1,len(catalogs)): \n",
    "    \n",
    "    # get catalog to add in read in as dict and list; get corresponding index\n",
    "    par_cat2_file = catalogs[i] # file name of the second catalog (10)\n",
    "    par_cat2_dict = read_catalog(par_cat2_file) # a dict of the second catalog\n",
    "    par_cat2_list = list(par_cat2_dict.items()) # a list of the second catalog\n",
    "    par_ind2_file = indicies[i] # the file name of the corresponding second index (10)\n",
    "    \n",
    "    # merge together and write to file (merged catalog and index)\n",
    "    merge_indicies(t, merging_file_path, merge_no, full_cat_list, full_index_file, par_cat2_list, par_ind2_file)\n",
    "    \n",
    "    # set new full list to the one we just created\n",
    "    full_cat_file = merging_file_path + \"merged\" + str(merge_no) + \"_catalog.txt\"\n",
    "    full_cat_dict = read_catalog(full_cat_file)\n",
    "    full_cat_list = list(full_cat_dict.items())\n",
    "    full_index_file = merging_file_path + \"merged\" + str(merge_no) + \"_index.txt\"\n",
    "\n",
    "    merge_no += 1\n",
    "    t += 1\n",
    "       \n",
    "print(\"All docs have been merged. File number {} has the full merged index.\".format(merge_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 catalogs\n",
      "We have 2 inverted indicies\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing compression before using all files \"\"\"\n",
    "\n",
    "\n",
    "# file paths for partial catalog and indicies\n",
    "partial_catalogs_path_t = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/testing/catalogs/'\n",
    "partial_indicies_path_t = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/testing/indicies/'\n",
    "\n",
    "# file path for saving files as we merge them\n",
    "merging_file_path_t = 'C:/6200-IR/homework-2-mplatt27/merging/testing'\n",
    "\n",
    "# collect list of file names\n",
    "catalogs_t = get_files_in_dir(partial_catalogs_path_t)\n",
    "indicies_t = get_files_in_dir(partial_indicies_path_t)\n",
    "\n",
    "print(\"We have {} catalogs\".format(len(catalogs_t)))\n",
    "print(\"We have {} inverted indicies\".format(len(indicies_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete! The documents for merge 1 have been saved.\n",
      "All docs have been merged. File number 2 has the full merged index.\n"
     ]
    }
   ],
   "source": [
    "# Merge all files together testing\n",
    "\n",
    "# get first catalog and read in as dict and list; get corresponding index\n",
    "# we will iterativly add to this one\n",
    "full_cat_file_t = catalogs_t[0] # the file name of the first partial catalog\n",
    "full_cat_dict_t = read_catalog(full_cat_file_t) # a dictionary of the first catalog\n",
    "full_cat_list_t = list(full_cat_dict_t.items()) # (term, (start, offset))\n",
    "full_index_file_t = indicies_t[0] # the file name of the corresponding first index\n",
    "\n",
    "# iterate 84 times to merge each catalog and index to the larger one\n",
    "merge_no_t = 1\n",
    "t = 1 # the round we are on, if greater than 1, we are merging a text file with a bytes file (first time is two txt files)\n",
    "for i in range(1,len(catalogs_t)): \n",
    "    \n",
    "    # get catalog to add in read in as dict and list; get corresponding index\n",
    "    par_cat2_file_t = catalogs_t[i] # file name of the second catalog (10)\n",
    "    par_cat2_dict_t = read_catalog(par_cat2_file_t) # a dict of the second catalog\n",
    "    par_cat2_list_t = list(par_cat2_dict_t.items()) # a list of the second catalog\n",
    "    par_ind2_file_t = indicies_t[i] # the file name of the corresponding second index (10)\n",
    "    \n",
    "    # merge together and write to file (merged catalog and index)\n",
    "    merge_indicies(t, merging_file_path_t, merge_no_t, full_cat_list_t, full_index_file_t, par_cat2_list_t, par_ind2_file_t)\n",
    "    \n",
    "    # set new full list to the one we just created\n",
    "    full_cat_file_t = merging_file_path_t + \"merged\" + str(merge_no_t) + \"_catalog.txt\"\n",
    "    full_cat_dict_t = read_catalog(full_cat_file_t)\n",
    "    full_cat_list_t = list(full_cat_dict_t.items())\n",
    "    full_index_file_t = merging_file_path_t + \"merged\" + str(merge_no_t) + \"_index.txt\"\n",
    "\n",
    "    merge_no_t += 1\n",
    "\n",
    "    \n",
    "print(\"All docs have been merged. File number {} has the full merged index.\".format(merge_no_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'x\\x9c\\x15\\xca\\xb1\\r\\x000\\x08\\x03\\xb0\\x872@\\x08\\xa5\\xcam\\x1c_\\xd5\\xb3+\\x9cP\\xaf\\x8eY\\xa0\\xc0\\xde\\nOn\\x87\\x0b\\x9a\\xd55\\xa1\\x7f\\x1e\\xe0Y\\n9'\n",
      "b'30:1,45|46:23,24,25|30:71|50:3,47|48:2,4,45|'\n",
      "30:1,45|46:23,24,25|30:71|50:3,47|48:2,4,45|\n"
     ]
    }
   ],
   "source": [
    "# test opening the merged file that was created\n",
    "\n",
    "# file path for merged catalog and index\n",
    "cat_m = 'C:/6200-IR/homework-2-mplatt27/merging/testingmerged1_catalog.txt'\n",
    "ind_m = 'C:/6200-IR/homework-2-mplatt27/merging/testingmerged1_index.txt'\n",
    "\n",
    "t2 = open(ind_m, \"rb\")\n",
    "t2.seek(45)\n",
    "details = t2.read(46)\n",
    "t2.close()\n",
    "\n",
    "# |30:1,45|46:23,24,25| + 30:71|50:3,47|48:2,4,45|\n",
    "\n",
    "print(details)\n",
    "details_c = zlib.decompress(details)\n",
    "print(details_c)\n",
    "print(str(details_c, 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58:57,68,84,144|148:72|30:1,45|46:23,24,25|\n",
      "b'x\\x9c\\x05\\xc1\\xc1\\x11\\x000\\x08\\x02\\xb0\\x85xTD\\xe5\\x9c\\xcd\\xe1\\x9b\\x94\\xb7\\x06mX\\x08\\xe9B\\xde\\xe1\\xe5\\xdb\\x80\\xea\\xd4\\xcb\\x04\\x05\\xd6}\\xd2!\\t\\xcc'\n"
     ]
    }
   ],
   "source": [
    "# Testing zlib and writing bytes to file\n",
    "\n",
    "# get strings\n",
    "string_1 = \"58:57,68,84,144|\"\n",
    "string_2 = \"148:72|30:1,45|46:23,24,25|\"\n",
    "\n",
    "# merge strings and compress\n",
    "string_3 = string_1 + string_2\n",
    "string_3_c = zlib.compress(string_3.encode('utf-8'),6)\n",
    "\n",
    "# see what they look like\n",
    "print(string_3)\n",
    "print(string_3_c)\n",
    "\n",
    "# write to files as string\n",
    "c = open(\"C:/6200-IR/homework-2-mplatt27/merging/testing/string_catalog.txt\", \"x\")\n",
    "c.write(\"apple\"+\":\"+\"0\"+\",\"+str(len(string_3)))\n",
    "c.close()\n",
    "t = open(\"C:/6200-IR/homework-2-mplatt27/merging/testing/string_index.txt\", \"x\")\n",
    "t.write(string_3)\n",
    "t.close()\n",
    "\n",
    "# write to files as bytes string\n",
    "c = open(\"C:/6200-IR/homework-2-mplatt27/merging/testing/bytes_catalog.txt\", \"x\")\n",
    "c.write(\"apple\"+\":\"+\"0\"+\",\"+str(len(string_3_c)))\n",
    "c.close()\n",
    "t = open(\"C:/6200-IR/homework-2-mplatt27/merging/testing/bytes_index.txt\", \"wb\")\n",
    "t.write(string_3_c)\n",
    "t.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'x\\x9c\\x05\\xc1\\xc1\\x11\\x000\\x08\\x02\\xb0\\x85xTD\\xe5\\x9c\\xcd\\xe1\\x9b\\x94\\xb7\\x06mX\\x08\\xe9B\\xde\\xe1\\xe5\\xdb\\x80\\xea\\xd4\\xcb\\x04\\x05\\xd6}\\xd2!\\t\\xcc'\n",
      "b'58:57,68,84,144|148:72|30:1,45|46:23,24,25|'\n",
      "58:57,68,84,144|148:72|30:1,45|46:23,24,25|\n"
     ]
    }
   ],
   "source": [
    "# open from files\n",
    "t2 = open(\"C:/6200-IR/homework-2-mplatt27/merging/testing/bytes_index.txt\", \"rb\")\n",
    "t2.seek(0)\n",
    "details = t2.read(44)\n",
    "t2.close()\n",
    "\n",
    "print(details)\n",
    "string_3_d = zlib.decompress(details)\n",
    "print(string_3_d)\n",
    "print(str(string_3_d, 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Compress the stemmed index that we made previously without merging \"\"\"\n",
    "\n",
    "# file paths for partial catalog and indicies\n",
    "catalog_path = 'C:/6200-IR/homework-2-mplatt27/index-stemmed/full_catalog.txt'\n",
    "index_path = 'C:/6200-IR/homework-2-mplatt27/index-stemmed/full_index.txt'\n",
    "# catalog_path = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/testing/catalogs/cat1_tester.txt'\n",
    "# index_path = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/testing/indicies/ind1_tester.txt'\n",
    "\n",
    "catalog_dict = read_catalog(catalog_path) # a dictionary of the first catalog (term --> (start, offset))\n",
    "\n",
    "offset = 0\n",
    "# compressed_catalog = open(\"C:/6200-IR/homework-2-mplatt27/merging/testing/bytes_catalog.txt\", \"x\")\n",
    "# compressed_index = open(\"C:/6200-IR/homework-2-mplatt27/merging/testing/bytes_index.txt\", \"wb\")\n",
    "compressed_catalog = open(\"C:/6200-IR/homework-2-mplatt27/merging/compressed_catalog_9.txt\", \"x\")\n",
    "compressed_index = open(\"C:/6200-IR/homework-2-mplatt27/merging/compressed_index_9.txt\", \"wb\")\n",
    "\n",
    "for term, pos in catalog_dict.items():\n",
    "    index_f = open(index_path, \"r\")\n",
    "    index_f.seek(int(pos[0]))\n",
    "    details = index_f.read(int(pos[1]))\n",
    "    index_f.close()\n",
    "    \n",
    "    details_c = zlib.compress(details.encode('utf-8'),9)\n",
    "    \n",
    "    compressed_catalog.write(term + \":\" + str(offset).strip() + \",\" + str(len(details_c)) + \"\\n\")\n",
    "    compressed_index.write(details_c)\n",
    "    offset += len(details_c)\n",
    "\n",
    "compressed_catalog.close()\n",
    "compressed_index.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_catalog.close()\n",
    "compressed_index.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
