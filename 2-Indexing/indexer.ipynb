{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Indexer\n",
    "\n",
    "CS 6200 Information Retreival \n",
    "Homework 2\n",
    "Melanie Platt\n",
    "\n",
    "Implementation of a document indexer. Parse, tokenize, and index documents to use ranking models on for query searching.\n",
    "                                                                    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict \n",
    "from itertools import islice\n",
    "import os\n",
    "import re\n",
    "\n",
    "STEMMING = True\n",
    "if STEMMING:\n",
    "    DOC_HASHES_PATH = \"C:/6200-IR/doc_hashes.txt\"\n",
    "else:\n",
    "    DOC_HASHES_PATH = \"C:/6200-IR/doc_hashes_nostemming.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Part 2: Indexing (Partial Indicies)\"\"\"\n",
    "\n",
    "# 1. Read in pickple files of doc_dict with tokens for each dict\n",
    "# 2. In groups of 1000 docs at a time, write two parallel files to create the inverted index: \n",
    "#    a. A file with a list of term, offset in parellel file, length in parellel file pairs\n",
    "#    b. A file listing document (that term is in) : position in doc\n",
    "#        * Positions are seperated by commas if there is more than 1\n",
    "#        * Each doc is seperated by a pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Indexer class collects tokens from the set of input documents and maps them to the documents that they exist in,\n",
    "and the positions that they occur in that document. It also writes this token dict as two files to create an \n",
    "inverted index. This can be used with any size set of documents, but is best with 1000 or less at a time. \n",
    "\"\"\"\n",
    "class Indexer:\n",
    "    \n",
    "    def __init__(self):\n",
    "            \n",
    "        # a dict of tokens mapped to a dict (doc --> list of positions that the term occurs in that doc)\n",
    "        self.tokens = OrderedDict()\n",
    "        \n",
    "            \n",
    "    \"\"\"\n",
    "    Function: index_docs\n",
    "    Input: A dictonary of documents mapped to a dictionary of tokens mapped to a list of those tokens' positions\n",
    "    Output: None\n",
    "    Description: For the set of input documents, creates an inverted index in the form of a dictionary (self.tokens)\n",
    "    mapping the tokens to a dictionary of documents mapped to a list of their positions in the doc\n",
    "    \"\"\"\n",
    "    def index_docs(self, d):\n",
    "        for doc, tokens in d.items():\n",
    "            for token in tokens: # token is (term, position)\n",
    "                if token[0] not in self.tokens.keys():\n",
    "                    self.tokens[token[0]] = {}\n",
    "                if doc not in self.tokens[token[0]].keys():\n",
    "                    self.tokens[token[0]][doc] = []\n",
    "                self.tokens[token[0]][doc].append(token[1])\n",
    "    \n",
    "    \"\"\"\n",
    "    Function: sort_tokens_dict\n",
    "    Input: None\n",
    "    Outpout: None\n",
    "    Descriptions: Sorts the inverted index alphabetically by token\n",
    "    \"\"\"\n",
    "    def sort_tokens_dict(self):\n",
    "        self.tokens = OrderedDict(sorted(self.tokens.items(), key = lambda t: t[0]))\n",
    "                \n",
    "\n",
    "    \"\"\"\n",
    "    Function: write_index_to_files\n",
    "    Input: path to save files, group #\n",
    "    Output: None\n",
    "    Description: Saves two parallel files: catalog and index, where the catelog lists the term, offset, and length of\n",
    "    information. This maps to the index file which lists the documents that the term appers in, as well as the positions\n",
    "    that the term is in, in those docs. Group No. is added to the file names. \n",
    "    \"\"\"\n",
    "    def write_index_to_files(self, save_path, group_no):\n",
    "        \n",
    "        catalog_name = \"catalog\" + str(group_no) + \".txt\"\n",
    "        inverted_index_name = \"inverted_index\" + str(group_no) + \".txt\"\n",
    "        \n",
    "        catalog = open(save_path + catalog_name, \"w\")\n",
    "        inverted_index = open(save_path + inverted_index_name, \"w\")\n",
    "        \n",
    "        offset = 0\n",
    "        for token, docs in self.tokens.items():\n",
    "            index_string = \"\"\n",
    "            for doc, pos_list in docs.items():\n",
    "                index_string = index_string + str(doc).strip() + \":\"\n",
    "                pos_count = 0\n",
    "                for pos in pos_list:\n",
    "                    if pos_count == 0:\n",
    "                        index_string = index_string + str(pos).strip()\n",
    "                    else:\n",
    "                        index_string = index_string + \",\" + str(pos).strip()\n",
    "                    pos_count += 1\n",
    "                index_string = index_string + \"|\"\n",
    "            \n",
    "            inverted_index.write(index_string)\n",
    "            catalog.write(token.strip() + \":\" + str(offset).strip() + \",\" + str(len(index_string)) + \"\\n\")\n",
    "            offset += len(index_string)\n",
    "        catalog.close()\n",
    "        inverted_index.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function: split_dicts\n",
    "Input: dictionary, number of keys we want in each dict (keys = doc)\n",
    "Output: A list dicts (each sub-dict contains n keys)\n",
    "\"\"\"\n",
    "def split_dicts(d, n):\n",
    "    for i in range(0, len(d), n):\n",
    "        yield OrderedDict(islice(d.items(), i, i+n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Main code \"\"\"\n",
    "\n",
    "# read in doc_dict from pickle file\n",
    "if STEMMING:\n",
    "    handle = open('C:/6200-IR/doc_dict.pickle', 'rb')\n",
    "    doc_dict = pickle.load(handle)\n",
    "    handle.close()\n",
    "else:\n",
    "    handle = open('C:/6200-IR/doc_dict_nostemming.pickle', 'rb')\n",
    "    doc_dict = pickle.load(handle)\n",
    "    handle.close()\n",
    "    \n",
    "# Create list that holds all dicts of documents with tokens (each sub-dict has 1000 docs in it)\n",
    "split_dicts_list = list(split_dicts(doc_dict, 1000))\n",
    "\n",
    "\n",
    "# Check that the list split correctly\n",
    "total = 0\n",
    "for each in split_dicts_list:\n",
    "    total += len(each)\n",
    "    \n",
    "print(\"We have {} documents to index\".format(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sub-dict of 1000 docs, we want to index these docs\n",
    "# create an Indexer object, which will create a dict of tokens --> {doc --> [positions of token]}\n",
    "# Write files for this set of docs (catalog and index)\n",
    "print(\"There are {} groups of 1000 documents to index\".format(len(split_dicts_list)))\n",
    "for i in range(len(split_dicts_list)):\n",
    "    indexer = Indexer()\n",
    "    indexer.index_docs(split_dicts_list[i])\n",
    "    indexer.sort_tokens_dict()\n",
    "    indexer.write_index_to_files(\"C:/6200-IR/\", i+1)\n",
    "    \n",
    "print(\"Index files have been generated!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Part 3: Merging the partial indicies \"\"\"\n",
    "\n",
    "# 1. Create a list of the file names for the partial catalog and indicies\n",
    "# 2. Create one larger catalog and index by cycling through partial files, each time appending\n",
    "#    another file to the larger one. Use merging algorithm to accomplish this.\n",
    "# 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for completing part 3\n",
    "\n",
    "\"\"\"\n",
    "Function: get_files_in_dir()\n",
    "Input: folder_path: a path to a folder of files\n",
    "Output: file_path_list: a list of paths to each file in the folder\n",
    "Description: Gets the names of all files in the folder then appends each file's path name to a list to return\n",
    "\"\"\"\n",
    "def get_files_in_dir(folder_path):\n",
    "    # gets all names of files in directory\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    # append them to list with their full paths\n",
    "    file_path_list = []\n",
    "    for file in file_list:\n",
    "        file_path_list.append(os.path.join(folder_path, file))\n",
    "\n",
    "    return file_path_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: read_catalog()\n",
    "Input: file path to the catalog file\n",
    "Output: the catalog as a dictionary (term --> (offset, length))\n",
    "\"\"\"\n",
    "def read_catalog(file_path):\n",
    "    cat = OrderedDict()\n",
    "    with open(file_path, encoding=\"ISO-8859-1\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line_list = re.split(r'[:,]', line)\n",
    "            # term --> (offset, length)\n",
    "            cat[line_list[0].strip()] = (line_list[1].strip(), line_list[2].strip())\n",
    "            \n",
    "    f.close()\n",
    "        \n",
    "    return cat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: merge_indicies()\n",
    "Input: merging_file_path: path where merged files will be saved,  m_no: the number merge we are on for file naming,\n",
    "       cat1_list: the catalog we are adding to in the form of a list of tuples (term, offset, len), \n",
    "       ind1_path: the path of the index we are adding to, cat2_listL the catalog we are adding, ind2_path: the path \n",
    "       of the index we are adding\n",
    "Output: None\n",
    "Description: Merges two catalog and index files together and writes two new files for each in the provided directory\n",
    "\"\"\"\n",
    "def merge_indicies(merge_path, m_no, cat1_list, ind1_path, cat2_list, ind2_path):\n",
    "    \n",
    "    # initialize files that we will write the merged catalog and index to\n",
    "    merged_cat = open(merge_path + \"merged\" + str(merge_no) + \"_catalog.txt\", \"x\")\n",
    "    merged_ind = open(merge_path + \"merged\" + str(merge_no) + \"_index.txt\", \"x\")\n",
    "\n",
    "    # pointers to the term we are on in each partial catalog\n",
    "    p1 = 0 \n",
    "    p2 = 0\n",
    "    \n",
    "    # offset for saving the index file\n",
    "    new_offset = 0\n",
    "    \n",
    "    # loop until we have finished the terms in the shorter catalog\n",
    "    while (p1 < len(cat1_list) and p2 < len(cat2_list)):\n",
    "        if cat1_list[p1][0] == cat2_list[p2][0]:\n",
    "\n",
    "            # get the start and offsets for each\n",
    "            term = cat1_list[p1][0].strip()\n",
    "            cat1_start = cat1_list[p1][1][0]\n",
    "            cat1_len = cat1_list[p1][1][1]\n",
    "            cat2_start = cat2_list[p2][1][0]\n",
    "            cat2_len = cat2_list[p2][1][1]\n",
    "\n",
    "            # get the doc information for each\n",
    "            f1, f2 = open(ind1_path, \"r\"), open(ind2_path, \"r\")\n",
    "            f1.seek(int(cat1_start))\n",
    "            f2.seek(int(cat2_start))\n",
    "            p1_doc_info = f1.read(int(cat1_len))\n",
    "            p2_doc_info = f2.read(int(cat2_len))\n",
    "            f1.close()\n",
    "            f2.close()\n",
    "\n",
    "            # combine it\n",
    "            new_info = p1_doc_info + p2_doc_info\n",
    "\n",
    "            # write to files\n",
    "            merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(new_info)) + \"\\n\")\n",
    "            merged_ind.write(new_info)\n",
    "\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "            new_offset += len(new_info)\n",
    "        elif cat1_list[p1][0] > cat2_list[p2][0]:\n",
    "                      \n",
    "            # just append from cat2\n",
    "            term = cat2_list[p2][0].strip()\n",
    "            cat2_start = cat2_list[p2][1][0]\n",
    "            cat2_len = cat2_list[p2][1][1]\n",
    "            f2 = open(ind2_path, \"r\")\n",
    "            f2.seek(int(cat2_start))\n",
    "            p2_doc_info = f2.read(int(cat2_len))\n",
    "            f2.close()\n",
    "\n",
    "            # write to files\n",
    "            merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p2_doc_info)) + \"\\n\")\n",
    "            merged_ind.write(p2_doc_info)\n",
    "            p2 += 1\n",
    "            new_offset += len(p2_doc_info)\n",
    "        else:\n",
    "                      \n",
    "            # just append from cat1\n",
    "            term = cat1_list[p1][0].strip()\n",
    "            cat1_start = cat1_list[p1][1][0]\n",
    "            cat1_len = cat1_list[p1][1][1]\n",
    "            f1 = open(ind1_path, \"r\")\n",
    "            f1.seek(int(cat1_start))\n",
    "            p1_doc_info = f1.read(int(cat1_len))\n",
    "            f1.close()\n",
    "\n",
    "            # write to files\n",
    "            merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p1_doc_info)) + \"\\n\")\n",
    "            merged_ind.write(p1_doc_info)\n",
    "            p1 += 1\n",
    "            new_offset += len(p1_doc_info)\n",
    "\n",
    "\n",
    "    # finish appending whatever is left over in each file\n",
    "    while p1 < len(cat1_list):\n",
    "        term = cat1_list[p1][0].strip()\n",
    "        cat1_start = cat1_list[p1][1][0]\n",
    "        cat1_len = cat1_list[p1][1][1]\n",
    "        f1 = open(ind1_path, \"r\")\n",
    "        f1.seek(int(cat1_start))\n",
    "        p1_doc_info = f1.read(int(cat1_len))\n",
    "        f1.close()\n",
    "\n",
    "        merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p1_doc_info)) + \"\\n\")\n",
    "        merged_ind.write(p1_doc_info)\n",
    "        p1 += 1\n",
    "        new_offset += len(p1_doc_info)\n",
    "\n",
    "    while p2 < len(cat2_list):\n",
    "        term = cat2_list[p2][0].strip()\n",
    "        cat2_start = cat2_list[p2][1][0]\n",
    "        cat2_len = cat2_list[p2][1][1]\n",
    "        f2 = open(ind2_path, \"r\")\n",
    "        f2.seek(int(cat2_start))\n",
    "        p2_doc_info = f2.read(int(cat2_len))\n",
    "        f2.close()\n",
    "\n",
    "        merged_cat.write(term + \":\" + str(new_offset).strip() + \",\" + str(len(p2_doc_info)) + \"\\n\")\n",
    "        merged_ind.write(p2_doc_info)\n",
    "        p2 += 1\n",
    "        new_offset += len(p2_doc_info)\n",
    "\n",
    "    merged_cat.close()\n",
    "    merged_ind.close()\n",
    "    print(\"Merging complete! The documents for merge {} have been saved.\".format(m_no))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 85 catalogs\n",
      "We have 85 inverted indicies\n"
     ]
    }
   ],
   "source": [
    "# file paths for partial catalog and indicies\n",
    "partial_catalogs_path = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/catalogs/'\n",
    "partial_indicies_path = 'C:/6200-IR/homework-2-mplatt27/inverted-index-files/indicies/'\n",
    "\n",
    "# file path for saving files as we merge them\n",
    "merging_file_path = 'C:/6200-IR/homework-2-mplatt27/merging/'\n",
    "\n",
    "# collect list of file names\n",
    "catalogs = get_files_in_dir(partial_catalogs_path)\n",
    "indicies = get_files_in_dir(partial_indicies_path)\n",
    "\n",
    "# for testing only DELETE LATER\n",
    "# cat_test = catalogs[:3]\n",
    "# ind_test = indicies[:3]\n",
    "\n",
    "print(\"We have {} catalogs\".format(len(catalogs)))\n",
    "print(\"We have {} inverted indicies\".format(len(indicies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete! The documents for merge 1 have been saved.\n",
      "Merging complete! The documents for merge 2 have been saved.\n",
      "Merging complete! The documents for merge 3 have been saved.\n",
      "Merging complete! The documents for merge 4 have been saved.\n",
      "Merging complete! The documents for merge 5 have been saved.\n",
      "Merging complete! The documents for merge 6 have been saved.\n",
      "Merging complete! The documents for merge 7 have been saved.\n",
      "Merging complete! The documents for merge 8 have been saved.\n",
      "Merging complete! The documents for merge 9 have been saved.\n",
      "Merging complete! The documents for merge 10 have been saved.\n",
      "Merging complete! The documents for merge 11 have been saved.\n",
      "Merging complete! The documents for merge 12 have been saved.\n",
      "Merging complete! The documents for merge 13 have been saved.\n",
      "Merging complete! The documents for merge 14 have been saved.\n",
      "Merging complete! The documents for merge 15 have been saved.\n",
      "Merging complete! The documents for merge 16 have been saved.\n",
      "Merging complete! The documents for merge 17 have been saved.\n",
      "Merging complete! The documents for merge 18 have been saved.\n",
      "Merging complete! The documents for merge 19 have been saved.\n",
      "Merging complete! The documents for merge 20 have been saved.\n",
      "Merging complete! The documents for merge 21 have been saved.\n",
      "Merging complete! The documents for merge 22 have been saved.\n",
      "Merging complete! The documents for merge 23 have been saved.\n",
      "Merging complete! The documents for merge 24 have been saved.\n",
      "Merging complete! The documents for merge 25 have been saved.\n",
      "Merging complete! The documents for merge 26 have been saved.\n",
      "Merging complete! The documents for merge 27 have been saved.\n",
      "Merging complete! The documents for merge 28 have been saved.\n",
      "Merging complete! The documents for merge 29 have been saved.\n",
      "Merging complete! The documents for merge 30 have been saved.\n",
      "Merging complete! The documents for merge 31 have been saved.\n",
      "Merging complete! The documents for merge 32 have been saved.\n",
      "Merging complete! The documents for merge 33 have been saved.\n",
      "Merging complete! The documents for merge 34 have been saved.\n",
      "Merging complete! The documents for merge 35 have been saved.\n",
      "Merging complete! The documents for merge 36 have been saved.\n",
      "Merging complete! The documents for merge 37 have been saved.\n",
      "Merging complete! The documents for merge 38 have been saved.\n",
      "Merging complete! The documents for merge 39 have been saved.\n",
      "Merging complete! The documents for merge 40 have been saved.\n",
      "Merging complete! The documents for merge 41 have been saved.\n",
      "Merging complete! The documents for merge 42 have been saved.\n",
      "Merging complete! The documents for merge 43 have been saved.\n",
      "Merging complete! The documents for merge 44 have been saved.\n",
      "Merging complete! The documents for merge 45 have been saved.\n",
      "Merging complete! The documents for merge 46 have been saved.\n",
      "Merging complete! The documents for merge 47 have been saved.\n",
      "Merging complete! The documents for merge 48 have been saved.\n",
      "Merging complete! The documents for merge 49 have been saved.\n",
      "Merging complete! The documents for merge 50 have been saved.\n",
      "Merging complete! The documents for merge 51 have been saved.\n",
      "Merging complete! The documents for merge 52 have been saved.\n",
      "Merging complete! The documents for merge 53 have been saved.\n",
      "Merging complete! The documents for merge 54 have been saved.\n",
      "Merging complete! The documents for merge 55 have been saved.\n",
      "Merging complete! The documents for merge 56 have been saved.\n",
      "Merging complete! The documents for merge 57 have been saved.\n",
      "Merging complete! The documents for merge 58 have been saved.\n",
      "Merging complete! The documents for merge 59 have been saved.\n",
      "Merging complete! The documents for merge 60 have been saved.\n",
      "Merging complete! The documents for merge 61 have been saved.\n",
      "Merging complete! The documents for merge 62 have been saved.\n",
      "Merging complete! The documents for merge 63 have been saved.\n",
      "Merging complete! The documents for merge 64 have been saved.\n",
      "Merging complete! The documents for merge 65 have been saved.\n",
      "Merging complete! The documents for merge 66 have been saved.\n",
      "Merging complete! The documents for merge 67 have been saved.\n",
      "Merging complete! The documents for merge 68 have been saved.\n",
      "Merging complete! The documents for merge 69 have been saved.\n",
      "Merging complete! The documents for merge 70 have been saved.\n",
      "Merging complete! The documents for merge 71 have been saved.\n",
      "Merging complete! The documents for merge 72 have been saved.\n",
      "Merging complete! The documents for merge 73 have been saved.\n",
      "Merging complete! The documents for merge 74 have been saved.\n",
      "Merging complete! The documents for merge 75 have been saved.\n",
      "Merging complete! The documents for merge 76 have been saved.\n",
      "Merging complete! The documents for merge 77 have been saved.\n",
      "Merging complete! The documents for merge 78 have been saved.\n",
      "Merging complete! The documents for merge 79 have been saved.\n",
      "Merging complete! The documents for merge 80 have been saved.\n",
      "Merging complete! The documents for merge 81 have been saved.\n",
      "Merging complete! The documents for merge 82 have been saved.\n",
      "Merging complete! The documents for merge 83 have been saved.\n",
      "Merging complete! The documents for merge 84 have been saved.\n",
      "All docs have been merged. File number 85 has the full merged index.\n"
     ]
    }
   ],
   "source": [
    "# Merge all files together\n",
    "\n",
    "# get first catalog and read in as dict and list; get corresponding index\n",
    "# we will iterativly add to this one\n",
    "full_cat_file = catalogs[0] # the file name of the first partial catalog\n",
    "full_cat_dict = read_catalog(full_cat_file) # a dictionary of the first catalog\n",
    "full_cat_list = list(full_cat_dict.items()) # (term, (start, offset))\n",
    "full_index_file = indicies[0] # the file name of the corresponding first index\n",
    "\n",
    "# iterate 84 times to merge each catalog and index to the larger one\n",
    "merge_no = 1\n",
    "for i in range(1,len(catalogs)): \n",
    "    \n",
    "    # get catalog to add in read in as dict and list; get corresponding index\n",
    "    par_cat2_file = catalogs[i] # file name of the second catalog (10)\n",
    "    par_cat2_dict = read_catalog(par_cat2_file) # a dict of the second catalog\n",
    "    par_cat2_list = list(par_cat2_dict.items()) # a list of the second catalog\n",
    "    par_ind2_file = indicies[i] # the file name of the corresponding second index (10)\n",
    "    \n",
    "    # merge together and write to file (merged catalog and index)\n",
    "    merge_indicies(merging_file_path, merge_no, full_cat_list, full_index_file, par_cat2_list, par_ind2_file)\n",
    "    \n",
    "    # set new full list to the one we just created\n",
    "    full_cat_file = merging_file_path + \"merged\" + str(merge_no) + \"_catalog.txt\"\n",
    "    full_cat_dict = read_catalog(full_cat_file)\n",
    "    full_cat_list = list(full_cat_dict.items())\n",
    "    full_index_file = merging_file_path + \"merged\" + str(merge_no) + \"_index.txt\"\n",
    "\n",
    "    merge_no += 1\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"All docs have been merged. File number {} has the full merged index.\".format(merge_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
