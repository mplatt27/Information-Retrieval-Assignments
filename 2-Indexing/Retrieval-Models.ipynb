{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import OrderedDict \n",
    "import os\n",
    "import operator\n",
    "from collections import Counter\n",
    "import math\n",
    "import zlib \n",
    "import time\n",
    "\n",
    "STOP_PATH = \"C:/6200-IR/homework-2-mplatt27/stop_words_list.txt\"\n",
    "QUERIES_PATH = \"C:/6200-IR/homework-2-mplatt27/queries_modified_x.txt\"\n",
    "QUERIES_PATH_UNMODIFIED = \"C:/6200-IR/homework-2-mplatt27/queries_unmodified.txt\"\n",
    "CATALOG_PATH = \"C:/6200-IR/homework-2-mplatt27/index-not-stemmed/full_catalog_nostemming.txt\"\n",
    "INVERTED_INDEX_PATH = \"C:/6200-IR/homework-2-mplatt27/index-not-stemmed/full_index_nostemming.txt\"\n",
    "DOC_HASHES_PATH = \"C:/6200-IR/homework-2-mplatt27/index-not-stemmed/doc_hashes_nostemming.txt\"\n",
    "STEMMING = False\n",
    "if STEMMING:\n",
    "    ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Part 1: Read in modified queries, catalog, doc_hashes \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for part 1\n",
    "\n",
    "\"\"\"\n",
    "Function: get_stop_words()\n",
    "Input: File path to stop words list\n",
    "Output: A list of stop words to leave out (of queries, index)\n",
    "\"\"\"\n",
    "def get_stop_words(file_path):\n",
    "    stop_words = []\n",
    "    for line in open(file_path, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        stop_words.append(line.strip())\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: query_analyzer()\n",
    "Input: The full query as a string (one or more words)\n",
    "Output: A list of strings where each string is one word (token) of the query\n",
    "\"\"\"\n",
    "def query_analyzer(query, stop):\n",
    "    \n",
    "    cleaned_queries = []\n",
    "    query_list = re.findall(r\"\\w+|[^\\w\\s]|[\\n\\r]+\", query, re.UNICODE)\n",
    "    for term in query_list:\n",
    "        if term.isalnum() and term not in stop: # leave out punct and stop words\n",
    "            # stem the token if required\n",
    "            if STEMMING:\n",
    "                term = ps.stem(term)\n",
    "            cleaned_queries.append(term)\n",
    "    return cleaned_queries\n",
    "\n",
    "\"\"\"\n",
    "Function: read_queries()\n",
    "Input: The folder path to the queries file as a string\n",
    "Output: A dictionary mapping each query ID to a list of terms in that query (as str)\n",
    "\"\"\"\n",
    "def read_queries(folder_path, stop):\n",
    "    # iterate over each line in the query\n",
    "    lines = []\n",
    "    ids = []\n",
    "    for line in open(folder_path, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        curr_query = str(line)\n",
    "        id_end = curr_query.find(\".\")\n",
    "        q_id = curr_query[:id_end].strip()\n",
    "        ids.append(q_id)\n",
    "        curr_query = curr_query[id_end + 3:].strip()\n",
    "        lines.append(curr_query)\n",
    "\n",
    "    # clean and stemp (remove stop words)\n",
    "    cleaned_queries = {}\n",
    "    for i in range(len(lines)):\n",
    "        # cleaned_query will be a list of the query words as strings\n",
    "        cleaned_query = query_analyzer(lines[i], stop)\n",
    "        cleaned_queries[ids[i]] = cleaned_query\n",
    "    return cleaned_queries\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: read_catalog()\n",
    "Input: file path to the catalog file\n",
    "Output: the catalog as a dictionary (term --> (offset, length))\n",
    "\"\"\"\n",
    "def read_catalog(file_path):\n",
    "    cat = OrderedDict()\n",
    "    with open(file_path, encoding=\"ISO-8859-1\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line_list = re.split(r'[:,]', line)\n",
    "            # term --> (offset, length)\n",
    "            cat[line_list[0].strip()] = (line_list[1].strip(), line_list[2].strip())\n",
    "    f.close()\n",
    "        \n",
    "    return cat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: parse_doc_details\n",
    "Input: String from index to parse of the form: doc:positions|doc:positions (e.g., \"58:57,68,84,144|148:72|\")\n",
    "Output: A dictoinary of docs mapped to a list of the term positions in that doc\n",
    "\"\"\"\n",
    "def parse_doc_details(info_string):\n",
    "    info_string_list = info_string.split(\"|\")\n",
    "\n",
    "    # list of doc:positions, always ends in \"\"\n",
    "    docs_dict = {}\n",
    "    for doc in info_string_list:\n",
    "        if doc == \"\":\n",
    "            continue\n",
    "        temp_list = doc.split(\":\")\n",
    "        docs_dict[temp_list[0]] = temp_list[1].split(\",\")\n",
    "        \n",
    "    return docs_dict \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: read_doc_hashes\n",
    "Input: File path to doc_hashes; Lines are of the form \"AP890101-0001 1|571\" (doc_name doc_hash|len(d))\n",
    "Output: Dictionary that maps doc hash --> (doc_name, len(d))\n",
    "\"\"\"\n",
    "def read_doc_hashes(file_path):\n",
    "    docs = OrderedDict()\n",
    "    with open(file_path, encoding=\"ISO-8859-1\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split()\n",
    "            # name --> hash, len\n",
    "            info = line_list[1].split(\"|\")\n",
    "            \n",
    "            docs[info[0].strip()] = (line_list[0].strip(), info[1].strip())\n",
    "    f.close()\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: calc_avg_len_d\n",
    "Input: doc_hashes dict and length of dict (number of documents)\n",
    "Output: averge length of all documents in corpus\n",
    "\"\"\"\n",
    "def calc_avg_len_d(docs, n):\n",
    "    # doc hash --> (name, length)\n",
    "    sum_lens = 0\n",
    "    for doc_hash, info in docs.items():\n",
    "        sum_lens += int(info[1])\n",
    "        \n",
    "    avg_len_d = sum_lens / n\n",
    "    return avg_len_d\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: sort_scores_dict()\n",
    "Input: scores, a dictionary that maps query # to a dictionary (docno --> score)\n",
    "Output: The same dictionary, but the value (dict that each key maps to) is now sorted by the scores\n",
    "\"\"\"\n",
    "def sort_scores_dict(scores):\n",
    "    for q_id, d in scores.items():\n",
    "        sorted_dict = dict(sorted(d.items(), key=operator.itemgetter(1), reverse=True))\n",
    "        scores[q_id] = sorted_dict\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Function: write_scores_to_file()\n",
    "# Input: A dictionary of scored documents for each query and a name for the file\n",
    "# Output: None\n",
    "# Does: Writes a file for the output. Assumes scores are already sorted. For each query response, writes a line\n",
    "# for each document that was returned that includes the query number, doc number, rank, and score.\n",
    "# Each line should be of the form: <query-number> Q0 <docno> <rank> <score> Exp\n",
    "# This is for all models, except the ES built in, due to the differing format of results.\n",
    "\"\"\"\n",
    "def write_scores_to_file(scores, name):\n",
    "    # assumes scores are already sorted\n",
    "    # scores is dict of query id --> dict (doc_id --> score)\n",
    "    file_name = name + \".txt\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    output = open(file_name, \"w\")\n",
    "\n",
    "    # iterate over query id responses\n",
    "    for q_id, dict in scores.items():\n",
    "        query_number = q_id\n",
    "        rank = 1\n",
    "        for doc_id, score in dict.items():\n",
    "            if rank > 1000:\n",
    "                break\n",
    "            new_line = str(query_number) + \" \" + \"Q0\" + \" \" + doc_id + \" \" + str(rank) + \" \" + str(score) + \" Exp\\n\"\n",
    "            output.write(new_line)\n",
    "            rank += 1\n",
    "    output.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ['alleg', 'corrupt', 'public', 'offici', 'govern']\n",
      "59 ['weather', 'caus', 'fatal']\n",
      "56 ['predict', 'prime', 'lend', 'rate', 'prime', 'rate', 'move']\n",
      "71 ['incurs', 'border', 'militari', 'forc', 'guerrilla']\n",
      "64 ['polit', 'hostag']\n",
      "62 ['militari', 'coup', 'd', 'etat']\n",
      "93 ['support', 'nation', 'rifl', 'associ', 'nra']\n",
      "99 ['iran', 'contra', 'affair']\n",
      "58 ['rail', 'strike']\n",
      "77 ['poach', 'wildlif']\n",
      "54 ['contract', 'preliminari', 'agreement', 'tent', 'reserv', 'launch', 'commerci', 'satellit']\n",
      "87 ['crimin', 'offic', 'fail', 'U', 'S', 'financi', 'institut']\n",
      "94 ['crime', 'aid', 'comput']\n",
      "100 ['non', 'communist', 'industri', 'state', 'regul', 'transfer', 'high', 'tech', 'good', 'technolog', 'undesir', 'nation']\n",
      "89 ['exist', 'pend', 'invest', 'opec', 'member', 'state', 'downstream', 'oper']\n",
      "61 ['israel', 'iran', 'contra', 'affair']\n",
      "95 ['comput', 'crime', 'solv']\n",
      "68 ['studi', 'safeti', 'manufactur', 'employe', 'instal', 'worker', 'fine', 'diamet', 'fiber', 'insul']\n",
      "57 ['mci', 'bell', 'system', 'breakup']\n",
      "97 ['fiber', 'optic', 'technolog', 'actual']\n",
      "98 ['individu', 'organ', 'produc', 'fiber', 'optic', 'equip']\n",
      "60 ['perform', 'salari', 'incent', 'pay', 'determin', 'senior']\n",
      "80 ['1988', 'presidenti', 'candid']\n",
      "63 ['machin', 'translat', 'system']\n",
      "91 ['US', 'armi', 'advanc', 'weapon', 'system']\n"
     ]
    }
   ],
   "source": [
    "# Main code to get the queries and print them out (check for appropriate stemming)\n",
    "stop_words = get_stop_words(STOP_PATH)\n",
    "queries = read_queries(QUERIES_PATH, stop_words)\n",
    "for key, value in queries.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 169974 terms in the catalog\n",
      "We have 84678 docs\n"
     ]
    }
   ],
   "source": [
    "# read in the catalog\n",
    "catalog = read_catalog(CATALOG_PATH)\n",
    "print(\"We have {} terms in the catalog\".format(len(catalog))) # less terms than w/ Elsasticsearch; \n",
    "                                                              # I didn't include anything with punctuation\n",
    "\n",
    "# read in doc_hashes as dictionary so they are easy to access\n",
    "doc_hashes = read_doc_hashes(DOC_HASHES_PATH)\n",
    "print(\"We have {} docs\".format(len(doc_hashes)))\n",
    "\n",
    "# calc avg len d\n",
    "AVG_LEN_D = calc_avg_len_d(doc_hashes, len(doc_hashes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Part 2: Retrieval models\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************************************************************** #\n",
    "# Model 1: Okapi TF\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: okapi_tf_calc, a helper function to calculate score\n",
    "\"\"\"\n",
    "def okapi_tf_calc(tf, doc_len, avg_corp_len):\n",
    "    score = tf / (tf + 0.5 + (1.5 * (doc_len / avg_corp_len)))\n",
    "    return score\n",
    "\n",
    "\"\"\"\n",
    "Function: okapi_tf\n",
    "Input: Catalog dict (term --> offset,len in index); doc_names dict (doc_hash --> (name, len)),\n",
    "       queries_dict (query id --> dictionary (doc_no --> score))\n",
    "Output: A scores dictionary (query id --> dictionary (doc-id --> score)\n",
    "Does: Iterates through each query term, and through each doc that the term appers in and calculates okapi-tf \n",
    "for the document-word combination. Sums score and returns as dict.\n",
    "\"\"\"\n",
    "def okapi_tf(catalog_dict, doc_names, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "        \n",
    "    # iterate over each query\n",
    "    for q_id, query in query_dict.items():\n",
    "        # for each word in the query, get the docs that have that word\n",
    "        for word in query:\n",
    "            \n",
    "            # access placement details from catalog\n",
    "            index_placement = catalog_dict.get(word,0)\n",
    "            if index_placement != 0:\n",
    "                offset = index_placement[0]\n",
    "                length = index_placement[1]\n",
    "\n",
    "                # access string of information from invertex index\n",
    "                f = open(INVERTED_INDEX_PATH, \"r\")\n",
    "                f.seek(int(offset))\n",
    "                doc_details = f.read(int(length))\n",
    "                doc_details_dict = parse_doc_details(doc_details)\n",
    "                f.close()\n",
    "\n",
    "                # iterate over each doc that the word appears in\n",
    "                for doc, positions in doc_details_dict.items():\n",
    "                    tf = len(positions)\n",
    "                    len_d = int(doc_names[doc][1])\n",
    "                    avg_len = AVG_LEN_D\n",
    "                    temp_score = okapi_tf_calc(tf, len_d, avg_len)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    doc_name = doc_names[doc][0]\n",
    "                    if doc_name not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_name] = temp_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_name] += temp_score\n",
    "                    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 4: Okapi BM25\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function for calculating BM25\n",
    "\"\"\"\n",
    "def okapi_BM25_calc(total_docs, df, tf, tf_query, doc_len, avg_doc_len, k_1, k_2, b):\n",
    "    calc1_num = total_docs + 0.5\n",
    "    calc1_den = df + 0.5\n",
    "    calc1 = math.log((calc1_num/calc1_den), 2)\n",
    "\n",
    "    calc2_num = tf + (k_1 * tf)\n",
    "    cal2_den = tf + k_1 * ((1-b) + (b * (doc_len/avg_doc_len)))\n",
    "    calc2 = calc2_num/cal2_den\n",
    "\n",
    "    calc3_num = tf_query + (k_2 * tf_query)\n",
    "    calc3_den = tf_query + k_2\n",
    "    calc3 = calc3_num/calc3_den\n",
    "\n",
    "    return calc1 * calc2 * calc3\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: okapi_BM25\n",
    "Input: Catalog dict (term --> offset,len in index); doc_names dict (doc_hash --> (name, len)),\n",
    "       queries_dict (query id --> dictionary (doc_no --> score))\n",
    "Output: A scores dictionary (query id --> dictionary (doc-id --> score)\n",
    "Does: Iterates through each query term, and through each doc that the term appers in and calculates okapi_BM25 \n",
    "for the document-word combination. Sums score and returns as dict.\n",
    "\"\"\"\n",
    "def okapi_BM25(catalog_dict, doc_names, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # maps the query # --> Counter (for each query)\n",
    "    queries_counter = {}\n",
    "    # populate with query ids mapped to empty dict and queries counter with tf_queries\n",
    "    # Counter is number of times the word occurs in the query\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "        queries_counter[q_id] = Counter()\n",
    "        for word in query:\n",
    "            queries_counter[q_id][word] += 1\n",
    "\n",
    "    # iterate over each query\n",
    "    for q_id, query in query_dict.items():\n",
    "        # for each word in the query, get the docs that have that word\n",
    "        for word in query:\n",
    "            \n",
    "            # access placement details from catalog\n",
    "            index_placement = catalog_dict.get(word,0)\n",
    "            if index_placement != 0:\n",
    "                offset = index_placement[0]\n",
    "                length = index_placement[1]\n",
    "\n",
    "                # access string of information from invertex index\n",
    "                f = open(INVERTED_INDEX_PATH, \"r\")\n",
    "                f.seek(int(offset))\n",
    "                doc_details = f.read(int(length))\n",
    "                doc_details_dict = parse_doc_details(doc_details)\n",
    "                f.close()\n",
    "\n",
    "                # iterate over each doc that the word appears in\n",
    "                for doc, positions in doc_details_dict.items():\n",
    "                    \n",
    "                    # first calc\n",
    "                    total_docs = len(doc_names) # total number of docs we have\n",
    "                    df = len(doc_details_dict) # number of docs this word occurs in\n",
    "                    \n",
    "                    # second calc\n",
    "                    tf = len(positions)\n",
    "                    len_d = int(doc_names[doc][1])\n",
    "                    avg_len = AVG_LEN_D\n",
    "                    \n",
    "                    # third calc\n",
    "                    tf_query = queries_counter[q_id][word]\n",
    "                    \n",
    "                    # constants\n",
    "                    k_1 = 1.5 # 1.5 is better when using with proximity search\n",
    "                    # k_1 = 1.2\n",
    "                    k_2 = 1.2\n",
    "                    b = 0.5 # 0.5 is better when using with proximity search\n",
    "                    # b = 0.75\n",
    "                    \n",
    "                    # calculate okapi BM25\n",
    "                    temp_score = okapi_BM25_calc(total_docs, df, tf, tf_query, len_d, avg_len, k_1, k_2, b)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    doc_name = doc_names[doc][0]\n",
    "                    if doc_name not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_name] = temp_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_name] += temp_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************** #\n",
    "# Model 5: Unigram LM with Laplace smoothing\n",
    "\n",
    "\"\"\"\n",
    "Function for laplace calculation\n",
    "\"\"\"\n",
    "def p_laplace_calc(tf, doc_len, v):\n",
    "    return (tf + 1) / (doc_len + v)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: unigram_lm_laplace\n",
    "Input: Catalog dict (term --> offset,len in index); doc_names dict (doc_hash --> (name, len)),\n",
    "       queries_dict (query id --> dictionary (doc_no --> score))\n",
    "Output: A scores dictionary (query id --> dictionary (doc-id --> score)\n",
    "Does: Iterates through each query term, and through each doc that the term appers in and calculates unigram_lm_laplace \n",
    "for the document-word combination. Sums score and returns as dict.\n",
    "\"\"\"\n",
    "def unigram_lm_laplace(catalog_dict, doc_names, query_dict):\n",
    "    \n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    \n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "\n",
    "    # get vocabulary size\n",
    "    v = len(catalog_dict)\n",
    "    \n",
    "    # iterate over each query\n",
    "    for q_id, query in query_dict.items():\n",
    "        # for each word in the query, get the docs that have that word\n",
    "        for word in query:\n",
    "            \n",
    "            # access placement details from catalog (just string at this point)\n",
    "            index_placement = catalog_dict.get(word,0)\n",
    "            \n",
    "            # get the docs that this word appers in\n",
    "            doc_details_dict = {}\n",
    "            # if word is in the catalog, score all docs it appears in\n",
    "            if index_placement != 0:\n",
    "                offset = index_placement[0]\n",
    "                length = index_placement[1]\n",
    "\n",
    "                # access string of information from invertex index\n",
    "                f = open(INVERTED_INDEX_PATH, \"r\")\n",
    "                f.seek(int(offset))\n",
    "                doc_details = f.read(int(length))\n",
    "                doc_details_dict = parse_doc_details(doc_details)\n",
    "                f.close()\n",
    "                \n",
    "            # iterate over each document (doc_hash --> (name, len))\n",
    "            for hash_val, info in doc_names.items():\n",
    "                # when the word is in the doc\n",
    "                if hash_val in doc_details_dict.keys():\n",
    "                    positions = doc_details_dict.get(hash_val)\n",
    "                    tf = len(positions)\n",
    "                    len_d = int(info[1])\n",
    "                    temp_score = p_laplace_calc(tf, len_d, v)\n",
    "                # when the word is not in the doc\n",
    "                else:\n",
    "                    len_d = int(info[1])\n",
    "                    temp_score = p_laplace_calc(0, len_d, v)\n",
    "                    \n",
    "                # add score to dictionary\n",
    "                doc_name = info[0]\n",
    "                score = math.log(temp_score)\n",
    "                if doc_name not in scores[q_id].keys():\n",
    "                    scores[q_id][doc_name] = score\n",
    "                else:\n",
    "                    scores[q_id][doc_name] += score\n",
    "                \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Okapi TF model for running with compressed index\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: okapi_tf_calc, a helper function to calculate score\n",
    "\"\"\"\n",
    "def okapi_tf_calc(tf, doc_len, avg_corp_len):\n",
    "    score = tf / (tf + 0.5 + (1.5 * (doc_len / avg_corp_len)))\n",
    "    return score\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: okapi_tf\n",
    "Input: Catalog dict (term --> offset,len in index); doc_names dict (doc_hash --> (name, len)),\n",
    "       queries_dict (query id --> dictionary (doc_no --> score))\n",
    "Output: A scores dictionary (query id --> dictionary (doc-id --> score)\n",
    "Does: Iterates through each query term, and through each doc that the term appers in and calculates okapi-tf \n",
    "for the document-word combination. Sums score and returns as dict.\n",
    "\"\"\"\n",
    "def okapi_tf(catalog_dict, doc_names, query_dict):\n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = {}\n",
    "    # populate with query ids mapped to empty dict\n",
    "    for q_id, query in query_dict.items():\n",
    "        scores[q_id] = {}\n",
    "        \n",
    "    # iterate over each query\n",
    "    for q_id, query in query_dict.items():\n",
    "        # for each word in the query, get the docs that have that word\n",
    "        for word in query:\n",
    "            \n",
    "            # access placement details from catalog\n",
    "            index_placement = catalog_dict.get(word,0)\n",
    "            if index_placement != 0:\n",
    "                offset = index_placement[0]\n",
    "                length = index_placement[1]\n",
    "\n",
    "                # access string of information from invertex index\n",
    "                f = open(INVERTED_INDEX_PATH, \"rb\")\n",
    "                f.seek(int(offset))\n",
    "                doc_details = f.read(int(length))\n",
    "                doc_details_decompressed = zlib.decompress(doc_details)\n",
    "                doc_details_string = str(doc_details_decompressed, 'utf-8')\n",
    "                doc_details_dict = parse_doc_details(doc_details_string)\n",
    "                f.close()\n",
    "\n",
    "                # iterate over each doc that the word appears in\n",
    "                for doc, positions in doc_details_dict.items():\n",
    "                    tf = len(positions)\n",
    "                    len_d = int(doc_names[doc][1])\n",
    "                    avg_len = AVG_LEN_D\n",
    "                    temp_score = okapi_tf_calc(tf, len_d, avg_len)\n",
    "\n",
    "                    # add score to dictionary\n",
    "                    doc_name = doc_names[doc][0]\n",
    "                    if doc_name not in scores[q_id].keys():\n",
    "                        scores[q_id][doc_name] = temp_score\n",
    "                    else:\n",
    "                        scores[q_id][doc_name] += temp_score\n",
    "                    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Main code for running retrieval models \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi-TF finished running!\n"
     ]
    }
   ],
   "source": [
    "# Run Okapi-TF\n",
    "doc_scores = okapi_tf(catalog, doc_hashes, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_tf_results\")\n",
    "print(\"Okapi-TF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi-BM25 finished running!\n"
     ]
    }
   ],
   "source": [
    "# Run Okapi-BM25\n",
    "doc_scores = okapi_BM25(catalog, doc_hashes, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_BM25_results\")\n",
    "print(\"Okapi-BM25 finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram LM with Lapalace Smoothing finished running!\n"
     ]
    }
   ],
   "source": [
    "# Run Laplace Smoothing\n",
    "doc_scores = unigram_lm_laplace(catalog, doc_hashes, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"unigram_laplace_results\")\n",
    "print(\"Unigram LM with Lapalace Smoothing finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okapi-TF finished running!\n"
     ]
    }
   ],
   "source": [
    "# Run okapi TF on compressed index\n",
    "doc_scores = okapi_tf(catalog, doc_hashes, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"okapi_tf_compressed_results\")\n",
    "print(\"Okapi-TF finished running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proximity search model:\n",
    "- Code derived from \"Experiments with Proximity-Aware Scoring for XML Retrieval at INEX 2008\"\n",
    "  by Broschart, Schenkel, and Theobald\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: calc_ief\n",
    "Input: Number doc docs in corpus\n",
    "Output: IEF calculation\n",
    "\"\"\"\n",
    "def calc_ief(n, term_freq):\n",
    "    return math.log( (n - (term_freq) + 1) / (term_freq + 0.5), 2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: calc_acc\n",
    "Input: total docs, distance, word, dictionary of words and their positions in the current document\n",
    "Output: acc score as defined by Broschart et al (2008)\n",
    "Does: Checks the distance between the current query word and all other words in the query. If the two words\n",
    "in question fall within the defined distance, calcuate ief score for those two words and sum to acc score. \n",
    "\"\"\"\n",
    "def calc_acc(td, d, wrd, w_positions_dict):\n",
    "    curr_positions = w_positions_dict.get(wrd)\n",
    "    acc = 0\n",
    "    for w, p in w_positions_dict.items():\n",
    "        if w == wrd:\n",
    "            continue\n",
    "        p1 = 0\n",
    "        p2 = 0\n",
    "        term_j_pos = p\n",
    "        while (p1 < len(curr_positions) and p2 < len(term_j_pos)):\n",
    "            window = abs(int(curr_positions[p1]) - int(term_j_pos[p2]))\n",
    "            if window <= d:\n",
    "                # log(total_docs - tf + 0.5 / tf + 1) / d^2\n",
    "                tf = len(term_j_pos)\n",
    "                temp_acc = calc_ief(td, tf) / d**2\n",
    "                acc += temp_acc\n",
    "            if int(curr_positions[p1]) < int(term_j_pos[p2]):\n",
    "                p1 += 1\n",
    "            else:\n",
    "                p2 += 1\n",
    "        while (p1 < len(curr_positions)):\n",
    "            window = abs(int(curr_positions[p1]) - int(term_j_pos[p2-1])) # may need to do p2 -1 or else out of range\n",
    "            if window <= d:\n",
    "                tf = len(term_j_pos)\n",
    "                temp_acc = calc_ief(td, tf) / d**2\n",
    "                acc += temp_acc\n",
    "            p1 += 1\n",
    "        while (p2 < len(term_j_pos)):\n",
    "            window = abs(int(curr_positions[p1-1]) - int(term_j_pos[p2]))\n",
    "            if window <= d:\n",
    "                tf = len(term_j_pos)\n",
    "                temp_acc = calc_ief(td, tf) / d**2\n",
    "                acc += temp_acc\n",
    "            p2 += 1\n",
    "    return acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: calc_proximity\n",
    "Input: total docs, tf, acc for term, doc len, avg doc len, k1, k2, b values\n",
    "Output: proximity score as defined by Broschart et al (2008)\n",
    "\"\"\"\n",
    "def calc_proximity(num_docs, tf, acc_t, len_d, avg_doc_len, k1, k2, b):\n",
    "    ief = calc_ief(num_docs, tf)\n",
    "    calc2 = (acc_t * (k1+1)) / (acc_t + (k1 * ((1-b) + (b * (len_d/avg_doc_len)))) ) \n",
    "    prox = min(ief, 1) * calc2\n",
    "    return prox\n",
    "    \n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "Function: proximity_search\n",
    "Input: catalog, dict mapping doc names to a number, query dict of terms\n",
    "Output: Dict of scores for each document in each query\n",
    "Does: First gets okapi BM25 scores for all docs relating to the queries. Then calculates a proximity score to\n",
    "append to the current score of each doc, using the proximity of the query terms in the doc. \n",
    "\"\"\"\n",
    "def proximity_search(catalog_dict, doc_names, query_dict):\n",
    "    \n",
    "    # constants\n",
    "    k_1 = 1.5\n",
    "    k_2 = 1.2\n",
    "    b = 0.4\n",
    "    d = 2\n",
    "    \n",
    "    # maps the query # --> dictionary (doc-no : score)\n",
    "    scores = okapi_BM25(catalog_dict, doc_names, query_dict)\n",
    "    \n",
    "    for q_id, query in query_dict.items():\n",
    "        \n",
    "        query_info = {} # doc: {word: positions} for only words in the current query\n",
    "        for word in query:\n",
    "            \n",
    "            # get the docs that this word appers in\n",
    "            index_placement = catalog_dict.get(word,0)\n",
    "            doc_details_dict = {} # doc: positions, for just the current word\n",
    "            if index_placement != 0: # if the word exists in the corpus\n",
    "                offset = index_placement[0]\n",
    "                length = index_placement[1]\n",
    "                # access string of information from invertex index\n",
    "                f = open(INVERTED_INDEX_PATH, \"r\")\n",
    "                f.seek(int(offset))\n",
    "                doc_details = f.read(int(length))\n",
    "                doc_details_dict = parse_doc_details(doc_details) # doc --> positions\n",
    "                f.close()\n",
    "                \n",
    "                # populate query_info with the docs that this word appears in\n",
    "                for doc, pos in doc_details_dict.items():\n",
    "                    if query_info.get(doc): # if doc is already in query_info\n",
    "                        query_info[doc][word] = pos\n",
    "                    else:\n",
    "                        query_info[doc] = {word : pos}\n",
    "                        \n",
    "        # once we have all of the docs that this query exists in, get proximity information for the words\n",
    "        for doc, words in query_info.items():\n",
    "            for word, pos in words.items():\n",
    "                tf = len(pos)\n",
    "                curr_acc = calc_acc(len(doc_names), d, word, words)\n",
    "                prox_score = calc_proximity(len(doc_names), tf, curr_acc, int(doc_names[doc][1]), AVG_LEN_D, k_1, k_2, b)\n",
    "                doc_name = doc_names[doc][0]\n",
    "                scores[q_id][doc_name] += prox_score\n",
    "                \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ['Document', 'discuss', 'allegations', 'measures', 'taken', 'corrupt', 'public', 'officials', 'governmental', 'jurisdiction', 'worldwide']\n",
      "59 ['Document', 'report', 'type', 'weather', 'event', 'directly', 'caused', 'least', 'fatality', 'location']\n",
      "56 ['Document', 'prediction', 'prime', 'lending', 'rate', 'report', 'actual', 'prime', 'rate', 'move']\n",
      "71 ['Document', 'report', 'incursions', 'land', 'air', 'water', 'border', 'area', 'country', 'military', 'forces', 'second', 'country', 'guerrilla', 'group', 'based', 'second', 'country']\n",
      "64 ['Document', 'report', 'event', 'result', 'politically', 'motivated', 'hostage', 'taking']\n",
      "62 ['Document', 'report', 'military', 'coup', 'd', 'etat', 'attempted', 'successful', 'country']\n",
      "93 ['Document', 'describe', 'identify', 'supporters', 'National', 'Rifle', 'Association', 'NRA', 'assets']\n",
      "99 ['Document', 'identify', 'development', 'Iran', 'Contra', 'Affair']\n",
      "58 ['Document', 'predict', 'anticipate', 'rail', 'strike', 'report', 'ongoing', 'rail', 'strike']\n",
      "77 ['Document', 'report', 'poaching', 'method', 'type', 'wildlife']\n",
      "54 ['Document', 'cite', 'signing', 'contract', 'preliminary', 'agreement', 'making', 'tentative', 'reservation', 'launch', 'commercial', 'satellite']\n",
      "87 ['Document', 'report', 'current', 'criminal', 'actions', 'officers', 'failed', 'U', 'S', 'financial', 'institution']\n",
      "94 ['Document', 'identify', 'crime', 'perpetrated', 'aid', 'computer']\n",
      "100 ['Document', 'identify', 'efforts', 'non', 'communist', 'industrialized', 'states', 'regulate', 'transfer', 'high', 'tech', 'goods', 'technologies', 'undesirable', 'nations']\n",
      "89 ['Document', 'identify', 'existing', 'pending', 'investment', 'OPEC', 'member', 'state', 'downstream', 'operation']\n",
      "61 ['Document', 'discuss', 'role', 'Israel', 'Iran', 'Contra', 'Affair']\n",
      "95 ['Document', 'describe', 'computer', 'application', 'crime', 'solving']\n",
      "68 ['Document', 'report', 'actual', 'studies', 'unsubstantiated', 'concerns', 'safety', 'manufacturing', 'employees', 'installation', 'workers', 'fine', 'diameter', 'fibers', 'insulation', 'products']\n",
      "57 ['Document', 'discuss', 'MCI', 'Bell', 'System', 'breakup']\n",
      "97 ['Document', 'identify', 'instances', 'fiber', 'optics', 'technology', 'actually']\n",
      "98 ['Document', 'identify', 'individuals', 'organizations', 'produce', 'fiber', 'optics', 'equipment']\n",
      "60 ['Document', 'describe', 'sides', 'controversy', 'standards', 'performance', 'determine', 'salary', 'levels', 'incentive', 'pay', 'contrasted', 'determining', 'pay', 'solely', 'basis', 'seniority', 'longevity', 'job']\n",
      "80 ['Document', 'identify', 'platform', '1988', 'presidential', 'candidate']\n",
      "63 ['Document', 'identify', 'machine', 'translation', 'system']\n",
      "91 ['Document', 'identify', 'acquisition', 'U', 'S', 'Army', 'specified', 'advanced', 'weapons', 'systems']\n"
     ]
    }
   ],
   "source": [
    "# read in queries without modification (except removing stop words/stemming)\n",
    "stop_words = get_stop_words(STOP_PATH)\n",
    "queries = read_queries(QUERIES_PATH_UNMODIFIED, stop_words)\n",
    "queries.pop(\"\")\n",
    "for key, value in queries.items():\n",
    "    print(key, value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proximity search model finished running!\n",
      "--- 6.93445611000061 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Run Proximity search\n",
    "start_time = time.time()\n",
    "doc_scores = proximity_search(catalog, doc_hashes, queries)\n",
    "doc_scores_sorted = sort_scores_dict(doc_scores)\n",
    "write_scores_to_file(doc_scores_sorted, \"proximity_results_unstemmed\")\n",
    "print(\"Proximity search model finished running!\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
