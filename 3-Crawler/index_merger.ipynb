{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Merge Index on Elasticsearch cloud\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "host='https://elastic:cwHN1LsyXbAGmb5LxCbADTkj@cs6200.es.us-west1.gcp.cloud.es.io:9243'\n",
    "\n",
    "es = Elasticsearch([host],timeout=3000)\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function: get_files_in_dir()\n",
    "Input: folder_path: a path to a folder of files to be indexed\n",
    "Returns: file_path_list: a list of paths to each file in the folder\n",
    "Does: Gets the names of all files in the folder then appends each\n",
    "file's path name to a list to return\n",
    "\"\"\"\n",
    "def get_files_in_dir(folder_path):\n",
    "    # gets all names of files in directory\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    # append them to list with their full paths\n",
    "    file_path_list = []\n",
    "    for file in file_list:\n",
    "        file_path_list.append(os.path.join(folder_path, file))\n",
    "\n",
    "    return file_path_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: get_data_from_text_file()\n",
    "Input: file: a single file that may contain multiple documents to be indexed\n",
    "Returns: data: a list of lists; each sub-list is a line from the file\n",
    "Does: reads each line of the file and appends it in a list to data\n",
    "\"\"\"\n",
    "def get_data_from_text_file(file):\n",
    "    # declare an empty list for the data\n",
    "    data = []\n",
    "    for line in open(file, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        data += [str(line)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function: yield_docs()\n",
    "Input: files: a list of each file path that we want to index (each file contains one doc)\n",
    "Returns: null\n",
    "Does: For each file, get the fields that we need and do some text clean up. Check if the doc is already in the corpus.\n",
    "If it is, update the author and inlinks. If it isn't stage it to be indexed. \n",
    "\"\"\"\n",
    "def yield_docs(files):\n",
    "    \n",
    "    # each file contains one doc\n",
    "    for count, file in enumerate(files):\n",
    "        \n",
    "        # retrieve data from file\n",
    "        doc = get_data_from_text_file(file)\n",
    "        doc = \"\".join(doc)\n",
    "        \n",
    "        # get doc no\n",
    "        docno_s = doc.find(\"<DOCNO>\") + len(\"<DOCNO>\") \n",
    "        docno_e = doc.find(\"</DOCNO>\")\n",
    "        docno = doc[docno_s:docno_e].strip()\n",
    "\n",
    "        # get title\n",
    "        title_s = doc.find(\"<TITLE>\") + len(\"<TITLE>\") \n",
    "        ttile_e = doc.find(\"</TITLE>\")\n",
    "        title = doc[title_s:ttile_e].strip()\n",
    "\n",
    "        # find author\n",
    "#         author_s = doc.find(\"<AUTHOR>\") + len(\"<AUTHOR>\") \n",
    "#         author_e = doc.find(\"</AUTHOR>\")\n",
    "#         author = doc[author_s:author_e].strip()\n",
    "        author = \"Melanie\"\n",
    "\n",
    "        # find text\n",
    "        text_s = doc.find(\"<TEXT>\") + len(\"<TEXT>\") \n",
    "        text_e = doc.find(\"</TEXT>\")\n",
    "        text = doc[text_s:text_e].strip()\n",
    "        text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "        text = text.lower()\n",
    "        \n",
    "        # text cleaning\n",
    "        text_start_cut = text.find(\"jump to search\")\n",
    "        if text_start_cut != -1:\n",
    "            text = text[text_start_cut+len(\"jump to search\"):]\n",
    "        text_end_cut3 = text.find(\"sources[edit]\")\n",
    "        if text_end_cut3 != -1:\n",
    "            text = text[:text_end_cut3]\n",
    "        text_end_cut4 = text.find(\"this page was last edited\")\n",
    "        if text_end_cut4 != -1:\n",
    "            text = text[:text_end_cut4]\n",
    "        text_end_cut5 = text.find(\"navigation menu\")\n",
    "        if text_end_cut5 != -1:\n",
    "            text = text[:text_end_cut5]\n",
    "        text = text.replace(\"[edit]\", \" \")\n",
    "\n",
    "        # find outlinks\n",
    "        out_s = doc.find(\"<OUTLINKS>\") + len(\"<OUTLINKS>\") \n",
    "        out_e = doc.find(\"</OUTLINKS>\")\n",
    "        outlinks = doc[out_s:out_e].strip()\n",
    "        outlinks = outlinks.split(\"http://\")\n",
    "        outlinks_final = []\n",
    "        for each in outlinks:\n",
    "            temp = \"http://\" + each\n",
    "            outlinks_final.append(temp)\n",
    "        if \"http://\" in outlinks_final:\n",
    "            outlinks_final.remove(\"http://\")\n",
    "\n",
    "        # find inlinks\n",
    "        in_s = doc.find(\"<INLINKS>\") + len(\"<INLINKS>\") \n",
    "        in_e = doc.find(\"</INLINKS>\")\n",
    "        inlinks = doc[in_s:in_e].strip()\n",
    "        inlinks = inlinks.split(\"http://\")\n",
    "        inlinks_final = []\n",
    "        for each in inlinks:\n",
    "            temp = \"http://\" + each\n",
    "            inlinks_final.append(temp)\n",
    "        if \"http://\" in inlinks_final:\n",
    "            inlinks_final.remove(\"http://\")\n",
    "\n",
    "        # find raw html\n",
    "        raw_s = doc.find(\"<RAW_HTML>\") + len(\"<RAW_HTML>\") \n",
    "        raw_e = doc.find(\"</RAW_HTML>\")\n",
    "        raw = doc[raw_s:raw_e].strip()\n",
    "        \n",
    "        # figure out if doc is already in index\n",
    "        body_check = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"_id\": docno\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        resp = es.search(index=\"corpus_wwii\", body=body_check)\n",
    "        if resp[\"hits\"][\"total\"][\"value\"] != 0:\n",
    "            \n",
    "            # get inlinks and update\n",
    "            r_author = resp[\"hits\"][\"hits\"][0][\"_source\"][\"author\"]\n",
    "            if \"Melanie\" not in r_author:\n",
    "                new_author = r_author + \" \" + author\n",
    "                \n",
    "                r_inlinks = resp[\"hits\"][\"hits\"][0][\"_source\"][\"inlinks\"]\n",
    "                r_inlinks_set = set(resp[\"hits\"][\"hits\"][0][\"_source\"][\"inlinks\"].split(\"\\n\"))\n",
    "                r_inlinks_new = r_inlinks + \"\\n\"\n",
    "                for each in inlinks_final:\n",
    "                    if each not in r_inlinks_set:\n",
    "                        r_inlinks_new += each + \"\\n\"\n",
    "\n",
    "                # use update() API to add new inlinks and author\n",
    "                source_to_update = {\n",
    "                    \"doc\" : {\n",
    "                        \"inlinks\" : r_inlinks_new,\n",
    "                        \"author\" : new_author\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                es.update(index='corpus_wwii', doc_type=\"_doc\", id=docno, body=source_to_update)\n",
    "            \n",
    "        else:\n",
    "            # yield doc for upload\n",
    "            formatted_inlinks = \"\\n\".join(inlinks_final)\n",
    "            formatted_outlinks = \"\\n\".join(outlinks_final)\n",
    "            doc_source = {\n",
    "                \"docno\": docno,\n",
    "                \"title\": title,\n",
    "                \"text\": text,\n",
    "                \"outlinks\": formatted_outlinks,\n",
    "                \"inlinks\": formatted_inlinks,\n",
    "                \"rawhtml\": raw,\n",
    "                \"author\": author\n",
    "            }\n",
    "#             add_body = {\n",
    "#                 \"_index\": \"corpus_wwii\",\n",
    "#                 \"_id\": docno,\n",
    "#                 \"_source\": doc_source\n",
    "#             }\n",
    "            es.index(index='corpus_wwii', doc_type='_doc', id=docno, body=doc_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test merge we should see that melanie1-4 docs were added\n",
    "# also that in history9 an additional inlink was added and melanie was added to author\n",
    "\n",
    "file_path = \"C:/6200-IR/hw3-mplatt27/docs/\"\n",
    "all_files = get_files_in_dir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_docs(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
