{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from url_normalize import url_normalize\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pickle\n",
    "import urllib.robotparser\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global\n",
    "LINK_GRAPH = {} # urls to link objects for saving later\n",
    "#DOMAINS = {} # list of domains we have visited\n",
    "INLINKS = {} # maps url to list of urls that point to it\n",
    "OUTLINKS = {} # maps url to list of urls that it points to\n",
    "VISITED = set([]) # list of urls we have crawled\n",
    "FRONTIER = None\n",
    "I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Link():\n",
    "    \"\"\"\n",
    "       A Link object holds the url as a string, the outlinks that this link points to, the\n",
    "       inlinks that point to this link, the wave number that this link belongs to in the crawl,\n",
    "       the relevance score this link is given, and the domain of the link.\n",
    "    \"\"\"\n",
    "    def __init__(self, link, inlinks, outlinks, wave, relevance_score):\n",
    "        self.link = link\n",
    "        self.outlinks = []\n",
    "        self.inlinks = []\n",
    "        self.wave = wave\n",
    "        self.relevance_score = relevance_score\n",
    "        self.domain = urlparse(link).netloc.strip()\n",
    "        \n",
    "    def update_outlinks(self, outlinks):\n",
    "        \"Add a list of outlinks\"\n",
    "        self.outlinks = outlinks\n",
    "        \n",
    "    def update_inlinks(self, inlinks):\n",
    "        \"Add a list of inlinks\"\n",
    "        self.inlinks = inlinks\n",
    "        \n",
    "    def update_score(self, relevance_score):\n",
    "        \"Add a relevence score\"\n",
    "        self.relevance_score = relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queue:\n",
    "    \"A container with a first-in-first-out (FIFO) queuing policy.\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.list = []\n",
    "        self.set = set([])\n",
    "\n",
    "    def push(self,item):\n",
    "        \"Enqueue the 'item' into the queue\"\n",
    "        self.list.insert(0,item)\n",
    "        self.set.add(item)\n",
    "\n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "          Dequeue the earliest enqueued item still in the queue. This\n",
    "          operation removes the item from the queue.\n",
    "        \"\"\"\n",
    "        return self.list.pop()\n",
    "\n",
    "    def isEmpty(self):\n",
    "        \"Returns true if the queue is empty\"\n",
    "        return len(self.list) == 0\n",
    "    \n",
    "    def sort_queue(self):\n",
    "        \"\"\"\n",
    "          Slice the first 1000 links off the queue, get those from the wave we are on,\n",
    "          sort those in the current wave, append back to list. \n",
    "        \"\"\"\n",
    "        if len(self.list) > 1000:\n",
    "            # cut off everything besides the first 1000\n",
    "            lnth = len(self.list)\n",
    "            first_group = self.list[-1000:]\n",
    "            last_group = self.list[:lnth-1000]\n",
    "\n",
    "            # get wave of next item to be dequeud\n",
    "            w = first_group[-1].wave\n",
    "\n",
    "            # get list of all from first group that are in wave 1\n",
    "            first_group_wave_w = [x for x in first_group if x.wave == w]\n",
    "            first_group_not_wave_w = [x for x in first_group if x.wave != w]\n",
    "\n",
    "            # sort first group that belongs to wave w\n",
    "            sorted_first_group = sorted(first_group_wave_w, key=lambda x: x.relevance_score)\n",
    "\n",
    "            # regroup lists\n",
    "            self.list = last_group + first_group_not_wave_w + sorted_first_group\n",
    "    \n",
    "    def print_queue(self):\n",
    "        \"Prints each link in the queue\"\n",
    "        for each in self.list:\n",
    "            print(each.link)\n",
    "        \n",
    "    def has_link(self, url):\n",
    "        \"Check if a link exists in the queue or not\"\n",
    "        if url in self.set:\n",
    "            return True\n",
    "        return False\n",
    "#         temp_set = set(self.list)\n",
    "#         if url in temp_set:\n",
    "#             return True\n",
    "#         return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    \"\"\"\n",
    "       Crawl the web to create a coprus of documents partaining to the topic of World War II.\n",
    "       The crawler supports the saving of the documents as txt files, scoring documents based\n",
    "       on relevance, and sorting the frontier so that the best documents are chosen. \n",
    "    \"\"\"\n",
    "    def __init__(self, frontier, visited, domains, inlinks, outlinks, link_graph, stop_criteria, i):\n",
    "        self.frontier = frontier\n",
    "        self.visited = visited\n",
    "        self.domains = domains\n",
    "        self.link_graph = link_graph\n",
    "        self.inlinks = inlinks\n",
    "        self.outlinks = outlinks\n",
    "        self.stop_criteria = stop_criteria\n",
    "        self.blacklist = set(['Citation_needed', 'Special:BookSources', 'Wikimedia_sister_projects', 'doi', 'CorpusID',\n",
    "                         'universitypublishingonline', 'index.php?', 'Category:', 'archive.org', 'Terms_of_Use',\n",
    "                         'wiktionary', 'omniatlas', 'Special:', 'Index_of', 'Panther%', 'SELIBR', 'Codebook', 'Help:',\n",
    "                         'S2CID', 'Press', 'MyContributions', 'Basic_Books', 'NOTRS', 'www.worldcat.org/oclc/278029256',\n",
    "                         'Weidenfeld_&_Nicolson', 'Template_talk:', 'index.php', 'Random_House', 'Contact_us', 'Publishing',\n",
    "                         'Creative_Commons', 'License', 'General_disclaimer', 'Simon_&_Schuster', 'Rodopi',\n",
    "                         'Houghton_Mifflin_Company', 'Template:', 'Portal:', 'Publish', 'Manual_of_Style', 'Talk:','OCLC',\n",
    "                             'Rowman_&_Littlefield', 'Wikiversity', 'wikiversity', 'youtube', 'YouTube'])\n",
    "        self.keywords = set(['war', 'world', 'ii', 'battle', 'army', 'united', 'states', 'us', 'u.s', 'germany', 'austria',\n",
    "                        'france', 'england', 'fought', 'fight', 'seige', 'poland', 'holocaust', 'hitler', 'japan', 'nazi',\n",
    "                            'jewish', 'second', 'concentration'])\n",
    "        self.i = i\n",
    "        self.files_to_write = {} # i --> text to put in file\n",
    "        \n",
    "    def get_doc_details(self, url, response, s):\n",
    "        \"\"\"\n",
    "         Retrives the document details from the request response that are needed to save the \n",
    "         document in the corpus, and crawl its outlinks. \n",
    "        \"\"\"\n",
    "        details = {}\n",
    "        details['doc_id'] = url\n",
    "        details['headers'] = response.headers\n",
    "        details['raw_html'] = s\n",
    "        if s.title == None:\n",
    "            details['title'] = \"\"\n",
    "        else:\n",
    "            details['title'] = s.title.string\n",
    "        if s.get_text() == None:\n",
    "            details['text'] = \"\"\n",
    "        else:\n",
    "            details['text'] = s.get_text() #strip=True\n",
    "        temp_list = [a['href'] for a in s.find_all('a', href=True) if a.text]\n",
    "        temp_list = list(set(temp_list))\n",
    "        details['outlinks'] = temp_list\n",
    "        return details\n",
    "    \n",
    "    \n",
    "    def write_doc_to_file(self, doc_dict, i):\n",
    "        \"\"\"\n",
    "          Writes a file in the corpus with the appropriate fields. One document for the corpus file,\n",
    "          and another to hold the headers and the raw html (each will be named with the same id number).\n",
    "        \"\"\"\n",
    "        f = open('C:/6200-IR/hw3-mplatt27/docs/' + \"_\" + str(i) + \".txt\", \"w\", encoding=\"utf-8\")\n",
    "        in_w = \"\"\n",
    "        out_w = \"\"\n",
    "        f.write('<DOC>\\n')\n",
    "        f.write('<DOCNO>' + doc_dict['doc_id'] + \"</DOCNO>\\n\")\n",
    "        if doc_dict['title'] != None:\n",
    "            f.write('<TITLE>' + doc_dict['title'] + '</TITLE>\\n')\n",
    "        else:\n",
    "            f.write('<TITLE></TITLE>\\n')\n",
    "        f.write('<AUTHOR>MELANIE PLATT</AUTHOR>\\n')\n",
    "        if doc_dict['text'] != None:\n",
    "            f.write('<TEXT>' + doc_dict['text'] + '</TEXT>\\n')\n",
    "        else:\n",
    "            f.write('<TEXT></TEXT>\\n')\n",
    "        f.write('<OUTLINKS>' + out_w.join(self.link_graph[doc_dict['doc_id']].outlinks) + '</OUTLINKS>\\n')\n",
    "        f.write('<INLINKS>' + in_w.join(self.inlinks[doc_dict['doc_id']]) + '</INLINKS>\\n')\n",
    "        f.write('<HEADERS>' + str(doc_dict['headers']) + '</HEADERS>\\n')\n",
    "        f.write('<RAW_HTML>' + str(doc_dict['raw_html']) + '</RAW_HTML>\\n')\n",
    "        f.write('</DOC>')\n",
    "        f.close()\n",
    "        \n",
    "    \n",
    "    def canonicalize_url(self, outlink_url, original_url=None):\n",
    "        \"\"\"\n",
    "          Normalize the urls so that they are all in the same format. Make relative urls\n",
    "          absolute, change scheme and host to lower case, remove port, and dup slashes.\n",
    "          Get rid of everything after #, make all http (not https). Check that they are\n",
    "          not pdfs or another unreadable type of page. \n",
    "        \"\"\"\n",
    "        # make all relative urls absolute\n",
    "        if original_url != None:\n",
    "            if urlparse(outlink_url).netloc.strip() == \"\":\n",
    "                outlink_url = urljoin(original_url, outlink_url)\n",
    "                \n",
    "#         pth = urlparse(outlink_url).path\n",
    "#         bad_paths = ['.jpeg', '.pdf', '.javascript', 'png', 'mpeg', 'mp4']\n",
    "#         for each in bad_paths:\n",
    "#             if each in pth:\n",
    "#                 return \"\"\n",
    "\n",
    "        # use url_normalize library to change scheme and host to lowercase, remove the port and dup slashes\n",
    "        outlink_url = url_normalize(outlink_url)\n",
    "\n",
    "        # get rid of everything after #\n",
    "        start_index = outlink_url.find(\"#\")\n",
    "        if start_index >= 0:\n",
    "            outlink_url = outlink_url[:start_index]\n",
    "\n",
    "        # make all http (not https)\n",
    "        outlink_url = outlink_url.replace(\"https\", \"http\")\n",
    "        return outlink_url \n",
    "    \n",
    "    \n",
    "    def is_blacklisted(self,url):\n",
    "        \"Check if the link is a type we know we don't want\"\n",
    "        for word in self.blacklist:\n",
    "            if word in url:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def filter_links(self, url, links):\n",
    "        \"\"\"\n",
    "          Filters outlinks that we want to keep by canonicalizing them, checking if they are in visited or the \n",
    "          frontier already, and they are not blacklisted. Creates an empty Link object for each link we are keeping\n",
    "          and initializes it with an empty in and outlink list and a score of 0. Appends each link object to the\n",
    "          link graph.\n",
    "        \"\"\"\n",
    "        new_links = []\n",
    "        for each in links:\n",
    "            try:\n",
    "                each = self.canonicalize_url(each, url)\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                if each == \"\":\n",
    "                    continue\n",
    "                elif each not in self.visited: # and not self.frontier.has_link(each):\n",
    "                    if not self.is_blacklisted(each):\n",
    "                        new_links.append(each)\n",
    "                if self.frontier.has_link(each): # added so that even if we don't want the link in frontier again, we know\n",
    "                    if each not in self.inlinks:\n",
    "                        self.inlinks[each] = [url]\n",
    "                    else:\n",
    "                        if url not in self.inlinks[each]:\n",
    "                            self.inlinks[each].append(url) # another link points to it\n",
    "            \n",
    "        return new_links\n",
    "                \n",
    "    \n",
    "    def update_inlink_counts(self, curr_link, new_outlinks):\n",
    "        \"\"\"\n",
    "          For each new outlink, if it is in the inlink list already, append the current link to that list, else\n",
    "          initialize the list as the current link (i.e, the current link points to the outlink). Once all inlink\n",
    "          lists are updated, update the object for each outlink in the linkgraph (so that it holds the updated\n",
    "          inlink list for them).\n",
    "        \"\"\"\n",
    "        \n",
    "        # update self.new_outlinks dictionary\n",
    "        for each in new_outlinks:\n",
    "            if each not in self.inlinks:\n",
    "                self.inlinks[each] = [curr_link]\n",
    "            else:\n",
    "                if curr_link not in self.inlinks[each]:\n",
    "                    self.inlinks[each].append(curr_link)\n",
    "        \n",
    "    \n",
    "    def has_x_keywords(self,link):\n",
    "        \"Checks if the link contains keywords that give it a higher score\"\n",
    "        count = 0\n",
    "        for each in self.keywords:\n",
    "            if each in link.lower():\n",
    "                count += 1\n",
    "        return count\n",
    "        \n",
    "        \n",
    "    def score_new_outlinks(self, links, wave):\n",
    "        \"\"\"\n",
    "         Scores links before adding them to the frontier based on the number of\n",
    "         inlinks, the key words that they have, if they are from wikipedia (good),\n",
    "         and their wave number. \n",
    "        \"\"\"\n",
    "        scored_objs = []\n",
    "        for each in links:\n",
    "            temp_obj = Link(each, [], [], wave+1, 0)\n",
    "            \n",
    "            # calculate score\n",
    "            score = 0\n",
    "            score += len(self.inlinks[each])\n",
    "            score -= wave+1\n",
    "            score += (self.has_x_keywords(each) * 5)\n",
    "            if 'wikipedia' in each.lower():\n",
    "                score += 10\n",
    "\n",
    "            # add score to object\n",
    "            temp_obj.update_score(score)\n",
    "            scored_objs.append(temp_obj)\n",
    "\n",
    "        return scored_objs\n",
    "    \n",
    "    \n",
    "    def save_variables(self):\n",
    "        \"Saves class variables to global variables so we have them in if program crashes and at end\"\n",
    "        \n",
    "        with open(save_path + \"inlinks.pickle\", 'wb') as handle:\n",
    "            pickle.dump(self.inlinks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        with open(save_path + \"visited.pickle\", 'wb') as handle:\n",
    "            pickle.dump(self.visited, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        temp_front = self.frontier.list[-5000:]\n",
    "        with open(save_path + \"frontier_list.pickle\", 'wb') as handle:\n",
    "            pickle.dump(temp_front, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        with open(save_path + \"link_graph.pickle\", 'wb') as handle:\n",
    "            pickle.dump(self.link_graph, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def save_frontier(self):\n",
    "        \n",
    "        with open('C:/6200-IR/hw3-mplatt27/frontier.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.frontier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "          Crawl the web for 40000 documents using a frontier Queue structure. For each link popped off the queue,\n",
    "          get the robots.txt file, make sure we sleep if we have recently crawled that domain and that we can crawl\n",
    "          that link. If we can, make a get reqest for the information and write the file. Explore the outlinks to \n",
    "          add to the frontier. Keep track of where we have been in a visited list. Periodically sort the frontier\n",
    "          and save variables. \n",
    "        \"\"\"\n",
    "        #rp = urllib.robotparser.RobotFileParser()\n",
    "        # i = 0\n",
    "        save_count = 0\n",
    "        sort_count = 0\n",
    "        write_count = 0\n",
    "        safe_stop = 0\n",
    "        front_save = 0\n",
    "        last_domain = None\n",
    "        last_rp_input = None\n",
    "        first_round = True\n",
    "        second_round = True\n",
    "        last_delay = 1\n",
    "        \n",
    "        while len(self.visited) != self.stop_criteria:\n",
    "            if safe_stop > 42000: # for testing\n",
    "                break\n",
    "            \n",
    "            if self.i % 100 == 0:\n",
    "                print(\"visited is this long: \", len(self.visited))\n",
    "            \n",
    "            current_link_obj = self.frontier.pop()\n",
    "            current_link = current_link_obj.link\n",
    "            current_wave = current_link_obj.wave\n",
    "            current_domain = current_link_obj.domain\n",
    "            \n",
    "            if current_link not in self.visited:\n",
    "                \n",
    "                # built input for robots.txt reader\n",
    "                sch = urlparse(current_link).scheme.strip()\n",
    "                rp_input = sch + '://' + current_domain + '/robots.txt'\n",
    "                can_crawl = False\n",
    "                \n",
    "                # if we crawled same domain last time, use rp that we have\n",
    "                if rp_input == last_rp_input:\n",
    "                    can_crawl = rp.can_fetch(\"*\", current_link)\n",
    "                    delay = last_delay\n",
    "                else:\n",
    "                    # we need to make a new rp\n",
    "                    try: \n",
    "                        rp = urllib.robotparser.RobotFileParser()\n",
    "                        rp.set_url(current_link)\n",
    "                        rp.read()\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        can_crawl = rp.can_fetch(\"*\", current_link)\n",
    "                    \n",
    "                # check if we can crawl the current url\n",
    "                if can_crawl:\n",
    "\n",
    "                    # if new domain, get the sleep delay\n",
    "                    if current_domain != last_domain:\n",
    "                        delay = rp.crawl_delay(\"*\")\n",
    "                        if delay == None:\n",
    "                            delay = 1\n",
    "                            last_delay = delay\n",
    "                        \n",
    "                    if delay < 3:\n",
    "                        # get current domain to see if we have to sleep\n",
    "                        if current_domain == last_domain:\n",
    "                            time.sleep(delay)\n",
    "\n",
    "                        # GET response for url\n",
    "                        try:\n",
    "                            resp = requests.get(current_link, timeout=3)\n",
    "                        except:\n",
    "                            pass\n",
    "                        else:\n",
    "                            \n",
    "                            # check that it is html and in english\n",
    "                            cont_type = resp.headers.get('Content-Type',0)\n",
    "                            language = resp.headers.get('Content-Language',0)\n",
    "                            if cont_type == 0:\n",
    "                                cont_type = \"None\"\n",
    "\n",
    "                            # create soup for later, and as a second check for language\n",
    "                            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "                            if soup != None:\n",
    "                                if language == 0:\n",
    "                                    if soup.html != None:\n",
    "                                        language = soup.html.get('lang',0)\n",
    "                                if 'text/html' in cont_type and language == 'en':\n",
    "\n",
    "                                    # get details from the response for file\n",
    "                                    doc_details = self.get_doc_details(current_link, resp, soup)\n",
    "                                    \n",
    "                                    # get outlinks and filter them\n",
    "                                    outlinks = self.filter_links(current_link, doc_details['outlinks'])\n",
    "                                    doc_details['outlinks'] = outlinks\n",
    "                                    \n",
    "                                    # save response to write file later\n",
    "                                    self.files_to_write[self.i] = doc_details\n",
    "\n",
    "                                    # update inlinks with the new outlinks (link objects will have empty inlink lists)\n",
    "                                    # until they are crawled\n",
    "                                    self.update_inlink_counts(current_link, outlinks)\n",
    "                                    \n",
    "                                    # score each outlink, create the object so it has the appropriate score while in front.\n",
    "                                    outlink_objects = self.score_new_outlinks(outlinks, current_wave)\n",
    "                                    \n",
    "                                    # put the link objects (which have updated inlinks and scores) in frontier\n",
    "                                    for each in outlink_objects:\n",
    "                                        self.frontier.push(each)\n",
    "                                    \n",
    "                                    # add outlinks to link graph, then update current url's oulinks to what we found\n",
    "                                    current_link_obj.update_outlinks(outlinks)\n",
    "                                    current_link_obj.update_inlinks(self.inlinks[current_link])\n",
    "                                    self.link_graph[current_link] = current_link_obj\n",
    "\n",
    "                                    # append link to visited and increase count we are on\n",
    "                                    self.visited.add(current_link)\n",
    "                                    self.i += 1\n",
    "                                    save_count += 1\n",
    "                                    sort_count += 1\n",
    "                                    write_count += 1\n",
    "                                    front_save += 1\n",
    "                                    last_domain = current_domain\n",
    "                                    last_rp_input = rp_input\n",
    "                            \n",
    "             \n",
    "            safe_stop += 1\n",
    "            \n",
    "            # save variables and write files\n",
    "            if save_count == 500:\n",
    "                self.save_variables()\n",
    "                save_count = 0\n",
    "                print(\"variables saved\")\n",
    "                \n",
    "            if front_save == 7000:\n",
    "                self.save_frontier()\n",
    "                front_save = 0\n",
    "                print(\"saved frontier\")\n",
    "                \n",
    "            if write_count == 50:\n",
    "                for num, dat in self.files_to_write.items():\n",
    "                    self.write_doc_to_file(dat, num)\n",
    "                self.files_to_write = {}\n",
    "                write_count = 0\n",
    "                print(\"wrote files\")\n",
    "                \n",
    "                \n",
    "            # sort frontier\n",
    "            if sort_count == 5 and first_round:\n",
    "                self.frontier.sort_queue()\n",
    "                sort_count = 0\n",
    "                print(\"frontier was sorted\")\n",
    "                first_round = False\n",
    "            elif sort_count == 100 and second_round:\n",
    "                self.frontier.sort_queue()\n",
    "                sort_count = 0\n",
    "                print(\"frontier was sorted\")\n",
    "                second_round = False\n",
    "            elif sort_count == 500:\n",
    "                self.frontier.sort_queue()\n",
    "                sort_count = 0\n",
    "                print(\"frontier was sorted\")\n",
    "        \n",
    "        # save variables if finished running, save remaining files\n",
    "        self.save_variables()\n",
    "        for num, dat in self.files_to_write.items():\n",
    "            self.write_doc_to_file(dat, num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean seed urls: \n",
      "\n",
      "http://en.wikipedia.org/wiki/World_War_II\n",
      "http://www.history.com/topics/world-war-ii\n",
      "http://en.wikipedia.org/wiki/List_of_World_War_II_battles_involving_the_United_States\n",
      "http://en.wikipedia.org/wiki/Military_history_of_the_United_States_during_World_War_II\n",
      "http://en.wikipedia.org/wiki/List_of_military_engagements_of_World_War_II\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main code to crawl the web\"\"\"\n",
    "\n",
    "# obtain seed urls and save\n",
    "seed_urls = ['http://en.wikipedia.org/wiki/World_War_II', 'http://www.history.com/topics/world-war-ii',\n",
    "            'http://en.wikipedia.org/wiki/List_of_World_War_II_battles_involving_the_United_States', \n",
    "            'http://en.wikipedia.org/wiki/Military_history_of_the_United_States_during_World_War_II',\n",
    "            'https://en.wikipedia.org/wiki/List_of_military_engagements_of_World_War_II']\n",
    "\n",
    "# canonicalize seeds and create link objects for each\n",
    "seed_link_objects = []\n",
    "for each in seed_urls:\n",
    "    each = url_normalize(each)\n",
    "    start_index = each.find(\"#\")\n",
    "    if start_index >= 0:\n",
    "        each = each[:start_index]\n",
    "    each = each.replace(\"https\", \"http\")\n",
    "    obj = Link(each, [], [], 1, 10000) # give seeds a very high score\n",
    "    seed_link_objects.append(obj)\n",
    "    \n",
    "print(\"Clean seed urls: \\n\")\n",
    "for each in seed_link_objects:\n",
    "    print(each.link)\n",
    "    \n",
    "# create frontier, inlinks, outlinks, and add link objects; add to global link graph also\n",
    "FRONTIER = Queue()\n",
    "for each in seed_link_objects:\n",
    "    FRONTIER.push(each)\n",
    "    LINK_GRAPH[each.link] = each\n",
    "    INLINKS[each.link] = []\n",
    "    OUTLINKS[each.link] = []  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link:  http://en.wikipedia.org/wiki/World_War_II\n",
      "outlinks:  []\n",
      "inlinks:  []\n",
      "wave:  1\n",
      "score:  10000\n",
      "*******************\n",
      "link:  http://www.history.com/topics/world-war-ii\n",
      "outlinks:  []\n",
      "inlinks:  []\n",
      "wave:  1\n",
      "score:  10000\n",
      "*******************\n",
      "link:  http://en.wikipedia.org/wiki/List_of_World_War_II_battles_involving_the_United_States\n",
      "outlinks:  []\n",
      "inlinks:  []\n",
      "wave:  1\n",
      "score:  10000\n",
      "*******************\n",
      "link:  http://en.wikipedia.org/wiki/Military_history_of_the_United_States_during_World_War_II\n",
      "outlinks:  []\n",
      "inlinks:  []\n",
      "wave:  1\n",
      "score:  10000\n",
      "*******************\n",
      "link:  http://en.wikipedia.org/wiki/List_of_military_engagements_of_World_War_II\n",
      "outlinks:  []\n",
      "inlinks:  []\n",
      "wave:  1\n",
      "score:  10000\n",
      "*******************\n"
     ]
    }
   ],
   "source": [
    "# print link graph to make sure seeds were added correctly\n",
    "for key, value in LINK_GRAPH.items():\n",
    "    print(\"link: \", key)\n",
    "    print(\"outlinks: \", value.outlinks)\n",
    "    print(\"inlinks: \", value.inlinks)\n",
    "    print(\"wave: \", value.wave)\n",
    "    print(\"score: \", value.relevance_score)\n",
    "    print(\"*******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frontier saved!\n",
      "visited list saved!\n",
      "link graph saved!\n",
      "inlinks saved!\n",
      "outlinks saved!\n"
     ]
    }
   ],
   "source": [
    "# pickle objects to start off (FRONTIER, VISITED, LINK_GRAPH, DOMAINS, INLINKS, OUTLINKS, I)\n",
    "# note that domains is not longer used (we just keep track of the last domain visited)\n",
    "\n",
    "save_path = 'C:/6200-IR/hw3-mplatt27/'\n",
    "with open(save_path + \"frontier.pickle\", 'wb') as handle:\n",
    "    pickle.dump(FRONTIER, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"frontier saved!\")\n",
    "\n",
    "with open(save_path + \"visited.pickle\", 'wb') as handle:\n",
    "    pickle.dump(VISITED, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"visited list saved!\")\n",
    "\n",
    "with open(save_path + \"link_graph.pickle\", 'wb') as handle:\n",
    "    pickle.dump(LINK_GRAPH, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"link graph saved!\")\n",
    "\n",
    "with open(save_path + \"inlinks.pickle\", 'wb') as handle:\n",
    "    pickle.dump(INLINKS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"inlinks saved!\")\n",
    "\n",
    "with open(save_path + \"outlinks.pickle\", 'wb') as handle:\n",
    "    pickle.dump(OUTLINKS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"outlinks saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize crawler with frontier, visited, domains, inlinks, outlinks, link_graph, stop_criteria, i\n",
    "c = Crawler(FRONTIER, VISITED, DOMAINS, INLINKS, OUTLINKS, LINK_GRAPH, 40000, I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visited is this long:  0\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  200\n",
      "visited is this long:  200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  600\n",
      "visited is this long:  600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  1000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1200\n",
      "visited is this long:  1200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  1500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1600\n",
      "visited is this long:  1600\n",
      "visited is this long:  1600\n",
      "visited is this long:  1600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  1900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  2000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2300\n",
      "visited is this long:  2300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "visited is this long:  2400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  2500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "visited is this long:  2700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "visited is this long:  2900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  3000\n",
      "visited is this long:  3000\n",
      "visited is this long:  3000\n",
      "visited is this long:  3000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3100\n",
      "visited is this long:  3100\n",
      "visited is this long:  3100\n",
      "visited is this long:  3100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3400\n",
      "visited is this long:  3400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  3500\n",
      "visited is this long:  3500\n",
      "visited is this long:  3500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3600\n",
      "visited is this long:  3600\n",
      "visited is this long:  3600\n",
      "visited is this long:  3600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  3900\n",
      "visited is this long:  3900\n",
      "visited is this long:  3900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  4000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "visited is this long:  4100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  4500\n",
      "visited is this long:  4500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4600\n",
      "visited is this long:  4600\n",
      "visited is this long:  4600\n",
      "visited is this long:  4600\n",
      "visited is this long:  4600\n",
      "visited is this long:  4600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  4900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  5000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5300\n",
      "visited is this long:  5300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  5500\n",
      "visited is this long:  5500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5600\n",
      "visited is this long:  5600\n",
      "visited is this long:  5600\n",
      "visited is this long:  5600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  5900\n",
      "visited is this long:  5900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  6000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "visited is this long:  6100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6400\n",
      "visited is this long:  6400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  6500\n",
      "visited is this long:  6500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  6900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  7000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  7100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  7200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  7300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  7400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  7500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  7600\n",
      "visited is this long:  7600\n",
      "visited is this long:  7600\n",
      "visited is this long:  7600\n",
      "frontier was sorted\n",
      "wrote files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote files\n",
      "visited is this long:  7700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  7800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  7900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  8000\n",
      "visited is this long:  8000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8100\n",
      "visited is this long:  8100\n",
      "visited is this long:  8100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  8500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  8900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  9000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  9500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9600\n",
      "visited is this long:  9600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  9900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  10000\n",
      "visited is this long:  10000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10100\n",
      "visited is this long:  10100\n",
      "visited is this long:  10100\n",
      "visited is this long:  10100\n",
      "visited is this long:  10100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10300\n",
      "visited is this long:  10300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10400\n",
      "visited is this long:  10400\n",
      "visited is this long:  10400\n",
      "visited is this long:  10400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  10500\n",
      "visited is this long:  10500\n",
      "visited is this long:  10500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "visited is this long:  10900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  11000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11100\n",
      "visited is this long:  11100\n",
      "visited is this long:  11100\n",
      "visited is this long:  11100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11200\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11300\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11400\n",
      "visited is this long:  11400\n",
      "visited is this long:  11400\n",
      "visited is this long:  11400\n",
      "visited is this long:  11400\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  11500\n",
      "visited is this long:  11500\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11600\n",
      "visited is this long:  11600\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11700\n",
      "visited is this long:  11700\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11800\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  11900\n",
      "wrote files\n",
      "variables saved\n",
      "wrote files\n",
      "visited is this long:  12000\n",
      "visited is this long:  12000\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  12100\n",
      "visited is this long:  12100\n",
      "visited is this long:  12100\n",
      "visited is this long:  12100\n",
      "visited is this long:  12100\n",
      "visited is this long:  12100\n",
      "visited is this long:  12100\n",
      "frontier was sorted\n",
      "wrote files\n",
      "wrote files\n",
      "visited is this long:  12200\n",
      "wrote files\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c.crawl()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl took 38653.04254245758 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Crawl took {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in pickles if program crashed\n",
    "save_path = 'C:/6200-IR/hw3-mplatt27/'\n",
    "handle = open(save_path + \"frontier.pickle\", 'rb')\n",
    "FRONTIER = pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "handle = open(save_path + \"visited.pickle\", 'rb')\n",
    "VISITED = pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "handle = open(save_path + \"link_graph.pickle\", 'rb')\n",
    "LINK_GRAPH = pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "# handle = open(save_path + \"domains.pickle\", 'rb')\n",
    "# DOMAINS = pickle.load(handle)\n",
    "# handle.close()\n",
    "\n",
    "handle = open(save_path + \"inlinks.pickle\", 'rb')\n",
    "INLINKS = pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "handle = open(save_path + \"outlinks.pickle\", 'rb')\n",
    "OUTLINKS = pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "# handle = open(save_path + \"i_value.pickle\", 'rb')\n",
    "# I = pickle.load(handle)\n",
    "# handle.close()\n",
    "I = 814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
